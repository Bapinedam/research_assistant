{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1a3181",
   "metadata": {},
   "source": [
    "## 1. Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07957376",
   "metadata": {},
   "outputs": [],
   "source": [
    "aim_research = \"\"\"\n",
    "\n",
    "                The aim of this study is to investigate how university students use, value, and trust feedback generated by Generative Artificial Intelligence (GenAI) \n",
    "                compared to feedback from teachers. Specifically, the research seeks to understand the different roles that GenAI and teacher feedback play in \n",
    "                higher education, the factors that influence students’ perceptions of their usefulness, reliability, and trustworthiness, and the ways in which these \n",
    "                forms of feedback shape students’ learning experiences, emotions, and engagement with feedback processes.  \n",
    "\n",
    "                \"\"\"\n",
    "\n",
    "actor_codes =  \"\"\"\n",
    "                # Code: A  \n",
    "\n",
    "                **Explanation:** GenAI (also variants such as AI, ChatGPT, etc.)  \n",
    "\n",
    "                ---\n",
    "\n",
    "                # Code: T  \n",
    "\n",
    "                **Explanation:** Teacher (also variants such as lecturer, tutor, staff, etc.)  \n",
    "\n",
    "                ---\n",
    "\n",
    "                # Code: B  \n",
    "\n",
    "                **Explanation:** Both (clear reference to both actors)  \n",
    "\n",
    "              \"\"\"\n",
    "               \n",
    "comparator_codes = \"\"\"\n",
    "\n",
    "                    # Code: More  \n",
    "\n",
    "                    **Explanation:** When the comment indicated that the actor was associated with more (volume, frequency, impact) of the characteristic.   \n",
    "\n",
    "                    ---\n",
    "\n",
    "                    # Code: Less  \n",
    "\n",
    "                    **Explanation:** As above but in reference to less or smaller volume, frequency or impact of the characteristic.  \n",
    "\n",
    "                    ---\n",
    "\n",
    "                    # Code: Similar  \n",
    "\n",
    "                    **Explanation:** When the comment made an explicit statement that both GenAI and Teachers were similar in some way (e.g., they were similarly useful).  \n",
    "\n",
    "\n",
    "                    \"\"\"\n",
    "                    \n",
    "characteristic_codes = \"\"\"\n",
    "                        # Theme: Processes  \n",
    "\n",
    "                        ## Sub-theme: Access  \n",
    "\n",
    "                        ### Code: Ease  \n",
    "                        **Definition:** Is easily accessible, available, or convenient.  \n",
    "\n",
    "                        ### Code: Speed  \n",
    "                        **Definition:** Is immediate, instant or fast. Described as time-efficient, saving time.  \n",
    "\n",
    "                        ### Code: Volume  \n",
    "                        **Definition:** Reference to quantity, volume, scale of information or feedback interactions (e.g., could ask many questions and get lots of responses).  \n",
    "\n",
    "                        ## Sub-theme: Timing  \n",
    "\n",
    "                        ### Code: Before submission  \n",
    "                        **Definition:** Provides information during the production of an assignment task.  \n",
    "\n",
    "                        ### Code: After submission  \n",
    "                        **Definition:** Provides information after the submission of an assignment task.  \n",
    "\n",
    "                        ## Sub-theme: Effort  \n",
    "\n",
    "                        ### Code: Less effort  \n",
    "                        **Definition:** Reduces effort associated with the task. This may be making a task easier in some way, or it may refer to doing the task entirely.  \n",
    "\n",
    "                        ---\n",
    "\n",
    "                        # Theme: Sense-making  \n",
    "\n",
    "                        ## Sub-theme: Sense-making \n",
    "\n",
    "                        ### Code: Understanding  \n",
    "                        **Definition:** Helps me understand.  \n",
    "\n",
    "                        ### Code: Reflection  \n",
    "                        **Definition:** Helps me reflect.  \n",
    "\n",
    "                        ### Code: Progress    \n",
    "                        **Definition:** Helps me know how I’m going.  \n",
    "\n",
    "                        ---\n",
    "\n",
    "                        # Theme: Information  \n",
    "\n",
    "                        ## Sub-theme: Quality  \n",
    "\n",
    "                        ### Code: Specificity  \n",
    "                        **Definition:** Provides specific or detailed information. Antonyms: vague, generic.  \n",
    "\n",
    "                        ### Code: In-depth  \n",
    "                        **Definition:** Provides in-depth or nuanced information. Antonyms: superficial, broad.  \n",
    "\n",
    "                        ### Code: Understandable  \n",
    "                        **Definition:** Information is presented in an understandable, digestible, and comprehensible way.  \n",
    "\n",
    "                        ### Code: Relevance  \n",
    "                        **Definition:** Information is relevant, on topic.  \n",
    "\n",
    "                        ### Code: Contextualised  \n",
    "                        **Definition:** Provides information that is adapted for the context of an assignment, rubrics, discipline, or class. (Not relational).  \n",
    "\n",
    "                        ### Code: Utility  \n",
    "                        **Definition:** Provides useful, usable, or helpful information.  \n",
    "\n",
    "                        ### Code: Reliable  \n",
    "                        **Definition:** Provides information that is accurate, precise, reliable, or trustworthy.  \n",
    "\n",
    "                        ### Code: Objective  \n",
    "                        **Definition:** Is objective in its feedback, judgment, or evaluation. Antonyms: subjective, biased.  \n",
    "\n",
    "                        ## Sub-theme: Tone  \n",
    "\n",
    "                        ### Code: Positivity  \n",
    "                        **Definition:** Makes positive statements, people pleaser\n",
    "\n",
    "                        ### Code: Negativity  \n",
    "                        **Definition:** Makes edits or provides statements that are perceived as negative in tone (e.g. dismissive, insulting, uncaring).  \n",
    "\n",
    "                        ---\n",
    "\n",
    "                        # Theme: Feeling  \n",
    "                        \n",
    "                        ## Sub-theme: Feeling  \n",
    "\n",
    "                        ### Code: Positive  \n",
    "                        **Definition:** Makes me feel a positive feeling. Synonyms: Encouraging, motivating.  \n",
    "\n",
    "                        ### Code: Negative  \n",
    "                        **Definition:** Makes me feel a negative feeling. Synonyms: Frustrating. \n",
    "\n",
    "                        ### Code: No impact\n",
    "                        **Definition:** Makes me feel nothing or indifferent. Has no impact on my emotions or feelings.\n",
    "\n",
    "                        ---\n",
    "\n",
    "                        # Theme: Relational  \n",
    "\n",
    "                        ## Sub-theme: Relational\n",
    "                         \n",
    "                        ### Code: Personal   \n",
    "                        **Definition:** Is supportive. Concerns relational type statements (e.g. the “teacher knows me”’). Closer/ more-distant or embodied (e.g. “I like being face to face with my teacher”). Some statements are about AI being more in tune, more personalised, personal to tastes in language, communication and \"learning styles\".\n",
    "\n",
    "                        ### Code: Risky  \n",
    "                        **Definition:** Makes me vulnerable to their judgement, causes me shame. Possibly related to power, position, self. Synonyms: Implies personal consequence, at risk. Antonyms: non-judgemental, no shame, preserves self, prevents vulnerability\n",
    "\n",
    "                        ### Code: Expert  \n",
    "                        **Definition:** Has a position of knowledge or expertise.  \n",
    "\n",
    "                        ---\n",
    "\n",
    "                        # Theme: Value  \n",
    "\n",
    "                        ## Sub-theme: Value  \n",
    "\n",
    "                        ### Code: Importance \n",
    "                        **Definition:** Is important or valuable.  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "characteristic_codes_v2 = \"\"\"\n",
    "\n",
    "            # Theme: Processes  \n",
    "\n",
    "            ## Sub-theme: Access  \n",
    "\n",
    "            ### Code: Ease  \n",
    "            **Definition:** Refers to the convenience of accessing feedback or support. It captures situations where obtaining assistance is straightforward, uncomplicated, and does not involve significant barriers compared to traditional sources like teachers or professors.  \n",
    "\n",
    "            ### Code: Speed  \n",
    "            **Definition:** Describes the immediacy of receiving responses or feedback. It highlights efficiency and time-saving aspects, where support is delivered rapidly enough to accelerate the learning or task process.  \n",
    "\n",
    "            ### Code: Volume  \n",
    "            **Definition:** Relates to the quantity of feedback or information available. It reflects the ability to engage in multiple interactions, ask numerous questions, and receive abundant responses compared to more limited human feedback.  \n",
    "\n",
    "            ## Sub-theme: Timing  \n",
    "\n",
    "            ### Code: Before submission  \n",
    "            **Definition:** Feedback is provided during the drafting or preparation stage of an assignment. This allows for real-time adjustments, improvements, and refinements prior to final submission.  \n",
    "\n",
    "            ### Code: After submission  \n",
    "            **Definition:** Feedback is received after an assignment has been submitted and graded. While it can inform future work, it does not directly impact the already completed task.  \n",
    "\n",
    "            ## Sub-theme: Effort  \n",
    "\n",
    "            ### Code: Less effort  \n",
    "            **Definition:** Highlights a reduction in the amount of work, energy, or cognitive load required to complete a task. This includes simplifying processes, providing starting points, or removing barriers that typically demand more effort from the student.  \n",
    "\n",
    "            ---\n",
    "\n",
    "            # Theme: Sense-making  \n",
    "\n",
    "            ## Sub-theme: Sense-making  \n",
    "\n",
    "            ### Code: Understanding  \n",
    "            **Definition:** Enables students to better comprehend concepts, ideas, or tasks. It involves organising thoughts, clarifying confusion, and making content more digestible for learning.  \n",
    "\n",
    "            ### Code: Reflection  \n",
    "            **Definition:** Encourages self-examination and critical thinking about one’s own learning, performance, or perspective. It involves generating insights that prompt deeper consideration of personal approaches.  \n",
    "\n",
    "            ### Code: Progress    \n",
    "            **Definition:** Provides indicators of how well a student is performing or advancing. It supports self-monitoring by offering feedback that signals whether learning or task outcomes are on track.  \n",
    "\n",
    "            ---\n",
    "\n",
    "            # Theme: Information  \n",
    "\n",
    "            ## Sub-theme: Quality  \n",
    "\n",
    "            ### Code: Specificity  \n",
    "            **Definition:** Refers to feedback or information that is detailed, concrete, and precise rather than vague or generic.  \n",
    "\n",
    "            ### Code: In-depth  \n",
    "            **Definition:** Captures the extent to which feedback is comprehensive, nuanced, and layered. It contrasts with broad or superficial responses by offering thorough explanations or contextual depth.  \n",
    "\n",
    "            ### Code: Understandable  \n",
    "            **Definition:** Information is communicated in a way that is clear, simple, and easy to grasp. This includes structuring content so that it is digestible and accessible to the learner.  \n",
    "\n",
    "            ### Code: Relevance  \n",
    "            **Definition:** Feedback or information aligns closely with the student’s needs, topic, or task requirements, avoiding off-topic or tangential content.  \n",
    "\n",
    "            ### Code: Contextualised  \n",
    "            **Definition:** Feedback is adapted to the specific assignment, discipline, or criteria. It demonstrates awareness of situational requirements rather than offering generic guidance.  \n",
    "\n",
    "            ### Code: Utility  \n",
    "            **Definition:** Information is practical, usable, and directly helpful in solving problems, advancing tasks, or exploring new ideas.  \n",
    "\n",
    "            ### Code: Reliable  \n",
    "            **Definition:** Feedback is characterised by trustworthiness, accuracy, and precision. It avoids misleading or erroneous information.  \n",
    "\n",
    "            ### Code: Objective  \n",
    "            **Definition:** Feedback is impartial and free of personal bias. It is grounded in factual or consistent standards rather than subjective judgments or preferences.  \n",
    "\n",
    "            ## Sub-theme: Tone  \n",
    "\n",
    "            ### Code: Positivity  \n",
    "            **Definition:** Feedback is encouraging, affirming, or expressed in a supportive tone. It creates a positive interpersonal dynamic, often aligning with a people-pleasing or agreeable style.  \n",
    "\n",
    "            ### Code: Negativity  \n",
    "            **Definition:** Feedback is expressed in a critical or discouraging tone, sometimes perceived as dismissive, harsh, or uncaring.  \n",
    "\n",
    "            ---\n",
    "\n",
    "            # Theme: Feeling  \n",
    "\n",
    "            ## Sub-theme: Feeling  \n",
    "\n",
    "            ### Code: Positive  \n",
    "            **Definition:** Feedback evokes feelings of encouragement, motivation, or appreciation. It creates a sense of being supported, valued, or cared for.  \n",
    "\n",
    "            ### Code: Negative  \n",
    "            **Definition:** Feedback generates negative emotions such as frustration, discouragement, or shame. It may undermine confidence or self-worth.  \n",
    "\n",
    "            ### Code: No impact  \n",
    "            **Definition:** Feedback is emotionally neutral, leaving the student indifferent. It does not trigger any significant positive or negative emotional response.  \n",
    "\n",
    "            ---\n",
    "\n",
    "            # Theme: Relational  \n",
    "\n",
    "            ## Sub-theme: Relational  \n",
    "\n",
    "            ### Code: Personal   \n",
    "            **Definition:** Feedback reflects personalisation and relational support. It demonstrates awareness of individual needs, progress, or preferences, fostering a sense of connection or tailored guidance.  \n",
    "\n",
    "            ### Code: Risky  \n",
    "            **Definition:** Feedback situations evoke vulnerability or fear of judgment. It reflects perceived risks to self-esteem, identity, or safety in the learning relationship.  \n",
    "\n",
    "            ### Code: Expert  \n",
    "            **Definition:** Feedback is grounded in knowledge, authority, and subject expertise. It reflects the credibility and informed position of the source.  \n",
    "\n",
    "            ---\n",
    "\n",
    "            # Theme: Value  \n",
    "\n",
    "            ## Sub-theme: Value  \n",
    "\n",
    "            ### Code: Importance \n",
    "            **Definition:** Captures the perceived significance or weight given to feedback. It reflects judgments about which sources of feedback are most valuable or influential in shaping learning outcomes.  \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "                        \n",
    "CODING_STRATEGY = \"\"\"\n",
    "    When coding student responses:\n",
    "\n",
    "    1. **Surface-Level Understanding**  \n",
    "    - Do not over-interpret student responses.  \n",
    "    - Work from the most direct, surface-level meaning of the text.  \n",
    "    - Avoid reading beyond what is explicitly written.\n",
    "\n",
    "    2. **Chunk-Based Coding**  \n",
    "    - Divide each response into meaningful chunks (a sentence or a significant phrase).  \n",
    "    - A chunk should only be coded once, even if it could theoretically fit multiple codes.\n",
    "\n",
    "    3. **Focus on Comparison**  \n",
    "    - Prioritize identifying any comparison between AI feedback and teacher feedback.  \n",
    "    - Look for explicit or implicit comparative language such as *more, less, better, worse, easier, harder, similar*.  \n",
    "\n",
    "    4. **Actor + Comparator + Characteristic Logic**  \n",
    "    - For each usable chunk, identify three components:  \n",
    "        **Actor** – Who or what the comment refers to (A = AI, T = Teacher, B = Both).  \n",
    "        **Comparator** – The relative position (e.g., more, less, similar, better, worse).  \n",
    "        **Characteristic** – The feedback property being described (e.g., clarity, accuracy, relevance, timeliness, emotional safety).  \n",
    "    - A valid coded chunk must include all three components.  \n",
    "\n",
    "    5. **Handling Unusable Responses**  \n",
    "    - If a response is unintelligible or does not appear to answer the question, code it as **UNUSABLE**.  \n",
    "    - If a chunk is missing **any one** of the three required components (Actor, Comparator, Characteristic), code it as **UNUSABLE**.  \n",
    "    - Example of unusable: \"It was better\" – we don’t know *which actor* it refers to.  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "CODING_STRATEGY_V2 = \"\"\"\n",
    "\n",
    "    When coding student responses:\n",
    "    1. **Surface-Level Understanding**  \n",
    "    - Keep interpretations minimal: code *only* what is directly and explicitly stated.  \n",
    "    - Do not infer hidden meanings, motivations, or consequences.  \n",
    "\n",
    "    2. **Chunk-Based Coding (Restrained)**  \n",
    "    - Divide a response only if there are **clear shifts in meaning** (e.g., “while,” “but,” “however”).  \n",
    "    - Avoid splitting into too many micro-chunks. A whole sentence is usually a single chunk.  \n",
    "    - **Max 2 chunks per sentence** unless multiple actors (AI vs Teacher) are explicitly compared.\n",
    "\n",
    "    3. **Salience over Exhaustiveness**  \n",
    "    - Prioritize **main comparisons** or feelings.  \n",
    "    - Do not code every possible nuance if it dilutes the focus.  \n",
    "    - Example: If a chunk has both a tone (“neutral”) and an efficiency descriptor (“efficient”), select the **dominant characteristic** (in this case, tone/feeling = “No impact”).\n",
    "\n",
    "    4. **Actor + Comparator + Characteristic Rule**  \n",
    "    - A valid code requires all three: Actor (A/T/B), Comparator (more/less/similar), and Characteristic (e.g., clarity, personal, timeliness).  \n",
    "    - If multiple characteristics appear but relate to the *same actor/comparator*, treat them as **one code**.  \n",
    "    - Example: “Teacher’s feedback was personal and encouraging” → single code: Actor = Teacher, Comparator = More, Characteristic = Personal/Positive.  \n",
    "\n",
    "    5. **Dominant Coding Principle**  \n",
    "    - When multiple possible codes exist in the same chunk, select the **most prominent or emotionally central one**.  \n",
    "    - Example: “GenAI felt neutral and efficient” → dominant code = “No impact” (feeling tone). “Efficient” is secondary and not coded separately.\n",
    "\n",
    "    6. **Handling Unusable Responses**  \n",
    "    - If actor, comparator, or characteristic is missing, mark as **UNUSABLE**.  \n",
    "    - Example: “It was better” → UNUSABLE.  \n",
    "    - Example: “Feedback was neutral” (no comparison between actors) → UNUSABLE.  \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "OUTPUT_FORMAT = \"\"\"\n",
    "    {\n",
    "        \"codes\": [\n",
    "            {   \n",
    "                \"actor\": \"...\",\n",
    "                \"comparator\": \"...\",\n",
    "                \"theme\": \"...\",\n",
    "                \"sub_theme\": \"...\",\n",
    "                \"code\": \"...\",\n",
    "                \"reasoning\": \"...\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "SHOTS = \"\"\"\n",
    "\n",
    "    Examples of a good output are the next ones:\n",
    "\n",
    "    Example 1:\n",
    "\n",
    "    Question: \"Please describe any differences in how it impacted your learning (comparing GenAI and your teacher)?\"\n",
    "\n",
    "    Student response: \"AI tends to be more uplifting or supportive than teachers, so when I ask for its opinion on something I’ve written it’ll compliment whereas my teacher will have edits to my work. Both are useful and important.\"\n",
    "\n",
    "    GOOD OUTPUT:\n",
    "\n",
    "    {\n",
    "        \"codes\": [\n",
    "            {   \n",
    "                \"actor\": \"A\",\n",
    "                \"comparator\": \"More\",\n",
    "                \"theme\": \"Information\",\n",
    "                \"sub_theme\": \"Tone\",\n",
    "                \"code\": \"Positivity\",\n",
    "                \"reasoning\": \"The phrases 'more uplifting or supportive' and 'it’ll compliment' directly indicate positive affective framing. By definition, the Positivity code captures feedback that is polite, supportive, or 'people-pleasing', which matches the student’s description of AI’s tone relative to teachers.\"\n",
    "            },\n",
    "            {   \n",
    "                \"actor\": \"T\",\n",
    "                \"comparator\": \"More\",\n",
    "                \"theme\": \"Information\",\n",
    "                \"sub_theme\": \"Quality\",\n",
    "                \"code\": \"Utility\",\n",
    "                \"reasoning\": \"The contrast 'AI will compliment whereas my teacher will have edits' signals that teacher feedback contains actionable, text-level changes. This aligns with the Utility code which captures feedback that is practical and useful for making concrete improvements, as evidenced by the teacher providing specific edits rather than just praise.\"\n",
    "            },\n",
    "            {\n",
    "                \"actor\": \"B\",\n",
    "                \"comparator\": \"Similar\",\n",
    "                \"theme\": \"Value\",\n",
    "                \"sub_theme\": \"Value\",\n",
    "                \"code\": \"Importance\",\n",
    "                \"reasoning\": \"The closing statement 'Both are useful and important' is an explicit equivalence claim about value. The Importance code captures perceived significance; here the student assigns comparable importance to both AI and teacher feedback.\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    Example 2:\n",
    "\n",
    "    Question: \"Please describe any differences in how it impacted your learning (comparing GenAI and your teacher)?\"\n",
    "\n",
    "    Student response: \"Provides more immediate feedback while working on assignments when the teacher wasn't yet able to.\"\n",
    "\n",
    "    GOOD OUTPUT:\n",
    "\n",
    "    {\n",
    "        \"codes\": [\n",
    "            {   \n",
    "                \"actor\": \"A\",\n",
    "                \"comparator\": \"More\",\n",
    "                \"theme\": \"Processes\",\n",
    "                \"sub_theme\": \"Access\",\n",
    "                \"code\": \"Speed\",\n",
    "                \"reasoning\": \"The term 'more immediate' directly maps to the Speed code (immediate/instant/time-efficient). The temporal clause 'while working on assignments' highlights availability during production (pre‑submission), reinforcing that AI’s response time outpaces the teacher’s in this context.\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    Example 3:\n",
    "\n",
    "    Question: \"Were there any differences in how the feedback made you feel (comparing GenAI and your teacher)?\"\n",
    "\n",
    "    Student response: \"AI made me feel good and teachers make me feel bad and dumb.\"\n",
    "\n",
    "    GOOD OUTPUT:\n",
    "\n",
    "    {\n",
    "        \"codes\": [\n",
    "            {   \n",
    "                \"actor\": \"A\",\n",
    "                \"comparator\": \"More\",\n",
    "                \"theme\": \"Feeling\",\n",
    "                \"sub_theme\": \"Feeling\",\n",
    "                \"code\": \"Positive\",\n",
    "                \"reasoning\": \"The literal statement 'AI made me feel good' is a direct indicator of positive emotional impact. The Positive code captures encouraging or motivating feelings, which the student explicitly attributes to AI in contrast to teachers.\"\n",
    "            },\n",
    "            {   \n",
    "                \"actor\": \"T\",\n",
    "                \"comparator\": \"More\",\n",
    "                \"theme\": \"Feeling\",\n",
    "                \"sub_theme\": \"Feeling\",\n",
    "                \"code\": \"Negative\",\n",
    "                \"reasoning\": \"The student reports teachers 'make me feel bad and dumb', which is an explicit negative affective response. The Negative code covers discouraging or frustrating emotions, indicating a stronger negative impact from teacher feedback relative to AI.\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "reasons_codes = \"\"\"\n",
    "\n",
    "                # Theme: Processes\n",
    "\n",
    "                ## Sub-theme: Access\n",
    "\n",
    "                ### Code: Unaware\n",
    "                **Definition:** Respondents didn’t know GenAI could provide feedback; or didn’t know how to use it; or didn’t think of using it.  \n",
    "\n",
    "                ## Sub-theme: Effort\n",
    "\n",
    "                ### Code: Effortful\n",
    "                **Definition:** Too much effort to use AI or learn how to use AI; don’t have time to use AI (e.g., last-minute assignments).  \n",
    "\n",
    "                ### Code: Less effort\n",
    "                **Definition:** Off-loads effort or experience of learning, which in this context is not desirable.  \n",
    "\n",
    "                ---\n",
    "\n",
    "                # Theme: Information\n",
    "\n",
    "                ## Sub-theme: Quality\n",
    "\n",
    "                ### Code: Specificity\n",
    "                **Definition:** Provides specific or detailed information. Antonyms: vague, generic.  \n",
    "\n",
    "                ### Code: In-depth\n",
    "                **Definition:** Not enough, or sometimes not useful because it offers too much depth or additional explanation than desired.  \n",
    "\n",
    "                ### Code: Contextualised\n",
    "                **Definition:** Information provided lacks the context of an assignment, rubrics, discipline or class.  \n",
    "\n",
    "                ### Code: Utility\n",
    "                **Definition:** Useful, usable, or helpful information.  \n",
    "\n",
    "                ### Code: Trustworthy\n",
    "                **Definition:** Respondent does not trust the information; it is not reliable, or the provider/source is not competent to provide trustworthy information.  \n",
    "\n",
    "                ## Sub-theme: Tone\n",
    "\n",
    "                ### Code: Positivity\n",
    "                **Definition:** Makes positive statements; people pleaser.  \n",
    "\n",
    "                ---\n",
    "\n",
    "                # Theme: Relational\n",
    "\n",
    "                ## Sub-theme: Relational\n",
    "\n",
    "                ### Code: Personal\n",
    "                **Definition:** Respondent values relationship or closer and embodied feedback; values the role or connection with humans.  \n",
    "\n",
    "                ### Code: Expert\n",
    "                **Definition:** Has a position of knowledge or expertise.  \n",
    "\n",
    "                ---\n",
    "\n",
    "                # Theme: Value\n",
    "\n",
    "                ## Sub-theme: Value\n",
    "\n",
    "                ### Code: Preference\n",
    "                **Definition:** Respondent preferred not using GenAI without specifying further reasons or expressing dislike.  \n",
    "\n",
    "                ### Code: Need\n",
    "                **Definition:** Does not want or need feedback; does not need feedback from AI (own skills or others are sufficient); no necessity to use AI for feedback.  \n",
    "\n",
    "                ### Code: Unsustainable\n",
    "                **Definition:** Respondent finds GenAI is not environmentally sustainable.  \n",
    "\n",
    "                ### Code: Privacy\n",
    "                **Definition:** Respondent reports concern regarding privacy.  \n",
    "\n",
    "                ### Code: Integrity\n",
    "                **Definition:** Respondent committed to the originality of own work and ethical behaviour in academic work.  \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "reasons_codes_v2 = \"\"\"\n",
    "\n",
    "        # Theme: Processes  \n",
    "\n",
    "        ## Sub-theme: Access  \n",
    "\n",
    "        ### Code: Unaware  \n",
    "        **Definition:** Captures situations where students are unaware that GenAI can provide feedback, lack knowledge of how to use it, or simply do not consider using it. It reflects both unawareness of the tool’s potential and uncertainty about its operation.  \n",
    "\n",
    "        ## Sub-theme: Effort  \n",
    "\n",
    "        ### Code: Effortful  \n",
    "        **Definition:** Highlights the perception that using GenAI requires excessive time or energy, either in learning how to use it effectively or in applying it under time pressure. This effort outweighs perceived benefits, particularly in rushed academic contexts.  \n",
    "\n",
    "        ### Code: Less effort  \n",
    "        **Definition:** Reflects concerns that using GenAI reduces the necessary effort involved in learning. Students perceive that offloading work to AI undermines the personal challenge, skill-building, and intellectual growth required for academic and professional development.  \n",
    "\n",
    "        ---\n",
    "\n",
    "        # Theme: Information  \n",
    "\n",
    "        ## Sub-theme: Quality  \n",
    "\n",
    "        ### Code: Specificity  \n",
    "        **Definition:** Emphasises the need for feedback that is detailed and tailored to the student’s work. AI-generated responses are perceived as too generic or vague compared to the specific input students seek.  \n",
    "\n",
    "        ### Code: In-depth  \n",
    "        **Definition:** Reflects dissatisfaction with the quality of feedback when AI provides overly broad or superficial input, lacking nuance, or alternatively giving excessive depth that is not practical or useful for the task.  \n",
    "\n",
    "        ### Code: Contextualised  \n",
    "        **Definition:** Highlights the limitation of AI feedback when it lacks alignment with assignment instructions, rubrics, or disciplinary expectations. Students value feedback that is situated in the academic context, which AI cannot fully replicate.  \n",
    "\n",
    "        ### Code: Utility  \n",
    "        **Definition:** Concerns the practical usefulness of AI-generated feedback. Students report that AI often fails to provide relevant or actionable guidance, limiting its effectiveness for improving their work.  \n",
    "\n",
    "        ### Code: Trustworthy  \n",
    "        **Definition:** Reflects doubts about the accuracy, reliability, and legitimacy of AI feedback. Students question the competence of AI as a source of evaluation and express distrust due to perceived biases or lack of accountability.  \n",
    "\n",
    "        ## Sub-theme: Tone  \n",
    "\n",
    "        ### Code: Positivity  \n",
    "        **Definition:** Identifies AI’s tendency to adopt an overly agreeable or affirming tone. Feedback is seen as flattering or “people-pleasing,” potentially at the expense of critical or constructive input.  \n",
    "\n",
    "        ---\n",
    "\n",
    "        # Theme: Relational  \n",
    "\n",
    "        ## Sub-theme: Relational  \n",
    "\n",
    "        ### Code: Personal  \n",
    "        **Definition:** Emphasises the value students place on human interaction and relational qualities in feedback. They view feedback as more meaningful when it comes from people who understand them personally, can express emotion, and provide embodied, human connection.  \n",
    "\n",
    "        ### Code: Expert  \n",
    "        **Definition:** Reflects the importance of feedback coming from recognised experts, such as tutors or lecturers. Students perceive human expertise as superior to AI, due to its grounding in disciplinary knowledge and professional experience.  \n",
    "\n",
    "        ---\n",
    "\n",
    "        # Theme: Value  \n",
    "\n",
    "        ## Sub-theme: Value  \n",
    "\n",
    "        ### Code: Preference  \n",
    "        **Definition:** Expresses a general preference for not using AI for feedback, often without detailed justification. Students report personal dislike or lack of interest in AI as a feedback source.  \n",
    "\n",
    "        ### Code: Need  \n",
    "        **Definition:** Indicates that students feel no necessity for AI feedback, either because their existing human feedback sources are sufficient or because they are confident in their own skills.  \n",
    "\n",
    "        ### Code: Unsustainable  \n",
    "        **Definition:** Reflects ethical or environmental concerns about the use of AI. Students reject AI feedback due to its perceived ecological cost or association with unsustainable and unethical practices.  \n",
    "\n",
    "        ### Code: Privacy  \n",
    "        **Definition:** Concerns about the security and confidentiality of student work when submitted to AI platforms. Students fear loss of control over their data, possible misuse, or breaches of academic confidentiality.  \n",
    "\n",
    "        ### Code: Integrity  \n",
    "        **Definition:** Highlights a strong commitment to academic honesty and originality. Students reject AI feedback to avoid compromising their learning, breaking institutional rules, or engaging in behaviour perceived as unethical or academically dishonest.  \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "                \n",
    "OUTPUT_FORMAT_43 =  \"\"\"\n",
    "\n",
    "    {\n",
    "        \"reasons\": [\n",
    "            {   \n",
    "                \"theme\": \"...\",\n",
    "                \"sub_theme\": \"...\",\n",
    "                \"code\": \"...\",\n",
    "                \"reasoning\": \"...\"\n",
    "            }\n",
    "        ]\n",
    "    }                        \n",
    "                    \n",
    "                    \n",
    "                    \"\"\"\n",
    "                    \n",
    "SHOTS_43 = \"\"\"\n",
    "\n",
    "        Examples of good output:\n",
    "\n",
    "        Example 1:\n",
    "\n",
    "        Question: \"Why didn’t you use GenAI for feedback on your work?\"\n",
    "\n",
    "        Student response: \"I didn’t even know that this could be done. I also prefer to use my brain more to have creativity and originality in my work, studies and personal life.\"\n",
    "\n",
    "        Good output:\n",
    "\n",
    "        {\n",
    "            \"reasons\": [\n",
    "                {   \n",
    "                    \"theme\": \"Processes\",\n",
    "                    \"sub_theme\": \"Access\",\n",
    "                    \"code\": \"Unaware\",\n",
    "                    \"reasoning\": \"The clause 'I didn’t even know that this could be done' is a direct admission of not knowing GenAI can provide feedback. The Unaware code explicitly covers respondents who did not know GenAI could do this or how to use it, so the statement maps precisely to Processes → Access → Unaware.\"\n",
    "                },\n",
    "                {\n",
    "                    \"theme\": \"Value\",\n",
    "                    \"sub_theme\": \"Value\",\n",
    "                    \"code\": \"Preference\",\n",
    "                    \"reasoning\": \"The student states a normative stance—'I prefer to use my brain… for creativity and originality'—which indicates an intentional choice to avoid GenAI irrespective of its capabilities. This aligns with the Preference code, which captures a stated preference not to use GenAI without invoking a technical limitation.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        Example 2:\n",
    "\n",
    "        Question: \"Why didn’t you use GenAI for feedback on your work?\"\n",
    "        Student response: \"I don't believe GenAI can provide more valuable feedback than I could get from a person proofreading it. I would rather ask a friend or family member to read what I've written.\"\n",
    "        Good output:\n",
    "        {\n",
    "            \"reasons\": [\n",
    "                {   \n",
    "                    \"theme\": \"Information\",\n",
    "                    \"sub_theme\": \"Quality\",\n",
    "                    \"code\": \"Trustworthy\",\n",
    "                    \"reasoning\": \"By asserting 'I don't believe GenAI can provide more valuable feedback' and preferring human proofreaders, the student signals skepticism about GenAI’s credibility and evaluative competence. The Trustworthy code captures doubts about accuracy/reliability of information or the source’s ability to provide sound feedback, which is exactly what this statement conveys.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "CODING_STRATEGY_43 = \"\"\"\n",
    "\n",
    "                        1) Scope & Principle\n",
    "\n",
    "                        Goal: Identify explicit reasons for not using GenAI.\n",
    "\n",
    "                        Surface-level only: Code the direct, literal meaning; do not infer beyond what is written.\n",
    "\n",
    "                        2) Chunking\n",
    "\n",
    "                        Split the response into meaningful chunks (sentence or key clause).\n",
    "\n",
    "                        Assign one code per chunk. If multiple codes could apply, choose the most specific.\n",
    "\n",
    "                        3) Required Fields\n",
    "\n",
    "                        For each usable chunk, produce one reason with:\n",
    "\n",
    "                        theme: Processes | Information | Tone | Relational | Value\n",
    "\n",
    "                        sub_theme: Use the correct sub-theme; if not stated, repeat the theme (i.e., sub_theme = theme).\n",
    "\n",
    "                        code: The specific characteristic (e.g., Unaware, Effortful, Less effort, Contextualised, Trustworthy, Privacy, Integrity, Unsustainable, Personal, Expert, Preference, Need, Positivity).\n",
    "\n",
    "                        reasoning: Brief justification that quotes or precisely references the triggering phrase(s) and ties them to the code definition.\n",
    "                        \n",
    "                        4) Usability Rules\n",
    "\n",
    "                        Usable: The chunk clearly states a reason for non-use (e.g., didn’t know, too hard/time, don’t trust, lacks context, not helpful, prefer human, no need, privacy, integrity, environmental concerns, style dislike).\n",
    "\n",
    "                        UNUSABLE: Unintelligible, off-topic, or no reason stated.\n",
    "\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcc0bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "\n",
    "    You're an expert in the field of students’ experience analyzing a student's response to a survey. Your task is to code the student's response following the coding strategy provided.\n",
    "\n",
    "    The aim of the research is {aim_research}\n",
    "\n",
    "    The coding strategy is {CODING_STRATEGY}\n",
    "\n",
    "    The code for the actors is {actor_codes}\n",
    "\n",
    "    The code for the comparators is {comparator_codes}\n",
    "\n",
    "    The code for the characteristics is {characteristic_codes_v2}\n",
    "\n",
    "    The output format is {OUTPUT_FORMAT}\n",
    "\n",
    "    Some examples of good outputs are:\n",
    "\n",
    "    {SHOTS}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "system_prompt_43 = f\"\"\"\n",
    "\n",
    "    You're an expert in the field of students’ experience analyzing a student's response to a survey. Your task is to code the student's response following the coding strategy provided.\n",
    "\n",
    "    The aim of the research is {aim_research}\n",
    "\n",
    "    The coding strategy is {CODING_STRATEGY_43}\n",
    "\n",
    "    The code for the themes is {reasons_codes_v2}\n",
    "\n",
    "    The output format is {OUTPUT_FORMAT_43}\n",
    "\n",
    "    Some examples of good outputs are:\n",
    "\n",
    "    {SHOTS_43}\n",
    "\"\"\"\n",
    "\n",
    "initial_prompt = f\"\"\"\n",
    "\n",
    "    Based on the coding framework, the coding strategy, the output format, and the shots, analyse the student's response.\n",
    "    \n",
    "    Code all the responses, and output the results in the format {OUTPUT_FORMAT}.\n",
    "    \n",
    "    Keep the order of the chunks as they are in the student's response. If you code several chunks, keep the order of the chunks as they are in the student's response.\n",
    "    \n",
    "    JUST RETURN THE DESIRED OUTPUT, DO NOT ADD ANYTHING ELSE.\n",
    "    \"Return a single valid JSON object ONLY. \"\n",
    "    \"No markdown, no backticks, no prose. \"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "initial_prompt_43 = f\"\"\"\n",
    "\n",
    "    Based on the coding framework, the coding strategy, the output format, and the shots, analyse the student's response.\n",
    "    \n",
    "    Code all the responses, and output the results in the format {OUTPUT_FORMAT_43}.\n",
    "    \n",
    "    Keep the order of the chunks as they are in the student's response. If you code several chunks, keep the order of the chunks as they are in the student's response.\n",
    "    \n",
    "    JUST RETURN THE DESIRED OUTPUT, DO NOT ADD ANYTHING ELSE.\n",
    "    \"Return a single valid JSON object ONLY. \"\n",
    "    \"No markdown, no backticks, no prose. \"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fef83c",
   "metadata": {},
   "source": [
    "## 2. API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2817358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from openai import AzureOpenAI\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "OPENAI_SECRET_ACCESS_KEY = os.getenv(\"OPENAI_SECRET_ACCESS_KEY\")\n",
    "OPENAI_DEFAULT_REGION = \"australiaeast\"\n",
    "AZURE_OPENAI_ENDPOINT = \"https://cic-topic-modeling.openai.azure.com/\"\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = \"chat\"\n",
    "SET_TEMP = 0.00000001\n",
    "\n",
    "\n",
    "model_azure = AzureChatOpenAI(\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=\"2024-05-01-preview\",\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    model = \"gpt-4-32k\", #gpt-4o model by default\n",
    "    temperature = SET_TEMP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc16c5",
   "metadata": {},
   "source": [
    "## 3. Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59944993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentic Coding System with LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from typing import TypedDict, Annotated, List, Dict, Any, Optional\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import json, re\n",
    "from typing import Tuple, Any\n",
    "import pathlib\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "SAFE_CHARS = f\"-_.() {string.ascii_letters}{string.digits}\"\n",
    "\n",
    "def _slugify(text: str) -> str:\n",
    "    \"\"\"Filesystem-friendly slug.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return \"\".join(c if c in SAFE_CHARS else \"_\" for c in text).strip(\"_\")\n",
    "\n",
    "def _ensure_dir(p: pathlib.Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _save_json(obj: dict, file_path: pathlib.Path):\n",
    "    _ensure_dir(file_path.parent)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "_CODEFENCE_RE = re.compile(r\"^\\s*```(?:json)?\\s*(.*?)\\s*```\\s*$\", re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "def _strip_codefences(s: str) -> str:\n",
    "    m = _CODEFENCE_RE.match(s)\n",
    "    return m.group(1).strip() if m else s\n",
    "\n",
    "def _extract_json_segment(s: str) -> str:\n",
    "    \"\"\"Grab the innermost JSON-looking block if extra text surrounds it.\"\"\"\n",
    "    s = s.strip()\n",
    "    # Prefer object\n",
    "    lo, ro = s.find(\"{\"), s.rfind(\"}\")\n",
    "    if 0 <= lo < ro:\n",
    "        return s[lo:ro+1]\n",
    "    # Fallback to array\n",
    "    la, ra = s.find(\"[\"), s.rfind(\"]\")\n",
    "    if 0 <= la < ra:\n",
    "        return s[la:ra+1]\n",
    "    return s  # last resort\n",
    "\n",
    "def _soft_fixes(s: str) -> str:\n",
    "    # Normalize smart quotes\n",
    "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "    # Remove trailing commas before } or ]\n",
    "    s = re.sub(r\",\\s*([}\\]])\", r\"\\1\", s)\n",
    "    return s\n",
    "\n",
    "def parse_iteration_json(raw: str) -> Tuple[Any, str]:\n",
    "    \"\"\"\n",
    "    Returns (obj, error). If parsing fails, obj=None and error explains why.\n",
    "    Accepts outputs with code fences, pre/post text, or minor JSON slop.\n",
    "    \"\"\"\n",
    "    candidate = _strip_codefences(raw)\n",
    "    candidate = _extract_json_segment(candidate)\n",
    "    candidate = _soft_fixes(candidate)\n",
    "    try:\n",
    "        return json.loads(candidate), \"\"\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None, f\"JSON parse error: {e}\"\n",
    "    \n",
    "def dump_iteration_to_file(base_dir: str,\n",
    "                           response_id: str,\n",
    "                           question_key: str,\n",
    "                           phase: str,      # \"initial\" or f\"feedback_{k}\"\n",
    "                           index: int,      # 1..N within that phase\n",
    "                           result: dict):\n",
    "    rid = _slugify(response_id or \"NO_ID\")\n",
    "    qkey = _slugify(question_key or \"NO_Q\")\n",
    "    folder = pathlib.Path(base_dir) / \"iterations\" / f\"{rid}_{qkey}\" / phase\n",
    "    filename = folder / f\"iter{index}.json\"\n",
    "    _save_json(result, filename)\n",
    "\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for the agentic coding system\"\"\"\n",
    "    # Input data\n",
    "    response_id: str\n",
    "    question_key: str\n",
    "    question_data: Dict[str, Any]\n",
    "    is_question_43: bool\n",
    "    \n",
    "    # Iteration results\n",
    "    iteration_results: List[Dict[str, Any]]\n",
    "    final_codes: Optional[Dict[str, Any]]\n",
    "    \n",
    "    # Conversation history for feedback loops\n",
    "    conversation_history: Annotated[List, add_messages]\n",
    "    \n",
    "    # Agreement metrics\n",
    "    agreement_metrics: Dict[str, Any]\n",
    "    \n",
    "    # Researcher feedback\n",
    "    researcher_feedback: Optional[Dict[str, Any]]\n",
    "    \n",
    "    # Control flow\n",
    "    max_iterations: int\n",
    "    current_iteration: int\n",
    "    is_complete: bool\n",
    "    needs_feedback: bool\n",
    "    \n",
    "    # Agents (will be added dynamically)\n",
    "    orchestrator: Optional[Any]\n",
    "    researcher: Optional[Any]\n",
    "    agreement_calculator: Optional[Any]\n",
    "    log_file: Optional[str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84fe75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedIterationCoder:\n",
    "    \"\"\"Enhanced IterationCoder that handles feedback loops and conversation history\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, model):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "    \n",
    "    def code_response(self, state: AgentState) -> Dict[str, Any]:\n",
    "        \"\"\"Code a response with conversation history support\"\"\"\n",
    "        question_data = state[\"question_data\"]\n",
    "        is_question_43 = state[\"is_question_43\"]\n",
    "        conversation_history = state.get(\"conversation_history\", [])\n",
    "        \n",
    "        # Prepare system and initial messages\n",
    "        if is_question_43:\n",
    "            system_msg = system_prompt_43\n",
    "            initial_msg = f\"{initial_prompt_43}\\n\\nQuestion: {question_data['Question']}\\n\\nStudent response: {question_data['Response']}\"\n",
    "        else:\n",
    "            system_msg = system_prompt\n",
    "            initial_msg = f\"{initial_prompt}\\n\\nQuestion: {question_data['Question']}\\n\\nStudent response: {question_data['Response']}\"\n",
    "        \n",
    "        # Build conversation with history\n",
    "        messages = [SystemMessage(content=system_msg)]\n",
    "        \n",
    "        # Add conversation history if available\n",
    "        if conversation_history:\n",
    "            messages.extend(conversation_history)\n",
    "        else:\n",
    "            # First iteration - add initial message\n",
    "            messages.append(HumanMessage(content=initial_msg))\n",
    "        \n",
    "        try:\n",
    "            response = self.model.invoke(messages)\n",
    "            return {\n",
    "                \"iteration_name\": self.name,\n",
    "                \"raw_response\": response.content,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in {self.name}: {str(e)}\")\n",
    "            return {\n",
    "                \"iteration_name\": self.name,\n",
    "                \"raw_response\": f\"Error: {str(e)}\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"error\": True\n",
    "            }\n",
    "    \n",
    "    def code_with_feedback(self, state: AgentState, feedback: str) -> Dict[str, Any]:\n",
    "        \"\"\"Code with researcher feedback\"\"\"\n",
    "        question_data = state[\"question_data\"]\n",
    "        is_question_43 = state[\"is_question_43\"]\n",
    "        conversation_history = state.get(\"conversation_history\", [])\n",
    "        \n",
    "        # Prepare system message\n",
    "        if is_question_43:\n",
    "            system_msg = system_prompt_43\n",
    "        else:\n",
    "            system_msg = system_prompt\n",
    "        \n",
    "        # Build conversation with history and feedback\n",
    "        messages = [SystemMessage(content=system_msg)]\n",
    "        messages.extend(conversation_history)\n",
    "        \n",
    "        # Add feedback message\n",
    "        feedback_msg = f\"\"\"Based on the researcher's feedback, please recode the response:\n",
    "\n",
    "                            Original Question: {question_data['Question']}\n",
    "                            Original Student Response: {question_data['Response']}\n",
    "\n",
    "                            Researcher Feedback: {feedback}\n",
    "\n",
    "                            Please provide your coding in the specified output format: {OUTPUT_FORMAT if not is_question_43 else OUTPUT_FORMAT_43}\n",
    "\n",
    "                            JUST RETURN THE DESIRED OUTPUT, DO NOT ADD ANYTHING ELSE.\n",
    "                            Return a single valid JSON object ONLY.\n",
    "                            No markdown, no backticks, no prose.\"\"\"\n",
    "        \n",
    "        messages.append(HumanMessage(content=feedback_msg))\n",
    "        \n",
    "        try:\n",
    "            response = self.model.invoke(messages)\n",
    "            return {\n",
    "                \"iteration_name\": self.name,\n",
    "                \"raw_response\": response.content,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"with_feedback\": True\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in {self.name} with feedback: {str(e)}\")\n",
    "            return {\n",
    "                \"iteration_name\": self.name,\n",
    "                \"raw_response\": f\"Error: {str(e)}\",\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"error\": True,\n",
    "                \"with_feedback\": True\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c10d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrchestratorAgent:\n",
    "    \"\"\"Orchestrator agent that manages the coding workflow and makes final decisions\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.iteration_coders = [\n",
    "            EnhancedIterationCoder(\"Iteration1\", model),\n",
    "            EnhancedIterationCoder(\"Iteration2\", model),\n",
    "            EnhancedIterationCoder(\"Iteration3\", model)\n",
    "        ]\n",
    "    \n",
    "    def run_initial_iterations(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Run the initial three iterations\"\"\"\n",
    "        logger.info(f\"Running initial iterations for {state['response_id']} - {state['question_key']}\")\n",
    "        \n",
    "        iteration_results = []\n",
    "        conversation_history = []\n",
    "        \n",
    "        for coder in self.iteration_coders:\n",
    "            result = coder.code_response(state)\n",
    "            iteration_results.append(result)\n",
    "            \n",
    "            # Parse the response\n",
    "            parsed_obj, error = parse_iteration_json(result[\"raw_response\"])\n",
    "            result[\"parsed_response\"] = parsed_obj\n",
    "            result[\"parse_error\"] = error\n",
    "            \n",
    "            # Add to conversation history\n",
    "            if not result.get(\"error\", False):\n",
    "                conversation_history.append(HumanMessage(content=result[\"raw_response\"]))\n",
    "        \n",
    "        # Update state\n",
    "        state[\"iteration_results\"] = iteration_results\n",
    "        state[\"conversation_history\"] = conversation_history\n",
    "        state[\"current_iteration\"] = 1\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def make_final_decision(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Make final decision on codes based on iteration results\"\"\"\n",
    "        logger.info(f\"Making final decision for {state['response_id']} - {state['question_key']}\")\n",
    "        \n",
    "        iteration_results = state[\"iteration_results\"]\n",
    "        valid_results = [r for r in iteration_results if not r.get(\"error\", False) and r.get(\"parsed_response\")]\n",
    "        \n",
    "        if not valid_results:\n",
    "            logger.warning(\"No valid iteration results found\")\n",
    "            state[\"final_codes\"] = {\"error\": \"No valid coding results\"}\n",
    "            return state\n",
    "        \n",
    "        # Use the orchestrator model to make final decision\n",
    "        decision_prompt = f\"\"\"As an expert researcher, you need to make a final decision on the coding for this question based on three iterations.\n",
    "\n",
    "                            Question: {state['question_data']['Question']}\n",
    "                            Student Response: {state['question_data']['Response']}\n",
    "\n",
    "                            Iteration Results:\n",
    "                            \"\"\"\n",
    "                                    \n",
    "        for i, result in enumerate(valid_results, 1):\n",
    "            decision_prompt += f\"\\nIteration {i}:\\n{json.dumps(result['parsed_response'], indent=2)}\\n\"\n",
    "        \n",
    "        decision_prompt += f\"\"\"\n",
    "\n",
    "                            Please analyze these iterations and provide the final coding decision. Consider:\n",
    "                            1. Consistency across iterations\n",
    "                            2. Quality of reasoning\n",
    "                            3. Adherence to coding framework\n",
    "\n",
    "                            Provide your final decision in the same format: {OUTPUT_FORMAT if not state['is_question_43'] else OUTPUT_FORMAT_43}\n",
    "\n",
    "                            JUST RETURN THE DESIRED OUTPUT, DO NOT ADD ANYTHING ELSE.\n",
    "                            Return a single valid JSON object ONLY.\n",
    "                            No markdown, no backticks, no prose.\n",
    "                            \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.model.invoke([HumanMessage(content=decision_prompt)])\n",
    "            final_obj, error = parse_iteration_json(response.content)\n",
    "            \n",
    "            state[\"final_codes\"] = final_obj if final_obj else {\"_raw\": response.content, \"_error\": error}\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in final decision: {str(e)}\")\n",
    "            state[\"final_codes\"] = {\"error\": str(e)}\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def handle_feedback_loop(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Handle feedback loop when researcher disagrees\"\"\"\n",
    "        logger.info(f\"Handling feedback loop for {state['response_id']} - {state['question_key']}\")\n",
    "        \n",
    "        feedback = state[\"researcher_feedback\"][\"feedback\"]\n",
    "        \n",
    "        # Run new iterations with feedback\n",
    "        new_iteration_results = []\n",
    "        conversation_history = state.get(\"conversation_history\", [])\n",
    "        \n",
    "        for coder in self.iteration_coders:\n",
    "            result = coder.code_with_feedback(state, feedback)\n",
    "            new_iteration_results.append(result)\n",
    "            \n",
    "            # Parse the response\n",
    "            parsed_obj, error = parse_iteration_json(result[\"raw_response\"])\n",
    "            result[\"parsed_response\"] = parsed_obj\n",
    "            result[\"parse_error\"] = error\n",
    "            \n",
    "            # Add to conversation history\n",
    "            if not result.get(\"error\", False):\n",
    "                conversation_history.append(HumanMessage(content=result[\"raw_response\"]))\n",
    "        \n",
    "        # Update state\n",
    "        state[\"iteration_results\"] = new_iteration_results\n",
    "        state[\"conversation_history\"] = conversation_history\n",
    "        state[\"current_iteration\"] += 1\n",
    "        state[\"needs_feedback\"] = False\n",
    "        state[\"researcher_feedback\"] = None\n",
    "        \n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34807620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearcherAgent:\n",
    "    \"\"\"Researcher agent for validation and feedback generation\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def validate_coding(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Validate the final coding and provide feedback\"\"\"\n",
    "        logger.info(f\"Validating coding for {state['response_id']} - {state['question_key']}\")\n",
    "        \n",
    "        question_data = state[\"question_data\"]\n",
    "        final_codes = state[\"final_codes\"]\n",
    "        iteration_results = state[\"iteration_results\"]\n",
    "        \n",
    "        # Prepare validation prompt\n",
    "        validation_prompt = f\"\"\"As a senior researcher in the field of students’ experience, you need to validate the final coding decision for this question.\n",
    "\n",
    "                            ORIGINAL CONTEXT:\n",
    "                            Question: {question_data['Question']}\n",
    "                            Student Response: {question_data['Response']}\n",
    "\n",
    "                            FINAL CODING DECISION:\n",
    "                            {json.dumps(final_codes, indent=2)}\n",
    "\n",
    "                            ORIGINAL ITERATION RESULTS:\n",
    "                            \"\"\"\n",
    "        \n",
    "        for i, result in enumerate(iteration_results, 1):\n",
    "            validation_prompt += f\"\\nIteration {i}:\\n{json.dumps(result.get('parsed_response', {}), indent=2)}\\n\"\n",
    "        \n",
    "        validation_prompt += \"\"\"\n",
    "\n",
    "                            VALIDATION CRITERIA:\n",
    "                            Please evaluate the final coding decision against the original question and student response.\n",
    "\n",
    "                            Consider:\n",
    "                            1. Accuracy of coding according to the framework\n",
    "                            2. Consistency with iteration results\n",
    "                            3. Quality of reasoning\n",
    "                            4. Completeness of analysis\n",
    "                            5. Adherence to coding strategy\n",
    "                            6. Whether the coding properly addresses the original question\n",
    "                            7. Whether the coding captures the student's response accurately\n",
    "\n",
    "                            Please provide your validation in the following JSON format:\n",
    "                            {\n",
    "                                \"agreement\": true/false,\n",
    "                                \"quality_score\": 1-5,\n",
    "                                \"feedback\": \"Detailed feedback explaining your decision\",\n",
    "                                \"discrepancies\": [\"list of major issues if any\"],\n",
    "                                \"recommendations\": [\"list of suggestions for improvement if any\"]\n",
    "                            }\n",
    "\n",
    "                            JUST RETURN THE DESIRED OUTPUT, DO NOT ADD ANYTHING ELSE.\n",
    "                            Return a single valid JSON object ONLY.\n",
    "                            No markdown, no backticks, no prose.\n",
    "                            \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.model.invoke([HumanMessage(content=validation_prompt)])\n",
    "            validation_obj, error = parse_iteration_json(response.content)\n",
    "            \n",
    "            if validation_obj:\n",
    "                state[\"researcher_feedback\"] = validation_obj\n",
    "                state[\"needs_feedback\"] = not validation_obj.get(\"agreement\", False)\n",
    "            else:\n",
    "                logger.error(f\"Failed to parse researcher validation: {error}\")\n",
    "                state[\"researcher_feedback\"] = {\"agreement\": True, \"error\": error}\n",
    "                state[\"needs_feedback\"] = False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in researcher validation: {str(e)}\")\n",
    "            state[\"researcher_feedback\"] = {\"agreement\": True, \"error\": str(e)}\n",
    "            state[\"needs_feedback\"] = False\n",
    "        \n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed7e5c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgreementCalculator:\n",
    "    \"\"\"Calculate agreement metrics between iterations and human coding\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def safe_eval_list(self, x):\n",
    "        \"\"\"Ensures that string values like \"['a','b']\" are converted into Python lists.\"\"\"\n",
    "        if isinstance(x, list):\n",
    "            return x\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except:\n",
    "                return [x]\n",
    "        return []\n",
    "    \n",
    "    def jaccard_similarity(self, list1, list2):\n",
    "        \"\"\"Calculate Jaccard similarity between two lists\"\"\"\n",
    "        set1, set2 = set(list1), set(list2)\n",
    "        if not set1 and not set2:\n",
    "            return 1.0  # both empty → perfect agreement\n",
    "        return len(set1 & set2) / len(set1 | set2)\n",
    "    \n",
    "    def calculate_iteration_agreement(self, iteration_results: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate agreement between iterations\"\"\"\n",
    "        if len(iteration_results) < 2:\n",
    "            return {\"agreement_score\": 0.0}\n",
    "        \n",
    "        # Extract characteristics from each iteration\n",
    "        iteration_characteristics = []\n",
    "        for result in iteration_results:\n",
    "            if result.get(\"parsed_response\") and \"codes\" in result[\"parsed_response\"]:\n",
    "                codes = result[\"parsed_response\"][\"codes\"]\n",
    "                characteristics = [code.get(\"code\", \"\") for code in codes if code.get(\"code\")]\n",
    "                iteration_characteristics.append(characteristics)\n",
    "            else:\n",
    "                iteration_characteristics.append([])\n",
    "        \n",
    "        # Calculate pairwise agreements\n",
    "        agreements = []\n",
    "        for i in range(len(iteration_characteristics)):\n",
    "            for j in range(i + 1, len(iteration_characteristics)):\n",
    "                agreement = self.jaccard_similarity(\n",
    "                    iteration_characteristics[i], \n",
    "                    iteration_characteristics[j]\n",
    "                )\n",
    "                agreements.append(agreement)\n",
    "        \n",
    "        avg_agreement = sum(agreements) / len(agreements) if agreements else 0.0\n",
    "        \n",
    "        return {\n",
    "            \"agreement_score\": avg_agreement,\n",
    "            \"pairwise_agreements\": agreements,\n",
    "            \"iteration_characteristics\": iteration_characteristics\n",
    "        }\n",
    "    \n",
    "    def calculate_human_agreement(self, final_codes: Dict, human_codes: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate agreement between final codes and human coding\"\"\"\n",
    "        if not final_codes or \"codes\" not in final_codes:\n",
    "            return {\"human_agreement\": 0.0}\n",
    "        \n",
    "        # Extract characteristics from final codes\n",
    "        final_characteristics = [code.get(\"code\", \"\") for code in final_codes[\"codes\"] if code.get(\"code\")]\n",
    "        \n",
    "        # Extract characteristics from human codes\n",
    "        human_characteristics = []\n",
    "        for human_code in human_codes:\n",
    "            if isinstance(human_code, dict):\n",
    "                characteristic = human_code.get(\"Characteristic\", \"\")\n",
    "                if characteristic:\n",
    "                    human_characteristics.append(characteristic)\n",
    "        \n",
    "        agreement = self.jaccard_similarity(final_characteristics, human_characteristics)\n",
    "        \n",
    "        return {\n",
    "            \"human_agreement\": agreement,\n",
    "            \"final_characteristics\": final_characteristics,\n",
    "            \"human_characteristics\": human_characteristics\n",
    "        }\n",
    "    \n",
    "    def log_agreement_metrics(self, state: AgentState, log_file: str):\n",
    "        \"\"\"Log agreement metrics to file\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        response_id = state[\"response_id\"]\n",
    "        question_key = state[\"question_key\"]\n",
    "        \n",
    "        # Calculate iteration agreement\n",
    "        iteration_agreement = self.calculate_iteration_agreement(state[\"iteration_results\"])\n",
    "        \n",
    "        # Calculate human agreement if available\n",
    "        human_agreement = {\"human_agreement\": 0.0}  # Will be updated if human codes available\n",
    "        \n",
    "        log_entry = {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"response_id\": response_id,\n",
    "            \"question_key\": question_key,\n",
    "            \"iteration\": state[\"current_iteration\"],\n",
    "            \"iteration_agreement\": iteration_agreement,\n",
    "            \"human_agreement\": human_agreement,\n",
    "            \"researcher_feedback\": state.get(\"researcher_feedback\", {}),\n",
    "            \"needs_feedback\": state.get(\"needs_feedback\", False)\n",
    "        }\n",
    "        \n",
    "        # Append to log file\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(log_entry) + \"\\n\")\n",
    "        \n",
    "        # Update state\n",
    "        state[\"agreement_metrics\"] = {\n",
    "            \"iteration_agreement\": iteration_agreement,\n",
    "            \"human_agreement\": human_agreement,\n",
    "            \"timestamp\": timestamp\n",
    "        }\n",
    "        \n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "277710d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph Node Functions\n",
    "def run_initial_iterations_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for running initial iterations\"\"\"\n",
    "    orchestrator = state.get(\"orchestrator\")\n",
    "    if not orchestrator:\n",
    "        raise ValueError(\"Orchestrator not found in state\")\n",
    "    \n",
    "    return orchestrator.run_initial_iterations(state)\n",
    "\n",
    "def make_final_decision_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for making final decision\"\"\"\n",
    "    orchestrator = state.get(\"orchestrator\")\n",
    "    if not orchestrator:\n",
    "        raise ValueError(\"Orchestrator not found in state\")\n",
    "    \n",
    "    return orchestrator.make_final_decision(state)\n",
    "\n",
    "def validate_coding_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for researcher validation\"\"\"\n",
    "    researcher = state.get(\"researcher\")\n",
    "    if not researcher:\n",
    "        raise ValueError(\"Researcher not found in state\")\n",
    "    \n",
    "    return researcher.validate_coding(state)\n",
    "\n",
    "def handle_feedback_loop_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for handling feedback loop\"\"\"\n",
    "    orchestrator = state.get(\"orchestrator\")\n",
    "    if not orchestrator:\n",
    "        raise ValueError(\"Orchestrator not found in state\")\n",
    "    \n",
    "    return orchestrator.handle_feedback_loop(state)\n",
    "\n",
    "def log_metrics_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Node function for logging agreement metrics\"\"\"\n",
    "    calculator = state.get(\"agreement_calculator\")\n",
    "    log_file = state.get(\"log_file\", \"agreement_logs.txt\")\n",
    "    \n",
    "    if not calculator:\n",
    "        raise ValueError(\"Agreement calculator not found in state\")\n",
    "    \n",
    "    return calculator.log_agreement_metrics(state, log_file)\n",
    "\n",
    "# Conditional routing functions\n",
    "def should_continue_workflow(state: AgentState) -> str:\n",
    "    \"\"\"Determine if workflow should continue or end\"\"\"\n",
    "    if state.get(\"is_complete\", False):\n",
    "        return \"end\"\n",
    "    elif state.get(\"needs_feedback\", False) and state.get(\"current_iteration\", 0) < state.get(\"max_iterations\", 3):\n",
    "        return \"feedback_loop\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "def should_validate(state: AgentState) -> str:\n",
    "    \"\"\"Determine if validation should proceed\"\"\"\n",
    "    if state.get(\"final_codes\"):\n",
    "        return \"validate\"\n",
    "    else:\n",
    "        return \"end\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9d4be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_conversation_history_to_serializable(conversation_history):\n",
    "    \"\"\"Convert conversation history to JSON-serializable format\"\"\"\n",
    "    serializable_history = []\n",
    "    for msg in conversation_history:\n",
    "        if hasattr(msg, 'content'):\n",
    "            serializable_history.append({\n",
    "                \"type\": type(msg).__name__,\n",
    "                \"content\": msg.content\n",
    "            })\n",
    "        else:\n",
    "            serializable_history.append(str(msg))\n",
    "    return serializable_history\n",
    "\n",
    "# Now let's create a fixed version of the process_question_simple method\n",
    "class FixedAgenticCodingSystemV2:\n",
    "    def __init__(self, model, log_file: str = \"agreement_logs.txt\", output_dir: str = \"agentic_coding_results\"):\n",
    "        self.model = model\n",
    "        self.log_file = log_file\n",
    "        self.output_dir = output_dir\n",
    "        self.orchestrator = OrchestratorAgent(model)\n",
    "        self.researcher = ResearcherAgent(model)\n",
    "        self.agreement_calculator = AgreementCalculator()\n",
    "    \n",
    "    def process_question_simple(self, response_id: str, question_key: str, question_data: Dict[str, Any], \n",
    "                               max_iterations: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single question through the agentic system (simplified version)\"\"\"\n",
    "        \n",
    "        # Determine if it's question 43\n",
    "        question_num = question_key.split(\" \")[1] if \" \" in question_key else question_key\n",
    "        is_question_43 = (question_num == \"43\")\n",
    "        \n",
    "        logger.info(f\"Processing {response_id} - {question_key}\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Run initial iterations\n",
    "            logger.info(\"Running initial iterations...\")\n",
    "            iteration_results = []\n",
    "            conversation_history = []\n",
    "            iteration_runs = []  # NEW: list of rounds [{\"phase\": \"initial\"|\"feedback_k\", \"results\":[...], \"feedback_text\": \"...\"}]\n",
    "\n",
    "            phase_label = \"initial\"\n",
    "            round_bucket = {\"phase\": phase_label, \"results\": []}\n",
    "\n",
    "            for i, coder in enumerate(self.orchestrator.iteration_coders, start=1):\n",
    "                result = coder.code_response({\n",
    "                    \"question_data\": question_data,\n",
    "                    \"is_question_43\": is_question_43,\n",
    "                    \"conversation_history\": conversation_history\n",
    "                })\n",
    "                # parse\n",
    "                parsed_obj, error = parse_iteration_json(result.get(\"raw_response\", \"\"))\n",
    "                result[\"parsed_response\"] = parsed_obj\n",
    "                result[\"parse_error\"] = error\n",
    "\n",
    "                # save file immediately\n",
    "                dump_iteration_to_file(self.output_dir, response_id, question_key, phase_label, i, result)\n",
    "\n",
    "                # append to round\n",
    "                round_bucket[\"results\"].append(result)\n",
    "                iteration_results.append(result)\n",
    "\n",
    "                # store AI output in conversation history with correct role\n",
    "                if not result.get(\"error\", False):\n",
    "                    conversation_history.append(AIMessage(content=result[\"raw_response\"]))\n",
    "\n",
    "            # push the round into runs\n",
    "            iteration_runs.append(round_bucket)\n",
    "            \n",
    "            # Step 2: Log initial metrics\n",
    "            logger.info(\"Logging initial metrics...\")\n",
    "            iteration_agreement = self.agreement_calculator.calculate_iteration_agreement(iteration_results)\n",
    "            \n",
    "            # Step 3: Make final decision\n",
    "            logger.info(\"Making final decision...\")\n",
    "            valid_results = [r for r in iteration_results if not r.get(\"error\", False) and r.get(\"parsed_response\")]\n",
    "            \n",
    "            if not valid_results:\n",
    "                logger.warning(\"No valid iteration results found\")\n",
    "                final_codes = {\"error\": \"No valid coding results\"}\n",
    "            else:\n",
    "                # Use the orchestrator model to make final decision\n",
    "                decision_prompt = f\"\"\"As an expert researcher, you need to make a final decision on the coding for this question based on three iterations.\n",
    "\n",
    "Question: {question_data['Question']}\n",
    "Student Response: {question_data['Response']}\n",
    "\n",
    "Iteration Results:\n",
    "\"\"\"\n",
    "                \n",
    "                for i, result in enumerate(valid_results, 1):\n",
    "                    decision_prompt += f\"\\nIteration {i}:\\n{json.dumps(result['parsed_response'], indent=2)}\\n\"\n",
    "                \n",
    "                decision_prompt += f\"\"\"\n",
    "\n",
    "Please analyze these iterations and provide the final coding decision. Consider:\n",
    "1. Consistency across iterations\n",
    "2. Quality of reasoning\n",
    "3. Adherence to coding framework\n",
    "\n",
    "Provide your final decision in the same format: {OUTPUT_FORMAT if not is_question_43 else OUTPUT_FORMAT_43}\n",
    "\n",
    "JUST RETURN THE DESIRED OUTPUT, DO NOT ADD ANYTHING ELSE.\n",
    "Return a single valid JSON object ONLY.\n",
    "No markdown, no backticks, no prose.\"\"\"\n",
    "\n",
    "                try:\n",
    "                    response = self.model.invoke([HumanMessage(content=decision_prompt)])\n",
    "                    final_obj, error = parse_iteration_json(response.content)\n",
    "                    final_codes = final_obj if final_obj else {\"_raw\": response.content, \"_error\": error}\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in final decision: {str(e)}\")\n",
    "                    final_codes = {\"error\": str(e)}\n",
    "                    \n",
    "                _save_json(\n",
    "                    final_codes,\n",
    "                    pathlib.Path(self.output_dir)\n",
    "                    / \"iterations\"\n",
    "                    / f\"{_slugify(response_id)}_{_slugify(question_key)}\"\n",
    "                    / \"final_decision.json\"\n",
    "                )\n",
    "            \n",
    "            # Step 4: Researcher validation\n",
    "            logger.info(\"Running researcher validation...\")\n",
    "            validation_prompt = f\"\"\"As a senior researcher, you need to validate the final coding decision for this question.\n",
    "\n",
    "Question: {question_data['Question']}\n",
    "Student Response: {question_data['Response']}\n",
    "\n",
    "Final Coding Decision:\n",
    "{json.dumps(final_codes, indent=2)}\n",
    "\n",
    "Original Iteration Results:\n",
    "\"\"\"\n",
    "            \n",
    "            for i, result in enumerate(iteration_results, 1):\n",
    "                validation_prompt += f\"\\nIteration {i}:\\n{json.dumps(result.get('parsed_response', {}), indent=2)}\\n\"\n",
    "            \n",
    "            validation_prompt += \"\"\"\n",
    "\n",
    "Please provide your validation in the following JSON format:\n",
    "{\n",
    "    \"agreement\": true/false,\n",
    "    \"quality_score\": 1-5,\n",
    "    \"feedback\": \"Detailed feedback explaining your decision\",\n",
    "    \"discrepancies\": [\"list of major issues if any\"],\n",
    "    \"recommendations\": [\"list of suggestions for improvement if any\"]\n",
    "}\n",
    "\n",
    "Consider:\n",
    "1. Accuracy of coding according to the framework\n",
    "2. Consistency with iteration results\n",
    "3. Quality of reasoning\n",
    "4. Completeness of analysis\n",
    "5. Adherence to coding strategy\n",
    "\n",
    "JUST RETURN THE DESIRED OUTPUT, DO NOT ADD ANYTHING ELSE.\n",
    "Return a single valid JSON object ONLY.\n",
    "No markdown, no backticks, no prose.\"\"\"\n",
    "\n",
    "            try:\n",
    "                response = self.model.invoke([HumanMessage(content=validation_prompt)])\n",
    "                validation_obj, error = parse_iteration_json(response.content)\n",
    "                \n",
    "                if validation_obj:\n",
    "                    researcher_feedback = validation_obj\n",
    "                    needs_feedback = not validation_obj.get(\"agreement\", False)\n",
    "                else:\n",
    "                    logger.error(f\"Failed to parse researcher validation: {error}\")\n",
    "                    researcher_feedback = {\"agreement\": True, \"error\": error}\n",
    "                    needs_feedback = False\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in researcher validation: {str(e)}\")\n",
    "                researcher_feedback = {\"agreement\": True, \"error\": str(e)}\n",
    "                needs_feedback = False\n",
    "            \n",
    "            _save_json(\n",
    "                researcher_feedback,\n",
    "                pathlib.Path(self.output_dir)\n",
    "                / \"iterations\"\n",
    "                / f\"{_slugify(response_id)}_{_slugify(question_key)}\"\n",
    "                / \"researcher_validation.json\"\n",
    "            )\n",
    "            # Step 5: Handle feedback loop if needed\n",
    "            current_iteration = 1\n",
    "            feedback_round = 0\n",
    "\n",
    "            if needs_feedback and current_iteration < max_iterations:\n",
    "                logger.info(\"Handling feedback loop...\")\n",
    "                feedback = researcher_feedback.get(\"feedback\", \"\")\n",
    "                feedback_round += 1\n",
    "                phase_label = f\"feedback_{feedback_round}\"\n",
    "                round_bucket = {\"phase\": phase_label, \"results\": [], \"feedback_text\": feedback}\n",
    "\n",
    "                new_iteration_results = []\n",
    "                for i, coder in enumerate(self.orchestrator.iteration_coders, start=1):\n",
    "                    result = coder.code_with_feedback({\n",
    "                        \"question_data\": question_data,\n",
    "                        \"is_question_43\": is_question_43,\n",
    "                        \"conversation_history\": conversation_history\n",
    "                    }, feedback)\n",
    "\n",
    "                    parsed_obj, error = parse_iteration_json(result.get(\"raw_response\", \"\"))\n",
    "                    result[\"parsed_response\"] = parsed_obj\n",
    "                    result[\"parse_error\"] = error\n",
    "\n",
    "                    dump_iteration_to_file(self.output_dir, response_id, question_key, phase_label, i, result)\n",
    "\n",
    "                    round_bucket[\"results\"].append(result)\n",
    "                    new_iteration_results.append(result)\n",
    "\n",
    "                    if not result.get(\"error\", False):\n",
    "                        conversation_history.append(AIMessage(content=result[\"raw_response\"]))\n",
    "\n",
    "                iteration_runs.append(round_bucket)\n",
    "                iteration_results = new_iteration_results\n",
    "                current_iteration += 1\n",
    "\n",
    "                # Recalculate agreement on this latest round\n",
    "                iteration_agreement = self.agreement_calculator.calculate_iteration_agreement(iteration_results)\n",
    "\n",
    "            \n",
    "            # Step 6: Final logging\n",
    "            timestamp = datetime.now().isoformat()\n",
    "            log_entry = {\n",
    "                \"timestamp\": timestamp,\n",
    "                \"response_id\": response_id,\n",
    "                \"question_key\": question_key,\n",
    "                \"iteration\": current_iteration,\n",
    "                \"iteration_agreement\": iteration_agreement,\n",
    "                \"researcher_feedback\": researcher_feedback,\n",
    "                \"needs_feedback\": needs_feedback\n",
    "            }\n",
    "            \n",
    "            # Append to log file\n",
    "            with open(self.log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(log_entry) + \"\\n\")\n",
    "            \n",
    "            # Convert conversation history to JSON-serializable format\n",
    "            serializable_conversation_history = convert_conversation_history_to_serializable(conversation_history)\n",
    "            \n",
    "            total_iters = sum(len(r[\"results\"]) for r in iteration_runs)\n",
    "\n",
    "            return {\n",
    "                \"response_id\": response_id,\n",
    "                \"question_key\": question_key,\n",
    "                \"final_codes\": final_codes,\n",
    "                \"iteration_results\": iteration_results,  # latest round for backward compat\n",
    "                \"iteration_runs\": iteration_runs,        # NEW: all rounds, separated by phase\n",
    "                \"researcher_feedback\": researcher_feedback,\n",
    "                \"agreement_metrics\": {\n",
    "                    \"iteration_agreement\": iteration_agreement,\n",
    "                    \"timestamp\": timestamp\n",
    "                },\n",
    "                \"conversation_history\": serializable_conversation_history,\n",
    "                \"total_iterations\": total_iters\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {response_id} - {question_key}: {str(e)}\")\n",
    "            return {\n",
    "                \"response_id\": response_id,\n",
    "                \"question_key\": question_key,\n",
    "                \"error\": str(e),\n",
    "                \"final_codes\": None\n",
    "            }\n",
    "\n",
    "# Test the fixed system\n",
    "def test_fixed_agentic_system_v2():\n",
    "    \"\"\"Test the fixed agentic coding system v2\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize the system\n",
    "        system = FixedAgenticCodingSystemV2(model_azure, log_file=\"test_agreement_logs_fixed_v2.txt\")\n",
    "        \n",
    "        # Sample question data\n",
    "        sample_question_data = {\n",
    "            \"Question\": \"Please describe any differences in how it impacted your learning (comparing GenAI and your teacher)?\",\n",
    "            \"Response\": \"AI tends to be more uplifting or supportive than teachers, so when I ask for its opinion on something I've written it'll compliment whereas my teacher will have edits to my work. Both are useful and important.\"\n",
    "        }\n",
    "        \n",
    "        # Process the question\n",
    "        result = system.process_question_simple(\n",
    "            response_id=\"TEST_001\",\n",
    "            question_key=\"Question 40\",\n",
    "            question_data=sample_question_data,\n",
    "            max_iterations=3\n",
    "        )\n",
    "        \n",
    "        print(\"Processing Result:\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in test_fixed_agentic_system_v2: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run the fixed test\n",
    "#test_result_fixed_v2 = test_fixed_agentic_system_v2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "581f90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_surveys_fixed_v2(survey_dir: str = \"temp_survey\", output_dir: str = \"agentic_coding_results\"):\n",
    "    \"\"\"Process multiple survey files using the fixed agentic system v2\"\"\"\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Initialize the fixed system\n",
    "    system = FixedAgenticCodingSystemV2(model_azure, log_file=f\"{output_dir}/agreement_logs.txt\")\n",
    "    \n",
    "    # Get survey files\n",
    "    survey_path = Path(survey_dir)\n",
    "    survey_files = [f for f in survey_path.glob(\"*.json\") if f.name != \".DS_Store\"]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for survey_file in survey_files:\n",
    "        print(f\"Processing {survey_file.name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load survey data\n",
    "            with open(survey_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                survey_data = json.load(f)\n",
    "            \n",
    "            response_id = survey_data.get(\"ResponseId\", \"\")\n",
    "            results = []\n",
    "            \n",
    "            logger.info(f\"Processing survey file: {survey_file}\")\n",
    "            \n",
    "            # Process each question\n",
    "            for key, value in survey_data.items():\n",
    "                if key.startswith(\"Question \") and isinstance(value, dict) and \"Response\" in value:\n",
    "                    logger.info(f\"Processing {key} for response {response_id}\")\n",
    "                    \n",
    "                    result = system.process_question_simple(response_id, key, value)\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    # Save individual result\n",
    "                    result_file = Path(output_dir) / f\"{response_id}_{key.replace(' ', '_')}_result.json\"\n",
    "                    with open(result_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            # Save combined results for this response\n",
    "            combined_file = Path(output_dir) / f\"{response_id}_all_results.json\"\n",
    "            with open(combined_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            all_results.extend(results)\n",
    "            logger.info(f\"Completed processing {response_id}. Results saved to {output_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {survey_file.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save all results\n",
    "    combined_file = Path(output_dir) / \"all_agentic_results.json\"\n",
    "    with open(combined_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nCompleted processing {len(survey_files)} survey files\")\n",
    "    print(f\"Total results: {len(all_results)}\")\n",
    "    print(f\"Combined results saved to: {combined_file}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "983ec500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing survey file: temp_survey\\R_1gTNWbmwxtRxrf8.json\n",
      "INFO:__main__:Processing Question 40 for response R_1gTNWbmwxtRxrf8\n",
      "INFO:__main__:Processing R_1gTNWbmwxtRxrf8 - Question 40\n",
      "INFO:__main__:Running initial iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing R_1gTNWbmwxtRxrf8.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 41 for response R_1gTNWbmwxtRxrf8\n",
      "INFO:__main__:Processing R_1gTNWbmwxtRxrf8 - Question 41\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 42 for response R_1gTNWbmwxtRxrf8\n",
      "INFO:__main__:Processing R_1gTNWbmwxtRxrf8 - Question 42\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Completed processing R_1gTNWbmwxtRxrf8. Results saved to agentic_coding_results\n",
      "INFO:__main__:Processing survey file: temp_survey\\R_1VQ5hmwtdpRmnmu.json\n",
      "INFO:__main__:Processing Question 40 for response R_1VQ5hmwtdpRmnmu\n",
      "INFO:__main__:Processing R_1VQ5hmwtdpRmnmu - Question 40\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 39.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing R_1VQ5hmwtdpRmnmu.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 41 for response R_1VQ5hmwtdpRmnmu\n",
      "INFO:__main__:Processing R_1VQ5hmwtdpRmnmu - Question 41\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 42 for response R_1VQ5hmwtdpRmnmu\n",
      "INFO:__main__:Processing R_1VQ5hmwtdpRmnmu - Question 42\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 29.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Completed processing R_1VQ5hmwtdpRmnmu. Results saved to agentic_coding_results\n",
      "INFO:__main__:Processing survey file: temp_survey\\R_43e00PZDT3YYIZ2.json\n",
      "INFO:__main__:Processing Question 43 for response R_43e00PZDT3YYIZ2\n",
      "INFO:__main__:Processing R_43e00PZDT3YYIZ2 - Question 43\n",
      "INFO:__main__:Running initial iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing R_43e00PZDT3YYIZ2.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Completed processing R_43e00PZDT3YYIZ2. Results saved to agentic_coding_results\n",
      "INFO:__main__:Processing survey file: temp_survey\\R_4aRLJoWmhKsYLOF.json\n",
      "INFO:__main__:Processing Question 43 for response R_4aRLJoWmhKsYLOF\n",
      "INFO:__main__:Processing R_4aRLJoWmhKsYLOF - Question 43\n",
      "INFO:__main__:Running initial iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing R_4aRLJoWmhKsYLOF.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Completed processing R_4aRLJoWmhKsYLOF. Results saved to agentic_coding_results\n",
      "INFO:__main__:Processing survey file: temp_survey\\R_4AYVtTmMd0q7vte.json\n",
      "INFO:__main__:Processing Question 40 for response R_4AYVtTmMd0q7vte\n",
      "INFO:__main__:Processing R_4AYVtTmMd0q7vte - Question 40\n",
      "INFO:__main__:Running initial iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing R_4AYVtTmMd0q7vte.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 41 for response R_4AYVtTmMd0q7vte\n",
      "INFO:__main__:Processing R_4AYVtTmMd0q7vte - Question 41\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 23.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 42 for response R_4AYVtTmMd0q7vte\n",
      "INFO:__main__:Processing R_4AYVtTmMd0q7vte - Question 42\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Completed processing R_4AYVtTmMd0q7vte. Results saved to agentic_coding_results\n",
      "INFO:__main__:Processing survey file: temp_survey\\R_4C4j4KNUq9N3VGC.json\n",
      "INFO:__main__:Processing Question 40 for response R_4C4j4KNUq9N3VGC\n",
      "INFO:__main__:Processing R_4C4j4KNUq9N3VGC - Question 40\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing R_4C4j4KNUq9N3VGC.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 3.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 41 for response R_4C4j4KNUq9N3VGC\n",
      "INFO:__main__:Processing R_4C4j4KNUq9N3VGC - Question 41\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 22.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 42 for response R_4C4j4KNUq9N3VGC\n",
      "INFO:__main__:Processing R_4C4j4KNUq9N3VGC - Question 42\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Completed processing R_4C4j4KNUq9N3VGC. Results saved to agentic_coding_results\n",
      "INFO:__main__:Processing survey file: temp_survey\\R_4CKOoHgU8hp01rg.json\n",
      "INFO:__main__:Processing Question 40 for response R_4CKOoHgU8hp01rg\n",
      "INFO:__main__:Processing R_4CKOoHgU8hp01rg - Question 40\n",
      "INFO:__main__:Running initial iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing R_4CKOoHgU8hp01rg.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 16.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 41 for response R_4CKOoHgU8hp01rg\n",
      "INFO:__main__:Processing R_4CKOoHgU8hp01rg - Question 41\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 42 for response R_4CKOoHgU8hp01rg\n",
      "INFO:__main__:Processing R_4CKOoHgU8hp01rg - Question 42\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 4.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 16.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Completed processing R_4CKOoHgU8hp01rg. Results saved to agentic_coding_results\n",
      "INFO:__main__:Processing survey file: temp_survey\\R_4Ka5vRl3NuBJ1Af.json\n",
      "INFO:__main__:Processing Question 40 for response R_4Ka5vRl3NuBJ1Af\n",
      "INFO:__main__:Processing R_4Ka5vRl3NuBJ1Af - Question 40\n",
      "INFO:__main__:Running initial iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing R_4Ka5vRl3NuBJ1Af.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 41 for response R_4Ka5vRl3NuBJ1Af\n",
      "INFO:__main__:Processing R_4Ka5vRl3NuBJ1Af - Question 41\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 11.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 18.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Handling feedback loop...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 10.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 42 for response R_4Ka5vRl3NuBJ1Af\n",
      "INFO:__main__:Processing R_4Ka5vRl3NuBJ1Af - Question 42\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Completed processing R_4Ka5vRl3NuBJ1Af. Results saved to agentic_coding_results\n",
      "INFO:__main__:Processing survey file: temp_survey\\R_4P0PnRK3k4DjAxR.json\n",
      "INFO:__main__:Processing Question 40 for response R_4P0PnRK3k4DjAxR\n",
      "INFO:__main__:Processing R_4P0PnRK3k4DjAxR - Question 40\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 8.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing R_4P0PnRK3k4DjAxR.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 22.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 41 for response R_4P0PnRK3k4DjAxR\n",
      "INFO:__main__:Processing R_4P0PnRK3k4DjAxR - Question 41\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 6.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing Question 42 for response R_4P0PnRK3k4DjAxR\n",
      "INFO:__main__:Processing R_4P0PnRK3k4DjAxR - Question 42\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 2.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Completed processing R_4P0PnRK3k4DjAxR. Results saved to agentic_coding_results\n",
      "INFO:__main__:Processing survey file: temp_survey\\R_4sn9pI5HdvhV5gI.json\n",
      "INFO:__main__:Processing Question 43 for response R_4sn9pI5HdvhV5gI\n",
      "INFO:__main__:Processing R_4sn9pI5HdvhV5gI - Question 43\n",
      "INFO:__main__:Running initial iterations...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 8.000000 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing R_4sn9pI5HdvhV5gI.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Logging initial metrics...\n",
      "INFO:__main__:Making final decision...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 20.000000 seconds\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Running researcher validation...\n",
      "INFO:httpx:HTTP Request: POST https://cic-topic-modeling.openai.azure.com/openai/deployments/gpt-4-32k/chat/completions?api-version=2024-05-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Completed processing R_4sn9pI5HdvhV5gI. Results saved to agentic_coding_results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed processing 10 survey files\n",
      "Total results: 24\n",
      "Combined results saved to: agentic_coding_results\\all_agentic_results.json\n"
     ]
    }
   ],
   "source": [
    "# Run the fixed batch processing\n",
    "batch_results_fixed_v2 = process_multiple_surveys_fixed_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe15a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FixedAgenticCodingSystemV2' object has no attribute 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m system \u001b[38;5;241m=\u001b[39m FixedAgenticCodingSystemV2(model_azure, log_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magreement_logs.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 2) Get a pydot graph (xray=True shows conditionals/dotted edges)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m g \u001b[38;5;241m=\u001b[39m system\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mget_graph(xray\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 3) Save to PNG\u001b[39;00m\n\u001b[0;32m      8\u001b[0m png_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magentic_workflow.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FixedAgenticCodingSystemV2' object has no attribute 'graph'"
     ]
    }
   ],
   "source": [
    "# # 1) Build your system (uses your already-defined model_azure)\n",
    "# system = FixedAgenticCodingSystemV2(model_azure, log_file=\"agreement_logs.txt\")\n",
    "\n",
    "# # 2) Get a pydot graph (xray=True shows conditionals/dotted edges)\n",
    "# g = system.graph.get_graph(xray=True)\n",
    "\n",
    "# # 3) Save to PNG\n",
    "# png_path = \"agentic_workflow.png\"\n",
    "# g.draw_png(png_path)\n",
    "\n",
    "# # 4) (Optional) Show inline in Jupyter\n",
    "# from IPython.display import Image, display\n",
    "# display(Image(filename=png_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a5146",
   "metadata": {},
   "source": [
    "## 4. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c953d4b8",
   "metadata": {},
   "source": [
    "### Iterations comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bf34f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "def compare_human_model_codings_iterations(iterations_folder: str, temp_survey_folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare human-coded responses with model-coded responses from the new iterations format.\n",
    "    \n",
    "    Args:\n",
    "        iterations_folder: Path to the iterations folder containing subfolders for each response-question combination\n",
    "        temp_survey_folder: Path to the temp_survey folder containing human-coded files\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with comparison of human vs model codings across iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get list of iteration folders\n",
    "    iteration_folders = [f for f in os.listdir(iterations_folder) \n",
    "                        if os.path.isdir(os.path.join(iterations_folder, f))]\n",
    "    \n",
    "    # Create a lookup for human responses\n",
    "    human_files = [f for f in os.listdir(temp_survey_folder) if f.endswith('.json')]\n",
    "    human_lookup = {}\n",
    "    for human_file in human_files:\n",
    "        response_id = human_file.replace('.json', '')\n",
    "        human_file_path = os.path.join(temp_survey_folder, human_file)\n",
    "        with open(human_file_path, 'r', encoding='utf-8') as f:\n",
    "            human_lookup[response_id] = json.load(f)\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for folder_name in iteration_folders:\n",
    "        # Parse folder name to extract ResponseId and Question\n",
    "        # Format: R_1gTNWbmwxtRxrf8_Question 40\n",
    "        parts = folder_name.split('_Question ')\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "            \n",
    "        response_id = parts[0]\n",
    "        question_key = f\"Question {parts[1]}\"\n",
    "        \n",
    "        # Skip if this response ID is not in human responses\n",
    "        if response_id not in human_lookup:\n",
    "            continue\n",
    "            \n",
    "        # Load human response\n",
    "        human_response = human_lookup[response_id]\n",
    "        \n",
    "        # Skip if question doesn't exist in human response\n",
    "        if question_key not in human_response:\n",
    "            continue\n",
    "        \n",
    "        # Load iteration files\n",
    "        initial_folder = os.path.join(iterations_folder, folder_name, 'initial')\n",
    "        if not os.path.exists(initial_folder):\n",
    "            continue\n",
    "            \n",
    "        iteration_files = ['iter1.json', 'iter2.json', 'iter3.json']\n",
    "        iteration_data = {}\n",
    "        \n",
    "        for iter_file in iteration_files:\n",
    "            iter_path = os.path.join(initial_folder, iter_file)\n",
    "            if not os.path.exists(iter_path):\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                with open(iter_path, 'r', encoding='utf-8') as f:\n",
    "                    iter_data = json.load(f)\n",
    "            except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "                print(f\"Warning: Could not load {iter_path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            iteration_name = iter_data.get('iteration_name', iter_file.replace('.json', ''))\n",
    "            parsed_response = iter_data.get('parsed_response', {})\n",
    "            \n",
    "            # Initialize empty lists\n",
    "            actor_comparator = []\n",
    "            characteristics = []\n",
    "            themes = []\n",
    "            sub_themes = []\n",
    "            reasonings = []\n",
    "            \n",
    "            # Check if parsed_response is valid (not None)\n",
    "            if parsed_response is not None:\n",
    "                # Handle Question 43 special structure (reasons instead of codes)\n",
    "                if question_key == \"Question 43\":\n",
    "                    iteration_reasons = parsed_response.get('reasons', [])\n",
    "                    \n",
    "                    for reason in iteration_reasons:\n",
    "                        actor_comparator.append(\"\")  # No actor/comparator for Question 43\n",
    "                        characteristics.append(reason.get('code', ''))\n",
    "                        themes.append(reason.get('theme', ''))\n",
    "                        sub_themes.append(reason.get('sub_theme', ''))\n",
    "                        reasonings.append(reason.get('reasoning', ''))\n",
    "                else:\n",
    "                    iteration_codes = parsed_response.get('codes', [])\n",
    "                    \n",
    "                    for code in iteration_codes:\n",
    "                        actor_comparator.append(f\"{code.get('actor', '')}-{code.get('comparator', '').lower()}\")\n",
    "                        characteristics.append(code.get('code', ''))\n",
    "                        themes.append(code.get('theme', ''))\n",
    "                        sub_themes.append(code.get('sub_theme', ''))\n",
    "                        reasonings.append(code.get('reasoning', ''))\n",
    "            else:\n",
    "                # Debug: print which file has None parsed_response\n",
    "                print(f\"Warning: parsed_response is None for {iter_path}\")\n",
    "            \n",
    "            iteration_data[iteration_name] = {\n",
    "                'actor_comparator': actor_comparator,\n",
    "                'characteristics': characteristics,\n",
    "                'themes': themes,\n",
    "                'sub_themes': sub_themes,\n",
    "                'reasonings': reasonings\n",
    "            }\n",
    "        \n",
    "        # Format human codes - FIXED FOR QUESTION 43\n",
    "        human_actor_comparator = []\n",
    "        human_characteristics = []\n",
    "        \n",
    "        if question_key == \"Question 43\":\n",
    "            # For Question 43, use \"Reasons\" field which is a list of strings\n",
    "            human_reasons = human_response[question_key].get('Reasons', [])\n",
    "            for reason in human_reasons:\n",
    "                human_actor_comparator.append(\"\")  # No actor/comparator for Question 43\n",
    "                human_characteristics.append(reason)\n",
    "        else:\n",
    "            # For other questions, use \"Codes\" field\n",
    "            human_codes = human_response[question_key].get('Codes', [])\n",
    "            for code in human_codes:\n",
    "                if code.get('Actor', '') not in ['nil', 'Nil', '']:\n",
    "                    human_actor_comparator.append(f\"{code.get('Actor', '')}-{code.get('Comparator', '').lower()}\")\n",
    "                    human_characteristics.append(code.get('Characteristic', ''))\n",
    "        \n",
    "        # Create comparison row\n",
    "        row_data = {\n",
    "            'ResponseId': response_id,\n",
    "            'Question': question_key,\n",
    "            'Question_Text': human_response[question_key].get('Question', ''),\n",
    "            'Response_Text': human_response[question_key].get('Response', ''),\n",
    "            'Human_Actor_Comparator': human_actor_comparator,\n",
    "            'Human_Characteristics': human_characteristics\n",
    "        }\n",
    "        \n",
    "        # Add iteration data\n",
    "        for iter_name, data in iteration_data.items():\n",
    "            row_data.update({\n",
    "                f\"{iter_name}_Actor_Comparator\": data['actor_comparator'],\n",
    "                f\"{iter_name}_Characteristics\": data['characteristics'],\n",
    "                f\"{iter_name}_Themes\": data['themes'],\n",
    "                f\"{iter_name}_SubThemes\": data['sub_themes'],\n",
    "                f\"{iter_name}_Reasonings\": data['reasonings']\n",
    "            })\n",
    "        \n",
    "        comparison_data.append(row_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    if not df.empty:\n",
    "        base_cols = ['ResponseId', 'Question', 'Question_Text', 'Response_Text', \n",
    "                    'Human_Actor_Comparator', 'Human_Characteristics']\n",
    "        iteration_cols = []\n",
    "        for iteration in ['Iteration1', 'Iteration2', 'Iteration3']:\n",
    "            iteration_cols.extend([\n",
    "                f'{iteration}_Actor_Comparator',\n",
    "                f'{iteration}_Characteristics',\n",
    "                f'{iteration}_Themes',\n",
    "                f'{iteration}_SubThemes',\n",
    "                f'{iteration}_Reasonings'\n",
    "            ])\n",
    "        \n",
    "        # Only include columns that exist in the dataframe\n",
    "        available_cols = [col for col in base_cols + iteration_cols if col in df.columns]\n",
    "        df = df[available_cols]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05bc5e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsed_response is None for agentic_coding_results/iterations\\R_1gTNWbmwxtRxrf8_Question 42\\initial\\iter2.json\n",
      "Warning: parsed_response is None for agentic_coding_results/iterations\\R_1gTNWbmwxtRxrf8_Question 42\\initial\\iter3.json\n",
      "Warning: parsed_response is None for agentic_coding_results/iterations\\R_4AYVtTmMd0q7vte_Question 41\\initial\\iter2.json\n",
      "Warning: parsed_response is None for agentic_coding_results/iterations\\R_4AYVtTmMd0q7vte_Question 41\\initial\\iter3.json\n",
      "Warning: parsed_response is None for agentic_coding_results/iterations\\R_4AYVtTmMd0q7vte_Question 42\\initial\\iter2.json\n",
      "Warning: parsed_response is None for agentic_coding_results/iterations\\R_4AYVtTmMd0q7vte_Question 42\\initial\\iter3.json\n",
      "Warning: parsed_response is None for agentic_coding_results/iterations\\R_4P0PnRK3k4DjAxR_Question 42\\initial\\iter2.json\n",
      "Warning: parsed_response is None for agentic_coding_results/iterations\\R_4P0PnRK3k4DjAxR_Question 42\\initial\\iter3.json\n"
     ]
    }
   ],
   "source": [
    "# Test the function with your data\n",
    "iterations_folder = \"agentic_coding_results/iterations\"\n",
    "temp_survey_folder = \"temp_survey\"\n",
    "\n",
    "# Create the comparison DataFrame\n",
    "comparison_df = compare_human_model_codings_iterations(iterations_folder, temp_survey_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "088b1a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24 entries, 0 to 23\n",
      "Data columns (total 21 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   ResponseId                   24 non-null     object\n",
      " 1   Question                     24 non-null     object\n",
      " 2   Question_Text                24 non-null     object\n",
      " 3   Response_Text                24 non-null     object\n",
      " 4   Human_Actor_Comparator       24 non-null     object\n",
      " 5   Human_Characteristics        24 non-null     object\n",
      " 6   Iteration1_Actor_Comparator  24 non-null     object\n",
      " 7   Iteration1_Characteristics   24 non-null     object\n",
      " 8   Iteration1_Themes            24 non-null     object\n",
      " 9   Iteration1_SubThemes         24 non-null     object\n",
      " 10  Iteration1_Reasonings        24 non-null     object\n",
      " 11  Iteration2_Actor_Comparator  24 non-null     object\n",
      " 12  Iteration2_Characteristics   24 non-null     object\n",
      " 13  Iteration2_Themes            24 non-null     object\n",
      " 14  Iteration2_SubThemes         24 non-null     object\n",
      " 15  Iteration2_Reasonings        24 non-null     object\n",
      " 16  Iteration3_Actor_Comparator  24 non-null     object\n",
      " 17  Iteration3_Characteristics   24 non-null     object\n",
      " 18  Iteration3_Themes            24 non-null     object\n",
      " 19  Iteration3_SubThemes         24 non-null     object\n",
      " 20  Iteration3_Reasonings        24 non-null     object\n",
      "dtypes: object(21)\n",
      "memory usage: 4.1+ KB\n"
     ]
    }
   ],
   "source": [
    "comparison_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2fbef8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_simplified_agreement_columns_iterations(df):\n",
    "    \"\"\"\n",
    "    Add agreement columns including Jaccard multiset and overall LLM inter-agreement.\n",
    "    Excludes pairwise iteration comparisons.\n",
    "    Also adds multiset agreement between each iteration and humans for the Actor-Comparator-Characteristic tuples,\n",
    "    where the first value in Actor-Comparator belongs to the first value in Characteristic, etc.\n",
    "    \n",
    "    Modified version for the new iterations structure.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame from compare_human_model_codings_iterations\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with simplified agreement columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # Define the columns we'll work with\n",
    "    human_characteristics_col = \"Human_Characteristics\"\n",
    "    human_actor_comparator_col = \"Human_Actor_Comparator\"\n",
    "    \n",
    "    iteration_characteristics_cols = [\n",
    "        \"Iteration1_Characteristics\",\n",
    "        \"Iteration2_Characteristics\", \n",
    "        \"Iteration3_Characteristics\"\n",
    "    ]\n",
    "    \n",
    "    iteration_actor_comparator_cols = [\n",
    "        \"Iteration1_Actor_Comparator\",\n",
    "        \"Iteration2_Actor_Comparator\",\n",
    "        \"Iteration3_Actor_Comparator\"\n",
    "    ]\n",
    "    \n",
    "    # Ensure all columns are lists\n",
    "    df_enhanced[human_characteristics_col] = df_enhanced[human_characteristics_col].apply(safe_eval_list)\n",
    "    df_enhanced[human_actor_comparator_col] = df_enhanced[human_actor_comparator_col].apply(safe_eval_list)\n",
    "    \n",
    "    for col in iteration_characteristics_cols + iteration_actor_comparator_cols:\n",
    "        df_enhanced[col] = df_enhanced[col].apply(safe_eval_list)\n",
    "    \n",
    "    # Add chunk count columns\n",
    "    df_enhanced[\"Human_Chunks\"] = df_enhanced[human_characteristics_col].apply(len)\n",
    "    \n",
    "    for i, col in enumerate(iteration_characteristics_cols, 1):\n",
    "        df_enhanced[f\"Iteration{i}_Chunks\"] = df_enhanced[col].apply(len)\n",
    "    \n",
    "    # Add Jaccard set agreement columns for characteristics\n",
    "    for i, col in enumerate(iteration_characteristics_cols, 1):\n",
    "        agreement_scores = []\n",
    "        for _, row in df_enhanced.iterrows():\n",
    "            score = jaccard_similarity(row[human_characteristics_col], row[col])\n",
    "            agreement_scores.append(score)\n",
    "        df_enhanced[f\"Iteration{i}_Agreement\"] = agreement_scores\n",
    "    \n",
    "    # Add Jaccard multiset agreement columns for characteristics\n",
    "    for i, col in enumerate(iteration_characteristics_cols, 1):\n",
    "        agreement_scores = []\n",
    "        for _, row in df_enhanced.iterrows():\n",
    "            score = jaccard_multiset(row[human_characteristics_col], row[col])\n",
    "            agreement_scores.append(score)\n",
    "        df_enhanced[f\"Iteration{i}_Multiset_Agreement\"] = agreement_scores\n",
    "    \n",
    "    # Add agreement columns for actor comparators\n",
    "    for i, col in enumerate(iteration_actor_comparator_cols, 1):\n",
    "        agreement_scores = []\n",
    "        for _, row in df_enhanced.iterrows():\n",
    "            score = jaccard_similarity(row[human_actor_comparator_col], row[col])\n",
    "            agreement_scores.append(score)\n",
    "        df_enhanced[f\"Iteration{i}_Actor_Agreement\"] = agreement_scores\n",
    "    \n",
    "    # Add overall LLM inter-agreement (average agreement between all pairs of iterations)\n",
    "    llm_set_agreement = []\n",
    "    llm_multiset_agreement = []\n",
    "    \n",
    "    for _, row in df_enhanced.iterrows():\n",
    "        # Get all pairwise agreements for this row\n",
    "        set_scores = []\n",
    "        multiset_scores = []\n",
    "        \n",
    "        for i, j in combinations(range(1, 4), 2):\n",
    "            col1 = iteration_characteristics_cols[i-1]\n",
    "            col2 = iteration_characteristics_cols[j-1]\n",
    "            set_scores.append(jaccard_similarity(row[col1], row[col2]))\n",
    "            multiset_scores.append(jaccard_multiset(row[col1], row[col2]))\n",
    "        \n",
    "        # Average the pairwise agreements\n",
    "        llm_set_agreement.append(sum(set_scores) / len(set_scores) if set_scores else 0)\n",
    "        llm_multiset_agreement.append(sum(multiset_scores) / len(multiset_scores) if multiset_scores else 0)\n",
    "    \n",
    "    df_enhanced[\"LLM_Inter_Agreement_Set\"] = llm_set_agreement\n",
    "    df_enhanced[\"LLM_Inter_Agreement_Multiset\"] = llm_multiset_agreement\n",
    "\n",
    "    # --- Add multiset agreement for Actor-Comparator-Characteristic tuples ---\n",
    "    def tuple_multiset_agreement(human_ac, human_char, model_ac, model_char):\n",
    "        # Pair up actor-comparator and characteristic by index, skip if lengths don't match\n",
    "        human_tuples = list(zip(human_ac, human_char))\n",
    "        model_tuples = list(zip(model_ac, model_char))\n",
    "        # Each tuple is (actor_comparator, characteristic)\n",
    "        # For multiset, treat as list of tuples\n",
    "        return jaccard_multiset(human_tuples, model_tuples)\n",
    "\n",
    "    for i, (ac_col, char_col) in enumerate(zip(iteration_actor_comparator_cols, iteration_characteristics_cols), 1):\n",
    "        agreement_scores = []\n",
    "        for _, row in df_enhanced.iterrows():\n",
    "            score = tuple_multiset_agreement(\n",
    "                row[human_actor_comparator_col],\n",
    "                row[human_characteristics_col],\n",
    "                row[ac_col],\n",
    "                row[char_col]\n",
    "            )\n",
    "            agreement_scores.append(score)\n",
    "        df_enhanced[f\"Iteration{i}_ACChar_Multiset_Agreement\"] = agreement_scores\n",
    "\n",
    "    return df_enhanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa5deb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for agreement calculations\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "def safe_eval_list(x):\n",
    "    \"\"\"Safely convert string representation of list to actual list\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    elif isinstance(x, str):\n",
    "        try:\n",
    "            return eval(x) if x else []\n",
    "        except:\n",
    "            return []\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Calculate Jaccard similarity between two sets\"\"\"\n",
    "    if not set1 and not set2:\n",
    "        return 1.0\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "    \n",
    "    set1, set2 = set(set1), set(set2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "def jaccard_multiset(list1, list2):\n",
    "    \"\"\"Calculate Jaccard similarity for multisets (lists with duplicates)\"\"\"\n",
    "    if not list1 and not list2:\n",
    "        return 1.0\n",
    "    if not list1 or not list2:\n",
    "        return 0.0\n",
    "    \n",
    "    counter1, counter2 = Counter(list1), Counter(list2)\n",
    "    intersection = sum((counter1 & counter2).values())\n",
    "    union = sum((counter1 | counter2).values())\n",
    "    return intersection / union if union > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "785e7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df_metrics = add_simplified_agreement_columns_iterations(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "995fbeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24 entries, 0 to 23\n",
      "Data columns (total 39 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   ResponseId                            24 non-null     object \n",
      " 1   Question                              24 non-null     object \n",
      " 2   Question_Text                         24 non-null     object \n",
      " 3   Response_Text                         24 non-null     object \n",
      " 4   Human_Actor_Comparator                24 non-null     object \n",
      " 5   Human_Characteristics                 24 non-null     object \n",
      " 6   Iteration1_Actor_Comparator           24 non-null     object \n",
      " 7   Iteration1_Characteristics            24 non-null     object \n",
      " 8   Iteration1_Themes                     24 non-null     object \n",
      " 9   Iteration1_SubThemes                  24 non-null     object \n",
      " 10  Iteration1_Reasonings                 24 non-null     object \n",
      " 11  Iteration2_Actor_Comparator           24 non-null     object \n",
      " 12  Iteration2_Characteristics            24 non-null     object \n",
      " 13  Iteration2_Themes                     24 non-null     object \n",
      " 14  Iteration2_SubThemes                  24 non-null     object \n",
      " 15  Iteration2_Reasonings                 24 non-null     object \n",
      " 16  Iteration3_Actor_Comparator           24 non-null     object \n",
      " 17  Iteration3_Characteristics            24 non-null     object \n",
      " 18  Iteration3_Themes                     24 non-null     object \n",
      " 19  Iteration3_SubThemes                  24 non-null     object \n",
      " 20  Iteration3_Reasonings                 24 non-null     object \n",
      " 21  Human_Chunks                          24 non-null     int64  \n",
      " 22  Iteration1_Chunks                     24 non-null     int64  \n",
      " 23  Iteration2_Chunks                     24 non-null     int64  \n",
      " 24  Iteration3_Chunks                     24 non-null     int64  \n",
      " 25  Iteration1_Agreement                  24 non-null     float64\n",
      " 26  Iteration2_Agreement                  24 non-null     float64\n",
      " 27  Iteration3_Agreement                  24 non-null     float64\n",
      " 28  Iteration1_Multiset_Agreement         24 non-null     float64\n",
      " 29  Iteration2_Multiset_Agreement         24 non-null     float64\n",
      " 30  Iteration3_Multiset_Agreement         24 non-null     float64\n",
      " 31  Iteration1_Actor_Agreement            24 non-null     float64\n",
      " 32  Iteration2_Actor_Agreement            24 non-null     float64\n",
      " 33  Iteration3_Actor_Agreement            24 non-null     float64\n",
      " 34  LLM_Inter_Agreement_Set               24 non-null     float64\n",
      " 35  LLM_Inter_Agreement_Multiset          24 non-null     float64\n",
      " 36  Iteration1_ACChar_Multiset_Agreement  24 non-null     float64\n",
      " 37  Iteration2_ACChar_Multiset_Agreement  24 non-null     float64\n",
      " 38  Iteration3_ACChar_Multiset_Agreement  24 non-null     float64\n",
      "dtypes: float64(14), int64(4), object(21)\n",
      "memory usage: 7.4+ KB\n"
     ]
    }
   ],
   "source": [
    "comparison_df_metrics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d79d2cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df_metrics.map(lambda x: str(x) if isinstance(x, list) else x).to_csv('comparison_results_v2.csv', index=False, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f94aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "def safe_eval_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except:\n",
    "            return [x]\n",
    "    return []\n",
    "\n",
    "# --- similarity metrics ---\n",
    "\n",
    "def jaccard_set(list1, list2):\n",
    "    s1, s2 = set(list1), set(list2)\n",
    "    if not s1 and not s2: return 1.0\n",
    "    return len(s1 & s2) / len(s1 | s2)\n",
    "\n",
    "def jaccard_multiset(list1, list2):\n",
    "    c1, c2 = Counter(list1), Counter(list2)\n",
    "    if not c1 and not c2: return 1.0\n",
    "    inter = sum((c1 & c2).values())                     # sum of mins\n",
    "    union = sum((c1 | c2).values())                     # sum of maxes\n",
    "    return inter / union if union else 1.0\n",
    "\n",
    "def dice_multiset(list1, list2):\n",
    "    c1, c2 = Counter(list1), Counter(list2)\n",
    "    if not c1 and not c2: return 1.0\n",
    "    inter = sum((c1 & c2).values())\n",
    "    total = sum(c1.values()) + sum(c2.values())\n",
    "    return (2 * inter) / total if total else 1.0\n",
    "\n",
    "def exact_set_match(list1, list2):\n",
    "    return 1.0 if set(list1) == set(list2) else 0.0\n",
    "\n",
    "def topk_jaccard(list_human, list_model):\n",
    "    k = len(list_human)\n",
    "    topk = list_model[:k]\n",
    "    return jaccard_set(list_human, topk)\n",
    "\n",
    "# --- driver ---\n",
    "\n",
    "def compute_human_iteration_agreement(\n",
    "    df, \n",
    "    question_col=\"Question\",\n",
    "    human_col=\"Human_Characteristics\",\n",
    "    iteration_cols=(\"Iteration1_Characteristics\",\"Iteration2_Characteristics\",\"Iteration3_Characteristics\"),\n",
    "    metric=\"jaccard_multiset\"  # options: jaccard_set, jaccard_multiset, dice_multiset, exact_set, topk\n",
    "):\n",
    "    # map metric name -> function\n",
    "    metric_map = {\n",
    "        \"jaccard_set\": jaccard_set,\n",
    "        \"jaccard_multiset\": jaccard_multiset,\n",
    "        \"dice_multiset\": dice_multiset,\n",
    "        \"exact_set\": exact_set_match,\n",
    "        \"topk\": topk_jaccard,\n",
    "    }\n",
    "    sim = metric_map[metric]\n",
    "\n",
    "    # ensure lists\n",
    "    df = df.copy()\n",
    "    df[human_col] = df[human_col].apply(safe_eval_list)\n",
    "    for col in iteration_cols:\n",
    "        df[col] = df[col].apply(safe_eval_list)\n",
    "\n",
    "    rows = []\n",
    "    for q, group in df.groupby(question_col):\n",
    "        for col in iteration_cols:\n",
    "            scores = []\n",
    "            for _, r in group.iterrows():\n",
    "                if metric == \"topk\":\n",
    "                    s = sim(r[human_col], r[col])\n",
    "                else:\n",
    "                    s = sim(r[human_col], r[col])\n",
    "                scores.append(s)\n",
    "            rows.append({\n",
    "                \"Question\": q,\n",
    "                \"Iteration\": col.replace(\"_Characteristics\", \"\"),\n",
    "                \"Agreement_with_Human\": sum(scores)/len(scores) if scores else 0.0,\n",
    "                \"Metric\": metric\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0620c5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Question   Iteration  Agreement_with_Human            Metric\n",
      "0   Question 40  Iteration1              0.417749  jaccard_multiset\n",
      "1   Question 40  Iteration2              0.417749  jaccard_multiset\n",
      "2   Question 40  Iteration3              0.417749  jaccard_multiset\n",
      "3   Question 41  Iteration1              0.500000  jaccard_multiset\n",
      "4   Question 41  Iteration2              0.500000  jaccard_multiset\n",
      "5   Question 41  Iteration3              0.500000  jaccard_multiset\n",
      "6   Question 42  Iteration1              0.657143  jaccard_multiset\n",
      "7   Question 42  Iteration2              0.676190  jaccard_multiset\n",
      "8   Question 42  Iteration3              0.676190  jaccard_multiset\n",
      "9   Question 43  Iteration1              0.777778  jaccard_multiset\n",
      "10  Question 43  Iteration2              0.611111  jaccard_multiset\n",
      "11  Question 43  Iteration3              0.611111  jaccard_multiset\n"
     ]
    }
   ],
   "source": [
    "agreement_human_df = compute_human_iteration_agreement(\n",
    "    comparison_df,\n",
    "    metric=\"jaccard_multiset\"   # or \"exact_set\", \"topk\", etc.\n",
    ")\n",
    "print(agreement_human_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da39e9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Question  Agreement\n",
      "0  Question 40   1.000000\n",
      "1  Question 41   0.904762\n",
      "2  Question 42   0.961905\n",
      "3  Question 43   0.888889\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from itertools import combinations\n",
    "\n",
    "def safe_eval_list(x):\n",
    "    \"\"\"\n",
    "    Ensures that string values like \"['a','b']\" \n",
    "    are converted into Python lists.\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except:\n",
    "            return [x]\n",
    "    return []\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    if not set1 and not set2:\n",
    "        return 1.0  # both empty → perfect agreement\n",
    "    return len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "def compute_agreement(df, question_col=\"Question\"):\n",
    "    iteration_cols = [\n",
    "        \"Iteration1_Characteristics\",\n",
    "        \"Iteration2_Characteristics\",\n",
    "        \"Iteration3_Characteristics\"\n",
    "    ]\n",
    "    \n",
    "    # Ensure lists\n",
    "    for col in iteration_cols:\n",
    "        df[col] = df[col].apply(safe_eval_list)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for q, group in df.groupby(question_col):\n",
    "        pair_scores = []\n",
    "        for _, row in group.iterrows():\n",
    "            for col1, col2 in combinations(iteration_cols, 2):\n",
    "                score = jaccard_similarity(row[col1], row[col2])\n",
    "                pair_scores.append(score)\n",
    "        avg_score = sum(pair_scores) / len(pair_scores) if pair_scores else 0\n",
    "        results.append({\"Question\": q, \"Agreement\": avg_score})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage\n",
    "agreement_df = compute_agreement(comparison_df)\n",
    "print(agreement_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0113445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Question   Iteration  Agreement_with_Human\n",
      "0   Question 40  Iteration1              0.496599\n",
      "1   Question 40  Iteration2              0.496599\n",
      "2   Question 40  Iteration3              0.496599\n",
      "3   Question 41  Iteration1              0.500000\n",
      "4   Question 41  Iteration2              0.500000\n",
      "5   Question 41  Iteration3              0.500000\n",
      "6   Question 42  Iteration1              0.667532\n",
      "7   Question 42  Iteration2              0.689796\n",
      "8   Question 42  Iteration3              0.689796\n",
      "9   Question 43  Iteration1              0.833333\n",
      "10  Question 43  Iteration2              0.666667\n",
      "11  Question 43  Iteration3              0.666667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def safe_eval_list(x):\n",
    "    \"\"\"Converts stringified lists into Python lists safely.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except:\n",
    "            return [x]\n",
    "    return []\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    if not set1 and not set2:\n",
    "        return 1.0  # both empty = perfect agreement\n",
    "    return len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "def compute_human_iteration_agreement(df, question_col=\"Question\"):\n",
    "    human_col = \"Human_Characteristics\"\n",
    "    iteration_cols = [\n",
    "        \"Iteration1_Characteristics\",\n",
    "        \"Iteration2_Characteristics\",\n",
    "        \"Iteration3_Characteristics\"\n",
    "    ]\n",
    "    \n",
    "    # Ensure all are lists\n",
    "    df[human_col] = df[human_col].apply(safe_eval_list)\n",
    "    for col in iteration_cols:\n",
    "        df[col] = df[col].apply(safe_eval_list)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for q, group in df.groupby(question_col):\n",
    "        for iteration_col in iteration_cols:\n",
    "            scores = []\n",
    "            for _, row in group.iterrows():\n",
    "                score = jaccard_similarity(row[human_col], row[iteration_col])\n",
    "                scores.append(score)\n",
    "            avg_score = sum(scores) / len(scores) if scores else 0\n",
    "            results.append({\n",
    "                \"Question\": q,\n",
    "                \"Iteration\": iteration_col.replace(\"_Characteristics\", \"\"),\n",
    "                \"Agreement_with_Human\": avg_score\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage\n",
    "agreement_human_df = compute_human_iteration_agreement(comparison_df)\n",
    "print(agreement_human_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c87265",
   "metadata": {},
   "source": [
    "### Agentic comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee569ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_agentic_human_codings(agentic_results_path: str, temp_survey_folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare agentic-coded responses with human-coded responses.\n",
    "    \n",
    "    Args:\n",
    "        agentic_results_path: Path to the all_agentic_results.json file\n",
    "        temp_survey_folder: Path to the temp_survey folder containing human-coded files\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with comparison of human vs agentic codings\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the agentic responses\n",
    "    with open(agentic_results_path, 'r', encoding='utf-8') as f:\n",
    "        agentic_responses = json.load(f)\n",
    "    \n",
    "    # Create a dictionary for quick lookup\n",
    "    agentic_lookup = {}\n",
    "    for resp in agentic_responses:\n",
    "        key = f\"{resp['response_id']}_{resp['question_key']}\"\n",
    "        agentic_lookup[key] = resp\n",
    "    \n",
    "    # Get list of human-coded files\n",
    "    human_files = [f for f in os.listdir(temp_survey_folder) if f.endswith('.json')]\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for human_file in human_files:\n",
    "        response_id = human_file.replace('.json', '')\n",
    "        \n",
    "        # Load human-coded response\n",
    "        human_file_path = os.path.join(temp_survey_folder, human_file)\n",
    "        with open(human_file_path, 'r', encoding='utf-8') as f:\n",
    "            human_response = json.load(f)\n",
    "        \n",
    "        # Process each question\n",
    "        for question_key in human_response:\n",
    "            if not question_key.startswith('Question'):\n",
    "                continue\n",
    "            \n",
    "            # Create lookup key\n",
    "            lookup_key = f\"{response_id}_{question_key}\"\n",
    "            \n",
    "            # Skip if question doesn't exist in agentic responses\n",
    "            if lookup_key not in agentic_lookup:\n",
    "                continue\n",
    "            \n",
    "            agentic_result = agentic_lookup[lookup_key]\n",
    "            \n",
    "            # Handle different structures for different questions\n",
    "            if question_key == \"Question 43\":\n",
    "                # For Question 43: Human has \"Reasons\" list, Agentic has \"final_codes\" with \"reasons\"\n",
    "                human_codes = human_response[question_key].get('Reasons', [])\n",
    "                agentic_final_codes = agentic_result.get('final_codes', {})\n",
    "                \n",
    "                # Extract human coding information as lists\n",
    "                human_characteristics = human_codes if isinstance(human_codes, list) else []\n",
    "                human_actor_comparators = []  # Not applicable for Question 43\n",
    "                \n",
    "                # Process agentic codes\n",
    "                agentic_characteristics = []\n",
    "                agentic_themes = []\n",
    "                agentic_subthemes = []\n",
    "                agentic_reasonings = []\n",
    "                \n",
    "                if agentic_final_codes and \"reasons\" in agentic_final_codes:\n",
    "                    for reason in agentic_final_codes[\"reasons\"]:\n",
    "                        agentic_characteristics.append(reason.get('code', ''))\n",
    "                        agentic_themes.append(reason.get('theme', ''))\n",
    "                        agentic_subthemes.append(reason.get('sub_theme', ''))\n",
    "                        agentic_reasonings.append(reason.get('reasoning', ''))\n",
    "                \n",
    "                # Create comparison row for Question 43\n",
    "                comparison_data.append({\n",
    "                    'ResponseId': response_id,\n",
    "                    'Question': question_key,\n",
    "                    'Human_Actor_Comparator': human_actor_comparators,\n",
    "                    'Human_Characteristics': human_characteristics,\n",
    "                    'Agentic_Characteristics': agentic_characteristics,\n",
    "                    'Agentic_Themes': agentic_themes,\n",
    "                    'Agentic_SubThemes': agentic_subthemes,\n",
    "                    'Agentic_Reasonings': agentic_reasonings,\n",
    "                    'Researcher_Agreement': agentic_result.get('researcher_feedback', {}).get('agreement', False),\n",
    "                    'Quality_Score': agentic_result.get('researcher_feedback', {}).get('quality_score', 0),\n",
    "                    'Total_Iterations': agentic_result.get('total_iterations', 0),\n",
    "                    'Iteration_Agreement': agentic_result.get('agreement_metrics', {}).get('iteration_agreement', {}).get('agreement_score', 0)\n",
    "                })\n",
    "                \n",
    "            else:\n",
    "                # For Questions 40, 41, 42: Original structure with Actor-Comparator-Characteristic\n",
    "                human_codes = human_response[question_key].get('Codes', [])\n",
    "                agentic_final_codes = agentic_result.get('final_codes', {})\n",
    "                \n",
    "                # Extract human coding information as lists\n",
    "                human_actors = []\n",
    "                human_comparators = []\n",
    "                human_characteristics = []\n",
    "                human_actor_comparators = []\n",
    "                \n",
    "                for human_code in human_codes:\n",
    "                    actor = human_code.get('Actor', '')\n",
    "                    comparator = human_code.get('Comparator', '')\n",
    "                    characteristic = human_code.get('Characteristic', '')\n",
    "                    \n",
    "                    # Skip if no meaningful coding\n",
    "                    if actor in ['nil', 'Nil', '']:\n",
    "                        continue\n",
    "                    \n",
    "                    human_actors.append(actor)\n",
    "                    human_comparators.append(comparator)\n",
    "                    human_characteristics.append(characteristic)\n",
    "                    human_actor_comparators.append(f\"{actor}-{comparator}\")\n",
    "                \n",
    "                # Process agentic codes\n",
    "                agentic_actors = []\n",
    "                agentic_comparators = []\n",
    "                agentic_characteristics = []\n",
    "                agentic_themes = []\n",
    "                agentic_subthemes = []\n",
    "                agentic_reasonings = []\n",
    "                agentic_actor_comparators = []\n",
    "                \n",
    "                if agentic_final_codes and \"codes\" in agentic_final_codes:\n",
    "                    for code in agentic_final_codes[\"codes\"]:\n",
    "                        actor = code.get('actor', '')\n",
    "                        comparator = code.get('comparator', '')\n",
    "                        characteristic = code.get('code', '')\n",
    "                        theme = code.get('theme', '')\n",
    "                        sub_theme = code.get('sub_theme', '')\n",
    "                        reasoning = code.get('reasoning', '')\n",
    "                        \n",
    "                        agentic_actors.append(actor)\n",
    "                        agentic_comparators.append(comparator)\n",
    "                        agentic_characteristics.append(characteristic)\n",
    "                        agentic_themes.append(theme)\n",
    "                        agentic_subthemes.append(sub_theme)\n",
    "                        agentic_reasonings.append(reasoning)\n",
    "                        agentic_actor_comparators.append(f\"{actor}-{comparator}\")\n",
    "                \n",
    "                # Create comparison row for Questions 40-42\n",
    "                comparison_data.append({\n",
    "                    'ResponseId': response_id,\n",
    "                    'Question': question_key,\n",
    "                    'Human_Actor_Comparator': human_actor_comparators,\n",
    "                    'Human_Characteristics': human_characteristics,\n",
    "                    'Agentic_Actor_Comparator': agentic_actor_comparators,\n",
    "                    'Agentic_Characteristics': agentic_characteristics,\n",
    "                    'Agentic_Themes': agentic_themes,\n",
    "                    'Agentic_SubThemes': agentic_subthemes,\n",
    "                    'Agentic_Reasonings': agentic_reasonings,\n",
    "                    'Researcher_Agreement': agentic_result.get('researcher_feedback', {}).get('agreement', False),\n",
    "                    'Quality_Score': agentic_result.get('researcher_feedback', {}).get('quality_score', 0),\n",
    "                    'Total_Iterations': agentic_result.get('total_iterations', 0),\n",
    "                    'Iteration_Agreement': agentic_result.get('agreement_metrics', {}).get('iteration_agreement', {}).get('agreement_score', 0)\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8d987872",
   "metadata": {},
   "outputs": [],
   "source": [
    "agentic_comparison_df = compare_agentic_human_codings(\n",
    "    'agentic_coding_results/all_agentic_results.json',\n",
    "    'temp_survey'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce7d49d",
   "metadata": {},
   "source": [
    "### Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "126b5219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24 entries, 0 to 23\n",
      "Data columns (total 13 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   ResponseId                24 non-null     object \n",
      " 1   Question                  24 non-null     object \n",
      " 2   Human_Actor_Comparator    24 non-null     object \n",
      " 3   Human_Characteristics     24 non-null     object \n",
      " 4   Agentic_Actor_Comparator  21 non-null     object \n",
      " 5   Agentic_Characteristics   24 non-null     object \n",
      " 6   Agentic_Themes            24 non-null     object \n",
      " 7   Agentic_SubThemes         24 non-null     object \n",
      " 8   Agentic_Reasonings        24 non-null     object \n",
      " 9   Researcher_Agreement      24 non-null     bool   \n",
      " 10  Quality_Score             24 non-null     int64  \n",
      " 11  Total_Iterations          24 non-null     int64  \n",
      " 12  Iteration_Agreement       24 non-null     float64\n",
      "dtypes: bool(1), float64(1), int64(2), object(9)\n",
      "memory usage: 2.4+ KB\n"
     ]
    }
   ],
   "source": [
    "agentic_comparison_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e24a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def safe_eval_list(x):\n",
    "    \"\"\"Converts stringified lists into Python lists safely.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except:\n",
    "            return [x]\n",
    "    return []\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    if not set1 and not set2:\n",
    "        return 1.0  # both empty = perfect agreement\n",
    "    return len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "def compute_human_iteration_agreement(df, question_col=\"Question\"):\n",
    "    human_col = \"Human_Characteristics\"\n",
    "    iteration_cols = [\n",
    "        \"Agentic_Characteristics\"\n",
    "    ]\n",
    "    \n",
    "    # Ensure all are lists\n",
    "    df[human_col] = df[human_col].apply(safe_eval_list)\n",
    "    for col in iteration_cols:\n",
    "        df[col] = df[col].apply(safe_eval_list)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for q, group in df.groupby(question_col):\n",
    "        for iteration_col in iteration_cols:\n",
    "            scores = []\n",
    "            for _, row in group.iterrows():\n",
    "                score = jaccard_similarity(row[human_col], row[iteration_col])\n",
    "                scores.append(score)\n",
    "            avg_score = sum(scores) / len(scores) if scores else 0\n",
    "            results.append({\n",
    "                \"Question\": q,\n",
    "                \"Iteration\": iteration_col.replace(\"_Characteristics\", \"\"),\n",
    "                \"Agreement_with_Human\": avg_score\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c680c8",
   "metadata": {},
   "source": [
    "#### Regular-agentic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81c60aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Question Iteration  Agreement_with_Human\n",
      "0  Question 40   Agentic              0.496599\n",
      "1  Question 41   Agentic              0.500000\n",
      "2  Question 42   Agentic              0.689796\n",
      "3  Question 43   Agentic              0.833333\n"
     ]
    }
   ],
   "source": [
    "agreement_agentic = compute_human_iteration_agreement(agentic_comparison_df)\n",
    "print(agreement_agentic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dfe1dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _jaccard(a, b):\n",
    "    sa, sb = set(a), set(b)\n",
    "    if not sa and not sb:\n",
    "        return 1.0\n",
    "    return len(sa & sb) / len(sa | sb)\n",
    "\n",
    "def _extract_iter_codes(iteration_results):\n",
    "    \"\"\"Return list of lists of codes (strings) from parsed iteration results.\"\"\"\n",
    "    outs = []\n",
    "    for r in (iteration_results or []):\n",
    "        parsed = r.get(\"parsed_response\") or {}\n",
    "        codes = parsed.get(\"codes\") or parsed.get(\"reasons\") or []\n",
    "        # normalize both formats (40–42 use 'codes', 43 might use 'reasons')\n",
    "        items = []\n",
    "        for c in codes:\n",
    "            # prefer 'code' field as the characteristic label\n",
    "            items.append(c.get(\"code\", \"\") if isinstance(c, dict) else str(c))\n",
    "        outs.append([x for x in items if x])\n",
    "    return outs\n",
    "\n",
    "def compare_agentic_human_codings_with_iterations(agentic_results_path: str,\n",
    "                                                  temp_survey_folder: str) -> pd.DataFrame:\n",
    "    import json, os, pandas as pd\n",
    "\n",
    "    with open(agentic_results_path, 'r', encoding='utf-8') as f:\n",
    "        agentic_responses = json.load(f)\n",
    "\n",
    "    agentic_lookup = {f\"{r['response_id']}_{r['question_key']}\": r for r in agentic_responses}\n",
    "    human_files = [f for f in os.listdir(temp_survey_folder) if f.endswith('.json')]\n",
    "\n",
    "    rows = []\n",
    "    for human_file in human_files:\n",
    "        response_id = human_file[:-5]\n",
    "        with open(os.path.join(temp_survey_folder, human_file), 'r', encoding='utf-8') as f:\n",
    "            human_response = json.load(f)\n",
    "\n",
    "        for question_key, payload in human_response.items():\n",
    "            if not str(question_key).startswith(\"Question\"):\n",
    "                continue\n",
    "\n",
    "            key = f\"{response_id}_{question_key}\"\n",
    "            if key not in agentic_lookup:\n",
    "                continue\n",
    "\n",
    "            agentic_result = agentic_lookup[key]\n",
    "            final_codes = agentic_result.get(\"final_codes\", {})\n",
    "            iteration_results = agentic_result.get(\"iteration_results\", [])\n",
    "\n",
    "            # ---- Human extraction (40–42 use Codes; 43 uses Reasons)\n",
    "            if question_key == \"Question 43\":\n",
    "                human_characteristics = payload.get(\"Reasons\", []) or []\n",
    "            else:\n",
    "                human_characteristics = []\n",
    "                for c in payload.get(\"Codes\", []) or []:\n",
    "                    ch = c.get(\"Characteristic\", \"\")\n",
    "                    if c.get(\"Actor\", \"\").lower() in (\"\", \"nil\"):  # skip empty/nil\n",
    "                        continue\n",
    "                    if ch:\n",
    "                        human_characteristics.append(ch)\n",
    "\n",
    "            # ---- Agentic FINAL extraction (normalize both schemas)\n",
    "            if question_key == \"Question 43\":\n",
    "                agentic_final_chars = [r.get(\"code\", \"\") for r in final_codes.get(\"reasons\", [])] if isinstance(final_codes, dict) else []\n",
    "            else:\n",
    "                agentic_final_chars = [c.get(\"code\", \"\") for c in final_codes.get(\"codes\", [])] if isinstance(final_codes, dict) else []\n",
    "\n",
    "            # ---- Iteration-level extraction\n",
    "            iter_codes = _extract_iter_codes(iteration_results)  # list[list[str]]\n",
    "            iter_human_jaccards = [_jaccard(human_characteristics, it) for it in iter_codes] if iter_codes else []\n",
    "            best_iter_human_jaccard = max(iter_human_jaccards) if iter_human_jaccards else None\n",
    "            avg_iter_human_jaccard = sum(iter_human_jaccards)/len(iter_human_jaccards) if iter_human_jaccards else None\n",
    "            best_iter_index = (iter_human_jaccards.index(best_iter_human_jaccard) + 1) if iter_human_jaccards else None  # 1-based\n",
    "\n",
    "            # ---- Final vs Human Jaccard\n",
    "            final_vs_human_jaccard = _jaccard(human_characteristics, agentic_final_chars)\n",
    "\n",
    "            rows.append({\n",
    "                \"ResponseId\": response_id,\n",
    "                \"Question\": question_key,\n",
    "                # Human\n",
    "                \"Human_Characteristics\": human_characteristics,\n",
    "                # Agentic (final)\n",
    "                \"Agentic_Final_Characteristics\": agentic_final_chars,\n",
    "                \"Final_vs_Human_Jaccard\": final_vs_human_jaccard,\n",
    "                # Agentic (iterations)\n",
    "                \"Iter_Characteristics\": iter_codes,  # optional: drop if too verbose\n",
    "                \"Best_Iter_vs_Human_Jaccard\": best_iter_human_jaccard,\n",
    "                \"Avg_Iter_vs_Human_Jaccard\": avg_iter_human_jaccard,\n",
    "                \"Best_Iter_Index\": best_iter_index,  # which iteration matched humans best\n",
    "                # Meta already available in your results:\n",
    "                \"Researcher_Agreement\": agentic_result.get(\"researcher_feedback\", {}).get(\"agreement\", False),\n",
    "                \"Quality_Score\": agentic_result.get(\"researcher_feedback\", {}).get(\"quality_score\", 0),\n",
    "                \"Total_Iterations\": agentic_result.get(\"total_iterations\", 0),\n",
    "                \"Iter_Agreement_Avg\": agentic_result.get(\"agreement_metrics\", {}).get(\"iteration_agreement\", {}).get(\"agreement_score\", 0),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e30d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compare_agentic_human_codings_with_iterations(\n",
    "    'agentic_coding_results/all_agentic_results.json',\n",
    "    'temp_survey'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ff508a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResponseId</th>\n",
       "      <th>Question</th>\n",
       "      <th>Human_Characteristics</th>\n",
       "      <th>Agentic_Final_Characteristics</th>\n",
       "      <th>Final_vs_Human_Jaccard</th>\n",
       "      <th>Iter_Characteristics</th>\n",
       "      <th>Best_Iter_vs_Human_Jaccard</th>\n",
       "      <th>Avg_Iter_vs_Human_Jaccard</th>\n",
       "      <th>Best_Iter_Index</th>\n",
       "      <th>Researcher_Agreement</th>\n",
       "      <th>Quality_Score</th>\n",
       "      <th>Total_Iterations</th>\n",
       "      <th>Iter_Agreement_Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R_1gTNWbmwxtRxrf8</td>\n",
       "      <td>Question 40</td>\n",
       "      <td>[Reliable, Understandable]</td>\n",
       "      <td>[Reliable, Understandable]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[Reliable, Understandable], [Reliable, Unders...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R_1gTNWbmwxtRxrf8</td>\n",
       "      <td>Question 41</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Understandable]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[Understandable], [Understandable], [Understa...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R_1gTNWbmwxtRxrf8</td>\n",
       "      <td>Question 42</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R_1VQ5hmwtdpRmnmu</td>\n",
       "      <td>Question 40</td>\n",
       "      <td>[Utility, Understanding, In-depth]</td>\n",
       "      <td>[Specificity, Reflection]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[Specificity, Reflection], [Specificity, Refl...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R_1VQ5hmwtdpRmnmu</td>\n",
       "      <td>Question 41</td>\n",
       "      <td>[No impact, Personal]</td>\n",
       "      <td>[Positive, Negative, Objective]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[Positive, Negative, Objective], [Positive, N...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>R_1VQ5hmwtdpRmnmu</td>\n",
       "      <td>Question 42</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Reflection]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[Reflection], [Reflection], [Reflection]]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R_43e00PZDT3YYIZ2</td>\n",
       "      <td>Question 43</td>\n",
       "      <td>[Unaware, Contextualised]</td>\n",
       "      <td>[Unaware, Unaware]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[[Unaware, Unaware], [Unaware, Unaware], [Unaw...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>R_4aRLJoWmhKsYLOF</td>\n",
       "      <td>Question 43</td>\n",
       "      <td>[Unaware]</td>\n",
       "      <td>[Unaware]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[Unaware], [Unaware, Preference], [Unaware, P...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>R_4AYVtTmMd0q7vte</td>\n",
       "      <td>Question 40</td>\n",
       "      <td>[Ease, Speed, Ease]</td>\n",
       "      <td>[Ease, Speed, Speed]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[Ease, Speed, Speed], [Ease, Speed, Speed], [...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>R_4AYVtTmMd0q7vte</td>\n",
       "      <td>Question 41</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>R_4AYVtTmMd0q7vte</td>\n",
       "      <td>Question 42</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>R_4C4j4KNUq9N3VGC</td>\n",
       "      <td>Question 40</td>\n",
       "      <td>[Speed, Personal, In-depth]</td>\n",
       "      <td>[Speed, Personal, Understanding]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[[Speed, Personal, Understanding], [Speed, Per...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>R_4C4j4KNUq9N3VGC</td>\n",
       "      <td>Question 41</td>\n",
       "      <td>[No impact, Positive]</td>\n",
       "      <td>[Speed, No impact, Personal, Positive]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[[No impact, Speed, Personal, Positive], [Spee...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>R_4C4j4KNUq9N3VGC</td>\n",
       "      <td>Question 42</td>\n",
       "      <td>[Speed, Specificity, Personal]</td>\n",
       "      <td>[Speed, Objective, In-depth, Personal]</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>[[Speed, Objective, In-depth, Personal], [Spee...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>R_4CKOoHgU8hp01rg</td>\n",
       "      <td>Question 40</td>\n",
       "      <td>[Speed, In-depth]</td>\n",
       "      <td>[Speed, Understanding, Understanding, Contextu...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>[[Speed, Understanding, Understanding, Context...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>R_4CKOoHgU8hp01rg</td>\n",
       "      <td>Question 41</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>R_4CKOoHgU8hp01rg</td>\n",
       "      <td>Question 42</td>\n",
       "      <td>[Speed, Contextualised, Relevance, Personal]</td>\n",
       "      <td>[Speed, Specificity, Personal, Contextualised,...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>[[In-depth, Specificity, Personal, Speed, Less...</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.376623</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>R_4Ka5vRl3NuBJ1Af</td>\n",
       "      <td>Question 40</td>\n",
       "      <td>[Specificity, Contextualised]</td>\n",
       "      <td>[In-depth, Specificity]</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[[In-depth, Specificity], [In-depth, Specifici...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>R_4Ka5vRl3NuBJ1Af</td>\n",
       "      <td>Question 41</td>\n",
       "      <td>[Utility]</td>\n",
       "      <td>[Importance]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[[Importance, Less effort], [Importance, Less ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>R_4Ka5vRl3NuBJ1Af</td>\n",
       "      <td>Question 42</td>\n",
       "      <td>[Ease, Risky]</td>\n",
       "      <td>[Ease, Risky]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[Ease, Risky], [Ease, Risky], [Ease, Risky]]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>R_4P0PnRK3k4DjAxR</td>\n",
       "      <td>Question 40</td>\n",
       "      <td>[Ease]</td>\n",
       "      <td>[Ease, Specificity]</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>[[Ease, Specificity], [Ease, Specificity], [Ea...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>R_4P0PnRK3k4DjAxR</td>\n",
       "      <td>Question 41</td>\n",
       "      <td>[Understanding]</td>\n",
       "      <td>[Understanding]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[Understanding], [Understanding], [Understand...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>R_4P0PnRK3k4DjAxR</td>\n",
       "      <td>Question 42</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>R_4sn9pI5HdvhV5gI</td>\n",
       "      <td>Question 43</td>\n",
       "      <td>[Unaware, Trustworthy]</td>\n",
       "      <td>[Unaware, Trustworthy]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[[Unaware, Trustworthy], [Unaware, Trustworthy...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ResponseId     Question  \\\n",
       "0   R_1gTNWbmwxtRxrf8  Question 40   \n",
       "1   R_1gTNWbmwxtRxrf8  Question 41   \n",
       "2   R_1gTNWbmwxtRxrf8  Question 42   \n",
       "3   R_1VQ5hmwtdpRmnmu  Question 40   \n",
       "4   R_1VQ5hmwtdpRmnmu  Question 41   \n",
       "5   R_1VQ5hmwtdpRmnmu  Question 42   \n",
       "6   R_43e00PZDT3YYIZ2  Question 43   \n",
       "7   R_4aRLJoWmhKsYLOF  Question 43   \n",
       "8   R_4AYVtTmMd0q7vte  Question 40   \n",
       "9   R_4AYVtTmMd0q7vte  Question 41   \n",
       "10  R_4AYVtTmMd0q7vte  Question 42   \n",
       "11  R_4C4j4KNUq9N3VGC  Question 40   \n",
       "12  R_4C4j4KNUq9N3VGC  Question 41   \n",
       "13  R_4C4j4KNUq9N3VGC  Question 42   \n",
       "14  R_4CKOoHgU8hp01rg  Question 40   \n",
       "15  R_4CKOoHgU8hp01rg  Question 41   \n",
       "16  R_4CKOoHgU8hp01rg  Question 42   \n",
       "17  R_4Ka5vRl3NuBJ1Af  Question 40   \n",
       "18  R_4Ka5vRl3NuBJ1Af  Question 41   \n",
       "19  R_4Ka5vRl3NuBJ1Af  Question 42   \n",
       "20  R_4P0PnRK3k4DjAxR  Question 40   \n",
       "21  R_4P0PnRK3k4DjAxR  Question 41   \n",
       "22  R_4P0PnRK3k4DjAxR  Question 42   \n",
       "23  R_4sn9pI5HdvhV5gI  Question 43   \n",
       "\n",
       "                           Human_Characteristics  \\\n",
       "0                     [Reliable, Understandable]   \n",
       "1                                             []   \n",
       "2                                             []   \n",
       "3             [Utility, Understanding, In-depth]   \n",
       "4                          [No impact, Personal]   \n",
       "5                                             []   \n",
       "6                      [Unaware, Contextualised]   \n",
       "7                                      [Unaware]   \n",
       "8                            [Ease, Speed, Ease]   \n",
       "9                                             []   \n",
       "10                                            []   \n",
       "11                   [Speed, Personal, In-depth]   \n",
       "12                         [No impact, Positive]   \n",
       "13                [Speed, Specificity, Personal]   \n",
       "14                             [Speed, In-depth]   \n",
       "15                                            []   \n",
       "16  [Speed, Contextualised, Relevance, Personal]   \n",
       "17                 [Specificity, Contextualised]   \n",
       "18                                     [Utility]   \n",
       "19                                 [Ease, Risky]   \n",
       "20                                        [Ease]   \n",
       "21                               [Understanding]   \n",
       "22                                            []   \n",
       "23                        [Unaware, Trustworthy]   \n",
       "\n",
       "                        Agentic_Final_Characteristics  Final_vs_Human_Jaccard  \\\n",
       "0                          [Reliable, Understandable]                1.000000   \n",
       "1                                    [Understandable]                0.000000   \n",
       "2                                                  []                1.000000   \n",
       "3                           [Specificity, Reflection]                0.000000   \n",
       "4                     [Positive, Negative, Objective]                0.000000   \n",
       "5                                        [Reflection]                0.000000   \n",
       "6                                  [Unaware, Unaware]                0.500000   \n",
       "7                                           [Unaware]                1.000000   \n",
       "8                                [Ease, Speed, Speed]                1.000000   \n",
       "9                                                  []                1.000000   \n",
       "10                                                 []                1.000000   \n",
       "11                   [Speed, Personal, Understanding]                0.500000   \n",
       "12             [Speed, No impact, Personal, Positive]                0.500000   \n",
       "13             [Speed, Objective, In-depth, Personal]                0.400000   \n",
       "14  [Speed, Understanding, Understanding, Contextu...                0.142857   \n",
       "15                                                 []                1.000000   \n",
       "16  [Speed, Specificity, Personal, Contextualised,...                0.428571   \n",
       "17                            [In-depth, Specificity]                0.333333   \n",
       "18                                       [Importance]                0.000000   \n",
       "19                                      [Ease, Risky]                1.000000   \n",
       "20                                [Ease, Specificity]                0.500000   \n",
       "21                                    [Understanding]                1.000000   \n",
       "22                                                 []                1.000000   \n",
       "23                             [Unaware, Trustworthy]                1.000000   \n",
       "\n",
       "                                 Iter_Characteristics  \\\n",
       "0   [[Reliable, Understandable], [Reliable, Unders...   \n",
       "1   [[Understandable], [Understandable], [Understa...   \n",
       "2                                        [[], [], []]   \n",
       "3   [[Specificity, Reflection], [Specificity, Refl...   \n",
       "4   [[Positive, Negative, Objective], [Positive, N...   \n",
       "5          [[Reflection], [Reflection], [Reflection]]   \n",
       "6   [[Unaware, Unaware], [Unaware, Unaware], [Unaw...   \n",
       "7   [[Unaware], [Unaware, Preference], [Unaware, P...   \n",
       "8   [[Ease, Speed, Speed], [Ease, Speed, Speed], [...   \n",
       "9                                        [[], [], []]   \n",
       "10                                       [[], [], []]   \n",
       "11  [[Speed, Personal, Understanding], [Speed, Per...   \n",
       "12  [[No impact, Speed, Personal, Positive], [Spee...   \n",
       "13  [[Speed, Objective, In-depth, Personal], [Spee...   \n",
       "14  [[Speed, Understanding, Understanding, Context...   \n",
       "15                                       [[], [], []]   \n",
       "16  [[In-depth, Specificity, Personal, Speed, Less...   \n",
       "17  [[In-depth, Specificity], [In-depth, Specifici...   \n",
       "18  [[Importance, Less effort], [Importance, Less ...   \n",
       "19      [[Ease, Risky], [Ease, Risky], [Ease, Risky]]   \n",
       "20  [[Ease, Specificity], [Ease, Specificity], [Ea...   \n",
       "21  [[Understanding], [Understanding], [Understand...   \n",
       "22                                       [[], [], []]   \n",
       "23  [[Unaware, Trustworthy], [Unaware, Trustworthy...   \n",
       "\n",
       "    Best_Iter_vs_Human_Jaccard  Avg_Iter_vs_Human_Jaccard  Best_Iter_Index  \\\n",
       "0                     1.000000                   1.000000                1   \n",
       "1                     0.000000                   0.000000                1   \n",
       "2                     1.000000                   1.000000                1   \n",
       "3                     0.000000                   0.000000                1   \n",
       "4                     0.000000                   0.000000                1   \n",
       "5                     0.000000                   0.000000                1   \n",
       "6                     0.500000                   0.500000                1   \n",
       "7                     1.000000                   0.666667                1   \n",
       "8                     1.000000                   1.000000                1   \n",
       "9                     1.000000                   1.000000                1   \n",
       "10                    1.000000                   1.000000                1   \n",
       "11                    0.500000                   0.500000                1   \n",
       "12                    0.500000                   0.500000                1   \n",
       "13                    0.400000                   0.400000                1   \n",
       "14                    0.142857                   0.142857                1   \n",
       "15                    1.000000                   1.000000                1   \n",
       "16                    0.428571                   0.376623                2   \n",
       "17                    0.333333                   0.333333                1   \n",
       "18                    0.000000                   0.000000                1   \n",
       "19                    1.000000                   1.000000                1   \n",
       "20                    0.500000                   0.500000                1   \n",
       "21                    1.000000                   1.000000                1   \n",
       "22                    1.000000                   1.000000                1   \n",
       "23                    1.000000                   1.000000                1   \n",
       "\n",
       "    Researcher_Agreement  Quality_Score  Total_Iterations  Iter_Agreement_Avg  \n",
       "0                   True              5                 3            1.000000  \n",
       "1                   True              5                 3            1.000000  \n",
       "2                   True              5                 3            1.000000  \n",
       "3                   True              5                 3            1.000000  \n",
       "4                   True              5                 3            1.000000  \n",
       "5                   True              5                 3            1.000000  \n",
       "6                   True              5                 3            1.000000  \n",
       "7                   True              5                 3            1.000000  \n",
       "8                   True              5                 3            1.000000  \n",
       "9                   True              5                 3            1.000000  \n",
       "10                  True              5                 3            1.000000  \n",
       "11                  True              5                 3            1.000000  \n",
       "12                  True              5                 3            1.000000  \n",
       "13                  True              5                 3            1.000000  \n",
       "14                  True              5                 3            1.000000  \n",
       "15                  True              5                 3            1.000000  \n",
       "16                  True              5                 3            0.733333  \n",
       "17                  True              5                 3            1.000000  \n",
       "18                 False              3                 6            1.000000  \n",
       "19                  True              5                 3            1.000000  \n",
       "20                  True              5                 3            1.000000  \n",
       "21                  True              5                 3            1.000000  \n",
       "22                  True              5                 3            1.000000  \n",
       "23                  True              5                 3            1.000000  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d5f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quali_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
