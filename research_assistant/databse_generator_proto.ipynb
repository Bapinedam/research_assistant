{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic Models\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "\n",
    "class BibliographicMetadata(BaseModel):\n",
    "    paper_id: str\n",
    "    title: str\n",
    "    authors: List[str]\n",
    "    publication_year: int\n",
    "    venue: str\n",
    "    doi_url: str\n",
    "    research_domain: str\n",
    "\n",
    "\n",
    "class SampleCharacteristics(BaseModel):\n",
    "    interview_count: Optional[Union[int, str]] = None\n",
    "    participant_demographics: str\n",
    "    data_collection_method: str\n",
    "\n",
    "\n",
    "class MethodologicalFramework(BaseModel):\n",
    "    analysis_type: str\n",
    "    theoretical_framework: str\n",
    "    sample_characteristics: SampleCharacteristics\n",
    "\n",
    "\n",
    "class WorkflowArchitecture(BaseModel):\n",
    "    preprocessing_steps: List[str]\n",
    "    analysis_pipeline: List[str]\n",
    "    postprocessing_steps: List[str]\n",
    "\n",
    "\n",
    "class TechnicalPipeline(BaseModel):\n",
    "    automation_level: str\n",
    "    software_tools: List[str]\n",
    "    computational_methods: List[str]\n",
    "    workflow_architecture: WorkflowArchitecture\n",
    "    coding_framework: str\n",
    "\n",
    "\n",
    "class PromptingStrategy(BaseModel):\n",
    "    approach_type: str\n",
    "    prompt_examples: List[str]\n",
    "    engineering_techniques: List[str]\n",
    "    model_specifications: str\n",
    "\n",
    "\n",
    "class PromptEngineering(BaseModel):\n",
    "    prompting_strategy: PromptingStrategy\n",
    "\n",
    "\n",
    "class EvaluationFramework(BaseModel):\n",
    "    metrics_employed: List[str]\n",
    "    validation_methodology: str\n",
    "    performance_indicators: str\n",
    "\n",
    "\n",
    "class EmpiricalResults(BaseModel):\n",
    "    primary_findings: List[str]\n",
    "    evaluation_framework: EvaluationFramework\n",
    "    methodological_limitations: List[str]\n",
    "\n",
    "\n",
    "class QualityAssurance(BaseModel):\n",
    "    reliability_measures: List[str]\n",
    "    validity_approaches: List[str]\n",
    "    bias_mitigation_strategies: List[str]\n",
    "\n",
    "\n",
    "class ResearchImpact(BaseModel):\n",
    "    novel_contributions: List[str]\n",
    "    practical_applications: List[str]\n",
    "    future_research_directions: List[str]\n",
    "    scalability_considerations: str\n",
    "\n",
    "\n",
    "class ResearchStudy(BaseModel):\n",
    "    bibliographic_metadata: BibliographicMetadata\n",
    "    methodological_framework: MethodologicalFramework\n",
    "    technical_pipeline: TechnicalPipeline\n",
    "    prompt_engineering: PromptEngineering\n",
    "    empirical_results: EmpiricalResults\n",
    "    quality_assurance: QualityAssurance\n",
    "    research_impact: ResearchImpact\n",
    "\n",
    "class ResearchState(BaseModel):\n",
    "    bibliographic_metadata: Optional[BibliographicMetadata] = None\n",
    "    methodological_framework: Optional[MethodologicalFramework] = None\n",
    "    technical_pipeline: Optional[TechnicalPipeline] = None\n",
    "    prompt_engineering: Optional[PromptEngineering] = None\n",
    "    empirical_results: Optional[EmpiricalResults] = None\n",
    "    quality_assurance: Optional[QualityAssurance] = None\n",
    "    research_impact: Optional[ResearchImpact] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction Schema\n",
    "\n",
    "section_schemas = {\n",
    "    \"bibliographic_metadata\": {\n",
    "        \"bibliographic_metadata\": {\n",
    "            \"paper_id\": \"string\",\n",
    "            \"title\": \"string\",\n",
    "            \"authors\": [\"string\"],\n",
    "            \"publication_year\": \"integer\",\n",
    "            \"venue\": \"string\",\n",
    "            \"doi_url\": \"string\",\n",
    "            \"research_domain\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"methodological_framework\": {\n",
    "        \"methodological_framework\": {\n",
    "            \"analysis_type\": \"string\",\n",
    "            \"theoretical_framework\": \"string\",\n",
    "            \"sample_characteristics\": {\n",
    "                \"interview_count\": \"integer\",\n",
    "                \"participant_demographics\": \"string\",\n",
    "                \"data_collection_method\": \"string\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"technical_pipeline\": {\n",
    "        \"technical_pipeline\": {\n",
    "            \"automation_level\": \"string\",\n",
    "            \"software_tools\": [\"string\"],\n",
    "            \"computational_methods\": [\"string\"],\n",
    "            \"workflow_architecture\": {\n",
    "                \"preprocessing_steps\": [\"string\"],\n",
    "                \"analysis_pipeline\": [\"string\"],\n",
    "                \"postprocessing_steps\": [\"string\"]\n",
    "            },\n",
    "            \"coding_framework\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"prompt_engineering\": {\n",
    "        \"prompt_engineering\": {\n",
    "            \"prompting_strategy\": {\n",
    "                \"approach_type\": \"string\",\n",
    "                \"prompt_examples\": [\"string\"],\n",
    "                \"engineering_techniques\": [\"string\"],\n",
    "                \"model_specifications\": \"string\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"empirical_results\": {\n",
    "        \"empirical_results\": {\n",
    "            \"primary_findings\": [\"string\"],\n",
    "            \"evaluation_framework\": {\n",
    "                \"metrics_employed\": [\"string\"],\n",
    "                \"validation_methodology\": \"string\",\n",
    "                \"performance_indicators\": \"string\"\n",
    "            },\n",
    "            \"methodological_limitations\": [\"string\"]\n",
    "        }\n",
    "    },\n",
    "    \"quality_assurance\": {\n",
    "        \"quality_assurance\": {\n",
    "            \"reliability_measures\": [\"string\"],\n",
    "            \"validity_approaches\": [\"string\"],\n",
    "            \"bias_mitigation_strategies\": [\"string\"]\n",
    "        }\n",
    "    },\n",
    "    \"research_impact\": {\n",
    "        \"research_impact\": {\n",
    "            \"novel_contributions\": [\"string\"],\n",
    "            \"practical_applications\": [\"string\"],\n",
    "            \"future_research_directions\": [\"string\"],\n",
    "            \"scalability_considerations\": \"string\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_base = \"\"\"\n",
    "You are an expert research assistant specializing in qualitative research methodology and computational text analysis. Your task is to perform systematic data extraction from academic literature focusing on deductive qualitative analysis pipelines for interview data.\n",
    "\n",
    "# Task Definition\n",
    "Analyze the provided research document and extract structured information related to deductive qualitative analysis methodologies, computational workflows, and evaluation frameworks. Focus on identifying technical specifications, methodological approaches, and empirical findings relevant to automated or semi-automated qualitative data analysis pipelines.\n",
    "\n",
    "# Extraction Schema\n",
    "\n",
    "Extract the following information and return it as a structured JSON object with the specified schema:\n",
    "{schema}\n",
    "\n",
    "Extraction Guidelines\n",
    "- Completeness: Extract all available information; use \"not_specified\" for missing data\n",
    "- Precision: Maintain technical terminology and methodological specificity\n",
    "- Contextual Accuracy: Preserve the original meaning and technical context\n",
    "- Standardization: Use consistent terminology across extractions\n",
    "- Null Handling: Use empty arrays [] for missing list items, null for missing values\n",
    "\n",
    "# IMPORTANT: Return ONLY valid JSON without any additional text, explanations, or markdown formatting.\n",
    "\n",
    "# Texto a analizar\n",
    "{document_text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0.002)\n",
    "\n",
    "# 1. Define prompts for each section\n",
    "prompts = {}\n",
    "for section, schema in section_schemas.items():\n",
    "    prompts[section] = PromptTemplate(\n",
    "        input_variables=[\"document_text\", \"schema\"],\n",
    "        template=template_base\n",
    "    )\n",
    "\n",
    "# 2. Create LLMChain for each section\n",
    "chains = {}\n",
    "for section, prompt_template in prompts.items():\n",
    "    chains[section] = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt_template.partial(schema=json.dumps(section_schemas[section], indent=2)),\n",
    "        output_key=section\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def run_extraction_pipeline(chains: Dict[str, LLMChain], output_dir=\"outputs\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for file in Path(\"../files\").glob(\"*.pdf\"):\n",
    "        print(f\"\\nÔøΩÔøΩ Procesando archivo: {file.name}\")\n",
    "        document_text = extract_text_from_pdf(str(file))\n",
    "\n",
    "        state_data = {}\n",
    "\n",
    "        for section, chain in chains.items():\n",
    "            try:\n",
    "                print(f\"  ÔøΩÔøΩ Extrayendo secci√≥n: {section}\")\n",
    "                response_dict = chain.invoke(input=document_text)\n",
    "                \n",
    "                # Extract the actual response text from the dictionary\n",
    "                response = response_dict[section]\n",
    "                \n",
    "                # Debug: Print the actual response to see what we're getting\n",
    "                print(f\"    Raw response: {response[:200]}...\")  # First 200 chars\n",
    "                \n",
    "                # Clean the response - remove any non-JSON text\n",
    "                response = response.strip()\n",
    "                \n",
    "                # Try to extract JSON from the response if it contains extra text\n",
    "                if response.startswith('```json'):\n",
    "                    response = response.split('```json')[1].split('```')[0].strip()\n",
    "                elif response.startswith('```'):\n",
    "                    response = response.split('```')[1].split('```')[0].strip()\n",
    "                \n",
    "                # Validate that we have content before parsing\n",
    "                if not response:\n",
    "                    print(f\"    ‚ö†Ô∏è Empty response for {section}\")\n",
    "                    state_data[section] = None\n",
    "                    continue\n",
    "                \n",
    "                parsed = json.loads(response)\n",
    "                if section in parsed:\n",
    "                    state_data[section] = parsed[section]\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Clave esperada '{section}' no encontrada en la respuesta.\")\n",
    "                    state_data[section] = None\n",
    "\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"  ‚ö†Ô∏è JSON parsing error for '{section}': {e}\")\n",
    "                print(f\"    Response content: {response}\")\n",
    "                state_data[section] = None\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Error al procesar '{section}': {e}\")\n",
    "                state_data[section] = None\n",
    "\n",
    "        # Construye el objeto ResearchState\n",
    "        try:\n",
    "            research_state = ResearchState.model_validate(state_data)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error al crear ResearchState: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Guarda JSON\n",
    "        json_path = Path(output_dir) / f\"{file.stem}.json\"\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(research_state.model_dump_json(indent=2))\n",
    "\n",
    "        # Guarda CSV (aplanado)\n",
    "        df = pd.json_normalize(research_state.model_dump())\n",
    "        csv_path = Path(output_dir) / f\"{file.stem}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        print(f\"‚úÖ Archivo procesado: {file.name} ‚Üí {json_path.name}, {csv_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ÔøΩÔøΩ Procesando archivo: 2401.04122v3.pdf\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"arXiv:2401.04122v3\",\n",
      "    \"title\": \"From Prompt Engineering to Prompt Science With Human in the Loop\",\n",
      "    \"authors\": [\n",
      "      \"Chirag Shah\"\n",
      "    ],\n",
      "    \"...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive qualitative analysis with human-in-the-loop validation, inspired by qualitative codebook construction and coding\",\n",
      "    \"theoretical_fr...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"semi-automated (human-in-the-loop with LLMs for labeling, coding, and prompt generation; automation increases with validated prompts and codebooks,...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"multi-phase, human-in-the-loop, codebook-inspired deductive prompt construction and validation\",\n",
      "      \"prompt_examples\"...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"A multi-phase, human-in-the-loop methodology inspired by qualitative codebook construction increases the reliability, transparency, and gene...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Involvement of at least two qualified researchers in the entire process\",\n",
      "      \"Independent application of criteria by multiple assesso...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Proposes a multi-phase, human-in-the-loop methodology for transforming ad-hoc prompt engineering into a systematic, verifiable, and replica...\n",
      "‚úÖ Archivo procesado: 2401.04122v3.pdf ‚Üí 2401.04122v3.json, 2401.04122v3.csv\n",
      "\n",
      "ÔøΩÔøΩ Procesando archivo: 2402.01386v1.pdf\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"arXiv:2402.01386v1\",\n",
      "    \"title\": \"Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis\",\n",
      "  ...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive_qualitative_analysis\",\n",
      "    \"theoretical_framework\": \"multi-agent LLM-based automation of qualitative data analysis (encompassing thema...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"fully_automated (autonomous execution of qualitative data analysis tasks by LLM-based multi-agent system with minimal manual intervention)\",\n",
      "    \"s...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"multi-agent, task-specialized prompting with user-specified qualitative analysis method\",\n",
      "      \"prompt_examples\": [\n",
      "   ...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"The LLM-based multi-agent model can autonomously perform various qualitative data analysis approaches, including thematic analysis, grounded...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Practitioner-based evaluation involving 10 practitioners from diverse professional backgrounds to assess model performance\",\n",
      "      \"Use ...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Introduction of an LLM-based multi-agent model that automates the entire qualitative data analysis pipeline, including thematic analysis, g...\n",
      "‚úÖ Archivo procesado: 2402.01386v1.pdf ‚Üí 2402.01386v1.json, 2402.01386v1.csv\n",
      "\n",
      "ÔøΩÔøΩ Procesando archivo: 2411.14473v4.pdf\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"arXiv:2411.14473v4\",\n",
      "    \"title\": \"Large Language Model for Qualitative Research: A Systematic Mapping Study\",\n",
      "    \"authors\": [\n",
      "      \"Cau√£ Ferreira Ba...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive_qualitative_analysis\",\n",
      "    \"theoretical_framework\": \"content_analysis; grounded_theory; thematic_analysis\",\n",
      "    \"sample_characteristic...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"semi-automated to fully automated (depending on study and workflow; LLMs automate coding, theme extraction, and categorization, but human oversight...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"structured prompt engineering with iterative refinement\",\n",
      "      \"prompt_examples\": [\n",
      "        \"Prompts designed to extrac...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"LLMs, particularly ChatGPT and its variants, are increasingly used to automate and enhance qualitative analysis across domains such as healt...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Comparison with human analysis\",\n",
      "      \"Accuracy\",\n",
      "      \"Precision\",\n",
      "      \"Recall\",\n",
      "      \"F1 Score\",\n",
      "      \"Cohen‚Äôs Kappa\",\n",
      "      \"Ag...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Systematic mapping of the state of the art on the use of Large Language Models (LLMs) in deductive and inductive qualitative analysis, with...\n",
      "‚úÖ Archivo procesado: 2411.14473v4.pdf ‚Üí 2411.14473v4.json, 2411.14473v4.csv\n",
      "\n",
      "ÔøΩÔøΩ Procesando archivo: 3636555.3636910.pdf\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"not_specified\",\n",
      "    \"title\": \"Prompt-based and Fine-tuned GPT Models for Context-Dependent and -Independent Deductive Coding in Social Annotation\",\n",
      "   ...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive qualitative coding; automated and semi-automated content analysis using prompt-based and fine-tuned GPT models\",\n",
      "    \"theoretical_fram...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"semi-automated (prompt-based and fine-tuned GPT models for deductive coding; human-in-the-loop for codebook development and evaluation)\",\n",
      "    \"soft...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"iterative manual prompt engineering with codebook-centered, chain-of-thought, step-based, and multi-prompt techniques\",\n",
      "...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"Prompt engineering with GPT-3.5-turbo achieves fair to substantial agreement with expert-labeled data for context-independent deductive codi...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Cohen‚Äôs Kappa calculated between human raters for each coding dimension (e.g., .63 for Appraisal, .95 for Questioning, .80 for Theorizin...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"First systematic evaluation of GPT-based prompt engineering and fine-tuning for both context-dependent and context-independent deductive co...\n",
      "‚úÖ Archivo procesado: 3636555.3636910.pdf ‚Üí 3636555.3636910.json, 3636555.3636910.csv\n",
      "\n",
      "ÔøΩÔøΩ Procesando archivo: 3706468.3706564.pdf\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"not_specified\",\n",
      "    \"title\": \"When the Prompt becomes the Codebook: Grounded Prompt Engineering (GROPROE) and its application to Belonging Analytics\",\n",
      "...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive qualitative analysis using automated and semi-automated computational workflows\",\n",
      "    \"theoretical_framework\": \"Four domains of belong...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"semi-automated\",\n",
      "    \"software_tools\": [\n",
      "      \"Azure Playground\",\n",
      "      \"GPT-4\"\n",
      "    ],\n",
      "    \"computational_methods\": [\n",
      "      \"Large Language Model-...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"Grounded Prompt Engineering (GROPROE); theory-driven, codebook-based, iterative refinement\",\n",
      "      \"prompt_examples\": [\n",
      "...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"The GROPROE (Grounded Prompt Engineering) process enables systematic, theory-grounded prompt engineering for deductive qualitative analysis ...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Inter-Rater Reliability (IRR) between human coders calculated using Cohen‚Äôs Kappa\",\n",
      "      \"Inter-Annotator Reliability (IAR) between hum...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Introduction of Grounded Prompt Engineering (GROPROE) as a systematic, literature-grounded process for developing prompts for deductive qua...\n",
      "‚úÖ Archivo procesado: 3706468.3706564.pdf ‚Üí 3706468.3706564.json, 3706468.3706564.csv\n",
      "\n",
      "ÔøΩÔøΩ Procesando archivo: ChatGPT_ICQE_FinalVersion.pdf\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"not_specified\",\n",
      "    \"title\": \"From nCoder to ChatGPT: From Automated Coding to Refining Human Coding\",\n",
      "    \"authors\": [\n",
      "      \"Andres Felipe Zambrano\",...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive qualitative coding; automated and semi-automated coding comparison\",\n",
      "    \"theoretical_framework\": \"Quantitative Ethnography; construct...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"semi-automated\",\n",
      "    \"software_tools\": [\n",
      "      \"nCoder\",\n",
      "      \"ChatGPT (GPT-4)\"\n",
      "    ],\n",
      "    \"computational_methods\": [\n",
      "      \"regular expressions\",...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"iterative refinement with human-in-the-loop\",\n",
      "      \"prompt_examples\": [\n",
      "        \"For each construct, provide ChatGPT wi...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"Both ChatGPT (GPT-4) and nCoder have distinct advantages and disadvantages for automated coding of qualitative interview data, depending on ...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Cohen's Kappa\",\n",
      "      \"Precision\",\n",
      "      \"Recall\",\n",
      "      \"Shaffer's rho\",\n",
      "      \"Inter-rater agreement\"\n",
      "    ],\n",
      "    \"validity_approaches\"...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Systematic comparison of ChatGPT (GPT-4) and nCoder for automated coding in Quantitative Ethnography (QE) using a real-world dataset of gov...\n",
      "‚úÖ Archivo procesado: ChatGPT_ICQE_FinalVersion.pdf ‚Üí ChatGPT_ICQE_FinalVersion.json, ChatGPT_ICQE_FinalVersion.csv\n",
      "\n",
      "ÔøΩÔøΩ Procesando archivo: extracted_pages_134_149.pdf\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"not_specified\",\n",
      "    \"title\": \"ChatGPT for Education Research: Exploring the Potential of Large Language Models for Qualitative Codebook Development\",\n",
      " ...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"inductive qualitative coding (codebook development and evaluation), with comparative analysis of manual, automated, and hybrid (human-LLM) workf...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"hybrid (manual, semi-automated, and fully automated approaches compared; focus on hybrid human-LLM collaboration)\",\n",
      "    \"software_tools\": [\n",
      "      \"...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"hybrid and automated prompting for codebook development and refinement in qualitative analysis\",\n",
      "      \"prompt_examples\"...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"Hybrid approaches (combining human and ChatGPT involvement in codebook development or refinement) produced codebooks that were more reliably...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Cohen‚Äôs kappa (Œ∫) used to assess consistency of code applications between coders\",\n",
      "      \"Percent agreement calculated across two rounds...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Systematic comparison of four codebook development pipelines for qualitative interview data: fully manual, fully automated (ChatGPT-only), ...\n",
      "‚úÖ Archivo procesado: extracted_pages_134_149.pdf ‚Üí extracted_pages_134_149.json, extracted_pages_134_149.csv\n",
      "\n",
      "ÔøΩÔøΩ Procesando archivo: tai-et-al-2024-an-examination-of-the-use-of-large-language-models-to-aid-analysis-of-textual-data.pdf\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"16094069241231168\",\n",
      "    \"title\": \"An Examination of the Use of Large Language Models to Aid Analysis of Textual Data\",\n",
      "    \"authors\": [\n",
      "      \"Robert H...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive qualitative coding using large language models (LLMs) as analytic instruments, with comparison to traditional human coding\",\n",
      "    \"theo...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"semi-automated (human-in-the-loop, LLM-assisted deductive coding)\",\n",
      "    \"software_tools\": [\n",
      "      \"ChatGPT 3.5 (OpenAI)\",\n",
      "      \"Dedoose (qualitati...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"codebook-driven deductive binary prompting\",\n",
      "      \"prompt_examples\": [\n",
      "        \"Can you find the five characteristics i...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"Large Language Models (LLMs), specifically ChatGPT 3.5, can be systematically used to support deductive qualitative coding of interview data...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Inter-rater reliability using Cohen‚Äôs kappa statistic\",\n",
      "      \"Inter-rater reliability using Fleiss‚Äô kappa statistic\",\n",
      "      \"Calculatio...\n",
      "  ÔøΩÔøΩ Extrayendo secci√≥n: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Proposes a systematic methodology for using Large Language Models (LLMs) to support traditional deductive coding in qualitative research.\",...\n",
      "‚úÖ Archivo procesado: tai-et-al-2024-an-examination-of-the-use-of-large-language-models-to-aid-analysis-of-textual-data.pdf ‚Üí tai-et-al-2024-an-examination-of-the-use-of-large-language-models-to-aid-analysis-of-textual-data.json, tai-et-al-2024-an-examination-of-the-use-of-large-language-models-to-aid-analysis-of-textual-data.csv\n"
     ]
    }
   ],
   "source": [
    "run_extraction_pipeline(chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded: 2401.04122v3.json\n",
      "‚úÖ Loaded: 2402.01386v1.json\n",
      "‚úÖ Loaded: 2411.14473v4.json\n",
      "‚úÖ Loaded: 3636555.3636910.json\n",
      "‚úÖ Loaded: 3706468.3706564.json\n",
      "‚úÖ Loaded: ChatGPT_ICQE_FinalVersion.json\n",
      "‚úÖ Loaded: extracted_pages_134_149.json\n",
      "‚úÖ Loaded: tai-et-al-2024-an-examination-of-the-use-of-large-language-models-to-aid-analysis-of-textual-data.json\n",
      "\n",
      "üìä Combined DataFrame shape: (8, 36)\n",
      "üìÅ Total files processed: 8\n"
     ]
    }
   ],
   "source": [
    "def load_all_json_to_dataframe(outputs_path=\"../research_assistant/outputs\"):\n",
    "    \"\"\"\n",
    "    Load all JSON files from the outputs directory and combine them into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        outputs_path (str): Path to the directory containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with all JSON data\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    \n",
    "    # Get all JSON files in the directory\n",
    "    json_files = list(Path(outputs_path).glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"No JSON files found in the specified directory\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # List to store all dataframes\n",
    "    all_dataframes = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            # Read JSON file\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.json_normalize(data)\n",
    "            \n",
    "            # Add source file information\n",
    "            df['source_file'] = json_file.name\n",
    "            \n",
    "            all_dataframes.append(df)\n",
    "            print(f\"‚úÖ Loaded: {json_file.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {json_file.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_dataframes:\n",
    "        print(\"No valid JSON files could be loaded\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\nüìä Combined DataFrame shape: {combined_df.shape}\")\n",
    "    print(f\"üìÅ Total files processed: {len(all_dataframes)}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Example usage\n",
    "df_combined = load_all_json_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                                                    Non-Null Count  Dtype \n",
      "---  ------                                                                    --------------  ----- \n",
      " 0   bibliographic_metadata.paper_id                                           8 non-null      object\n",
      " 1   bibliographic_metadata.title                                              8 non-null      object\n",
      " 2   bibliographic_metadata.authors                                            8 non-null      object\n",
      " 3   bibliographic_metadata.publication_year                                   8 non-null      int64 \n",
      " 4   bibliographic_metadata.venue                                              8 non-null      object\n",
      " 5   bibliographic_metadata.doi_url                                            8 non-null      object\n",
      " 6   bibliographic_metadata.research_domain                                    8 non-null      object\n",
      " 7   methodological_framework.analysis_type                                    8 non-null      object\n",
      " 8   methodological_framework.theoretical_framework                            8 non-null      object\n",
      " 9   methodological_framework.sample_characteristics.interview_count           8 non-null      object\n",
      " 10  methodological_framework.sample_characteristics.participant_demographics  8 non-null      object\n",
      " 11  methodological_framework.sample_characteristics.data_collection_method    8 non-null      object\n",
      " 12  technical_pipeline.automation_level                                       8 non-null      object\n",
      " 13  technical_pipeline.software_tools                                         8 non-null      object\n",
      " 14  technical_pipeline.computational_methods                                  8 non-null      object\n",
      " 15  technical_pipeline.workflow_architecture.preprocessing_steps              8 non-null      object\n",
      " 16  technical_pipeline.workflow_architecture.analysis_pipeline                8 non-null      object\n",
      " 17  technical_pipeline.workflow_architecture.postprocessing_steps             8 non-null      object\n",
      " 18  technical_pipeline.coding_framework                                       8 non-null      object\n",
      " 19  prompt_engineering.prompting_strategy.approach_type                       8 non-null      object\n",
      " 20  prompt_engineering.prompting_strategy.prompt_examples                     8 non-null      object\n",
      " 21  prompt_engineering.prompting_strategy.engineering_techniques              8 non-null      object\n",
      " 22  prompt_engineering.prompting_strategy.model_specifications                8 non-null      object\n",
      " 23  empirical_results.primary_findings                                        8 non-null      object\n",
      " 24  empirical_results.evaluation_framework.metrics_employed                   8 non-null      object\n",
      " 25  empirical_results.evaluation_framework.validation_methodology             8 non-null      object\n",
      " 26  empirical_results.evaluation_framework.performance_indicators             8 non-null      object\n",
      " 27  empirical_results.methodological_limitations                              8 non-null      object\n",
      " 28  quality_assurance.reliability_measures                                    8 non-null      object\n",
      " 29  quality_assurance.validity_approaches                                     8 non-null      object\n",
      " 30  quality_assurance.bias_mitigation_strategies                              8 non-null      object\n",
      " 31  research_impact.novel_contributions                                       8 non-null      object\n",
      " 32  research_impact.practical_applications                                    8 non-null      object\n",
      " 33  research_impact.future_research_directions                                8 non-null      object\n",
      " 34  research_impact.scalability_considerations                                8 non-null      object\n",
      " 35  source_file                                                               8 non-null      object\n",
      "dtypes: int64(1), object(35)\n",
      "memory usage: 2.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bibliographic_metadata.paper_id</th>\n",
       "      <th>bibliographic_metadata.title</th>\n",
       "      <th>bibliographic_metadata.authors</th>\n",
       "      <th>bibliographic_metadata.publication_year</th>\n",
       "      <th>bibliographic_metadata.venue</th>\n",
       "      <th>bibliographic_metadata.doi_url</th>\n",
       "      <th>bibliographic_metadata.research_domain</th>\n",
       "      <th>methodological_framework.analysis_type</th>\n",
       "      <th>methodological_framework.theoretical_framework</th>\n",
       "      <th>methodological_framework.sample_characteristics.interview_count</th>\n",
       "      <th>...</th>\n",
       "      <th>empirical_results.evaluation_framework.performance_indicators</th>\n",
       "      <th>empirical_results.methodological_limitations</th>\n",
       "      <th>quality_assurance.reliability_measures</th>\n",
       "      <th>quality_assurance.validity_approaches</th>\n",
       "      <th>quality_assurance.bias_mitigation_strategies</th>\n",
       "      <th>research_impact.novel_contributions</th>\n",
       "      <th>research_impact.practical_applications</th>\n",
       "      <th>research_impact.future_research_directions</th>\n",
       "      <th>research_impact.scalability_considerations</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arXiv:2401.04122v3</td>\n",
       "      <td>From Prompt Engineering to Prompt Science With...</td>\n",
       "      <td>[Chirag Shah]</td>\n",
       "      <td>2023</td>\n",
       "      <td>Proceedings of ACM Conference (Conference‚Äô17)</td>\n",
       "      <td>https://doi.org/10.1145/nnnnnnn.nnnnnnn</td>\n",
       "      <td>Computational qualitative analysis, Human-cent...</td>\n",
       "      <td>deductive qualitative analysis with human-in-t...</td>\n",
       "      <td>systematic, multi-phase prompt science methodo...</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>...</td>\n",
       "      <td>High inter-coder reliability between human ann...</td>\n",
       "      <td>[Increased cost and resource requirements due ...</td>\n",
       "      <td>[Involvement of at least two qualified researc...</td>\n",
       "      <td>[Establishment and iterative refinement of cle...</td>\n",
       "      <td>[Involvement of multiple researchers to dissol...</td>\n",
       "      <td>[Proposes a multi-phase, human-in-the-loop met...</td>\n",
       "      <td>[Automated and semi-automated labeling of inte...</td>\n",
       "      <td>[Exploring the automation of human-in-the-loop...</td>\n",
       "      <td>The methodology increases process cost by at l...</td>\n",
       "      <td>2401.04122v3.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arXiv:2402.01386v1</td>\n",
       "      <td>Can Large Language Models Serve as Data Analys...</td>\n",
       "      <td>[Zeeshan Rasheed, Muhammad Waseem, Aakash Ahma...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Proceedings of ACM Conference (Conference‚Äô17)</td>\n",
       "      <td>https://doi.org/XXXXXXX.XXXXXXX</td>\n",
       "      <td>Software Engineering, Qualitative Data Analysi...</td>\n",
       "      <td>deductive_qualitative_analysis</td>\n",
       "      <td>multi-agent LLM-based automation of qualitativ...</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>87% practitioner satisfaction rate; positive q...</td>\n",
       "      <td>[Limited sample size for practitioner evaluati...</td>\n",
       "      <td>[Practitioner-based evaluation involving 10 pr...</td>\n",
       "      <td>[Engagement of practitioners from academia and...</td>\n",
       "      <td>[Selection of practitioners from diverse domai...</td>\n",
       "      <td>[Introduction of an LLM-based multi-agent mode...</td>\n",
       "      <td>[Automated and expedited qualitative analysis ...</td>\n",
       "      <td>[Exploration of the model's performance and ad...</td>\n",
       "      <td>The multi-agent LLM-based model significantly ...</td>\n",
       "      <td>2402.01386v1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arXiv:2411.14473v4</td>\n",
       "      <td>Large Language Model for Qualitative Research:...</td>\n",
       "      <td>[Cau√£ Ferreira Barros, Bruna Borges Azevedo, V...</td>\n",
       "      <td>2025</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>https://arxiv.org/abs/2411.14473</td>\n",
       "      <td>Qualitative Research Methodology, Computationa...</td>\n",
       "      <td>deductive_qualitative_analysis</td>\n",
       "      <td>content_analysis; grounded_theory; thematic_an...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>LLMs demonstrated performance equivalent to or...</td>\n",
       "      <td>[Strong dependence on well-structured prompt e...</td>\n",
       "      <td>[Comparison with human analysis, Accuracy, Pre...</td>\n",
       "      <td>[Manual review and full reading of articles to...</td>\n",
       "      <td>[Human verification of LLM outputs and transla...</td>\n",
       "      <td>[Systematic mapping of the state of the art on...</td>\n",
       "      <td>[Automation of open and axial coding in qualit...</td>\n",
       "      <td>[Refinement of prompt engineering techniques t...</td>\n",
       "      <td>LLMs enable scalable qualitative analysis by a...</td>\n",
       "      <td>2411.14473v4.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not_specified</td>\n",
       "      <td>Prompt-based and Fine-tuned GPT Models for Con...</td>\n",
       "      <td>[Chenyu Hou, Gaoxia Zhu, Juan Zheng, Lishan Zh...</td>\n",
       "      <td>2024</td>\n",
       "      <td>The 14th Learning Analytics and Knowledge Conf...</td>\n",
       "      <td>https://doi.org/10.1145/3636555.3636910</td>\n",
       "      <td>Learning Analytics, Educational Technology, Co...</td>\n",
       "      <td>deductive qualitative coding; automated and se...</td>\n",
       "      <td>coding scheme based on cognitive, emotional, a...</td>\n",
       "      <td>7482</td>\n",
       "      <td>...</td>\n",
       "      <td>Cohen's Kappa scores for each coding dimension...</td>\n",
       "      <td>[Limited size of expert-labeled dataset (204 c...</td>\n",
       "      <td>[Cohen‚Äôs Kappa calculated between human raters...</td>\n",
       "      <td>[Expert-labeled data used as ground truth for ...</td>\n",
       "      <td>[Iterative prompt engineering to reduce ambigu...</td>\n",
       "      <td>[First systematic evaluation of GPT-based prom...</td>\n",
       "      <td>[Automated or semi-automated deductive coding ...</td>\n",
       "      <td>[Expansion of expert-labeled datasets and vali...</td>\n",
       "      <td>The pipeline is designed to be scalable to lar...</td>\n",
       "      <td>3636555.3636910.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not_specified</td>\n",
       "      <td>When the Prompt becomes the Codebook: Grounded...</td>\n",
       "      <td>[Sriram Ramanathan, Lisa-Angelique Lim, Nazani...</td>\n",
       "      <td>2025</td>\n",
       "      <td>LAK 2025: The 15th International Learning Anal...</td>\n",
       "      <td>https://doi.org/10.1145/3706468.3706564</td>\n",
       "      <td>Learning Analytics, Qualitative Research Metho...</td>\n",
       "      <td>deductive qualitative analysis using automated...</td>\n",
       "      <td>Four domains of belonging framework (Ahn &amp; Dav...</td>\n",
       "      <td>860</td>\n",
       "      <td>...</td>\n",
       "      <td>Substantial agreement between human and LLM co...</td>\n",
       "      <td>[Human-coded sample comprised only 2% of the f...</td>\n",
       "      <td>[Inter-Rater Reliability (IRR) between human c...</td>\n",
       "      <td>[Theory-driven codebook development grounded i...</td>\n",
       "      <td>[Human-in-the-loop iterative prompt and codebo...</td>\n",
       "      <td>[Introduction of Grounded Prompt Engineering (...</td>\n",
       "      <td>[Automated or semi-automated deductive coding ...</td>\n",
       "      <td>[Testing the GROPROE process and developed pro...</td>\n",
       "      <td>GROPROE enables deductive qualitative analysis...</td>\n",
       "      <td>3706468.3706564.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>not_specified</td>\n",
       "      <td>From nCoder to ChatGPT: From Automated Coding ...</td>\n",
       "      <td>[Andres Felipe Zambrano, Xiner Liu, Amanda Bar...</td>\n",
       "      <td>2024</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>Qualitative Research Methodology, Computationa...</td>\n",
       "      <td>deductive qualitative coding; automated and se...</td>\n",
       "      <td>Quantitative Ethnography; construct validity; ...</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>nCoder achieved higher average Kappa (0.77 tra...</td>\n",
       "      <td>[nCoder's reliance on regular expressions limi...</td>\n",
       "      <td>[Cohen's Kappa, Precision, Recall, Shaffer's r...</td>\n",
       "      <td>[Construct validity assessment via iterative r...</td>\n",
       "      <td>[Inclusion of original dataset author to reduc...</td>\n",
       "      <td>[Systematic comparison of ChatGPT (GPT-4) and ...</td>\n",
       "      <td>[Use of ChatGPT as a semi-automated coding ass...</td>\n",
       "      <td>[Exploration of ChatGPT and other LLMs for sup...</td>\n",
       "      <td>Manual coding is unsuitable for large datasets...</td>\n",
       "      <td>ChatGPT_ICQE_FinalVersion.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>not_specified</td>\n",
       "      <td>ChatGPT for Education Research: Exploring the ...</td>\n",
       "      <td>[Amanda Barany, Nidhi Nasiar, Chelsea Porter, ...</td>\n",
       "      <td>2024</td>\n",
       "      <td>AIED 2024, LNAI 14830, pp. 134‚Äì149, Springer N...</td>\n",
       "      <td>https://doi.org/10.1007/978-3-031-64299-9_10</td>\n",
       "      <td>Educational Research / Qualitative Data Analys...</td>\n",
       "      <td>inductive qualitative coding (codebook develop...</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>Hybrid approaches achieved the highest ratings...</td>\n",
       "      <td>[Study focused on inductive codebook developme...</td>\n",
       "      <td>[Cohen‚Äôs kappa (Œ∫) used to assess consistency ...</td>\n",
       "      <td>[Human review and revision of codebooks genera...</td>\n",
       "      <td>[Independent codebook development by separate ...</td>\n",
       "      <td>[Systematic comparison of four codebook develo...</td>\n",
       "      <td>[Acceleration of qualitative codebook developm...</td>\n",
       "      <td>[Further investigation into optimal division o...</td>\n",
       "      <td>Hybrid human-LLM approaches demonstrated impro...</td>\n",
       "      <td>extracted_pages_134_149.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16094069241231168</td>\n",
       "      <td>An Examination of the Use of Large Language Mo...</td>\n",
       "      <td>[Robert H. Tai, Lillian R. Bentley, Xin Xia, J...</td>\n",
       "      <td>2024</td>\n",
       "      <td>International Journal of Qualitative Methods</td>\n",
       "      <td>https://doi.org/10.1177/16094069241231168</td>\n",
       "      <td>qualitative research methodology, computationa...</td>\n",
       "      <td>deductive qualitative coding using large langu...</td>\n",
       "      <td>codebook-driven deductive analysis based on pr...</td>\n",
       "      <td>125</td>\n",
       "      <td>...</td>\n",
       "      <td>LLMq values plateaued and stabilized after app...</td>\n",
       "      <td>[LLMs rely on patterns in their training data;...</td>\n",
       "      <td>[Inter-rater reliability using Cohen‚Äôs kappa s...</td>\n",
       "      <td>[Use of a detailed codebook with clear definit...</td>\n",
       "      <td>[Multiple coders independently coding and reso...</td>\n",
       "      <td>[Proposes a systematic methodology for using L...</td>\n",
       "      <td>[LLMs can be used as an efficient screening to...</td>\n",
       "      <td>[Exploration of LLMs for inductive qualitative...</td>\n",
       "      <td>LLMs offer high scalability for qualitative an...</td>\n",
       "      <td>tai-et-al-2024-an-examination-of-the-use-of-la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows √ó 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  bibliographic_metadata.paper_id  \\\n",
       "0              arXiv:2401.04122v3   \n",
       "1              arXiv:2402.01386v1   \n",
       "2              arXiv:2411.14473v4   \n",
       "3                   not_specified   \n",
       "4                   not_specified   \n",
       "5                   not_specified   \n",
       "6                   not_specified   \n",
       "7               16094069241231168   \n",
       "\n",
       "                        bibliographic_metadata.title  \\\n",
       "0  From Prompt Engineering to Prompt Science With...   \n",
       "1  Can Large Language Models Serve as Data Analys...   \n",
       "2  Large Language Model for Qualitative Research:...   \n",
       "3  Prompt-based and Fine-tuned GPT Models for Con...   \n",
       "4  When the Prompt becomes the Codebook: Grounded...   \n",
       "5  From nCoder to ChatGPT: From Automated Coding ...   \n",
       "6  ChatGPT for Education Research: Exploring the ...   \n",
       "7  An Examination of the Use of Large Language Mo...   \n",
       "\n",
       "                      bibliographic_metadata.authors  \\\n",
       "0                                      [Chirag Shah]   \n",
       "1  [Zeeshan Rasheed, Muhammad Waseem, Aakash Ahma...   \n",
       "2  [Cau√£ Ferreira Barros, Bruna Borges Azevedo, V...   \n",
       "3  [Chenyu Hou, Gaoxia Zhu, Juan Zheng, Lishan Zh...   \n",
       "4  [Sriram Ramanathan, Lisa-Angelique Lim, Nazani...   \n",
       "5  [Andres Felipe Zambrano, Xiner Liu, Amanda Bar...   \n",
       "6  [Amanda Barany, Nidhi Nasiar, Chelsea Porter, ...   \n",
       "7  [Robert H. Tai, Lillian R. Bentley, Xin Xia, J...   \n",
       "\n",
       "   bibliographic_metadata.publication_year  \\\n",
       "0                                     2023   \n",
       "1                                     2018   \n",
       "2                                     2025   \n",
       "3                                     2024   \n",
       "4                                     2025   \n",
       "5                                     2024   \n",
       "6                                     2024   \n",
       "7                                     2024   \n",
       "\n",
       "                        bibliographic_metadata.venue  \\\n",
       "0      Proceedings of ACM Conference (Conference‚Äô17)   \n",
       "1      Proceedings of ACM Conference (Conference‚Äô17)   \n",
       "2                                              arXiv   \n",
       "3  The 14th Learning Analytics and Knowledge Conf...   \n",
       "4  LAK 2025: The 15th International Learning Anal...   \n",
       "5                                      not_specified   \n",
       "6  AIED 2024, LNAI 14830, pp. 134‚Äì149, Springer N...   \n",
       "7       International Journal of Qualitative Methods   \n",
       "\n",
       "                 bibliographic_metadata.doi_url  \\\n",
       "0       https://doi.org/10.1145/nnnnnnn.nnnnnnn   \n",
       "1               https://doi.org/XXXXXXX.XXXXXXX   \n",
       "2              https://arxiv.org/abs/2411.14473   \n",
       "3       https://doi.org/10.1145/3636555.3636910   \n",
       "4       https://doi.org/10.1145/3706468.3706564   \n",
       "5                                 not_specified   \n",
       "6  https://doi.org/10.1007/978-3-031-64299-9_10   \n",
       "7     https://doi.org/10.1177/16094069241231168   \n",
       "\n",
       "              bibliographic_metadata.research_domain  \\\n",
       "0  Computational qualitative analysis, Human-cent...   \n",
       "1  Software Engineering, Qualitative Data Analysi...   \n",
       "2  Qualitative Research Methodology, Computationa...   \n",
       "3  Learning Analytics, Educational Technology, Co...   \n",
       "4  Learning Analytics, Qualitative Research Metho...   \n",
       "5  Qualitative Research Methodology, Computationa...   \n",
       "6  Educational Research / Qualitative Data Analys...   \n",
       "7  qualitative research methodology, computationa...   \n",
       "\n",
       "              methodological_framework.analysis_type  \\\n",
       "0  deductive qualitative analysis with human-in-t...   \n",
       "1                     deductive_qualitative_analysis   \n",
       "2                     deductive_qualitative_analysis   \n",
       "3  deductive qualitative coding; automated and se...   \n",
       "4  deductive qualitative analysis using automated...   \n",
       "5  deductive qualitative coding; automated and se...   \n",
       "6  inductive qualitative coding (codebook develop...   \n",
       "7  deductive qualitative coding using large langu...   \n",
       "\n",
       "      methodological_framework.theoretical_framework  \\\n",
       "0  systematic, multi-phase prompt science methodo...   \n",
       "1  multi-agent LLM-based automation of qualitativ...   \n",
       "2  content_analysis; grounded_theory; thematic_an...   \n",
       "3  coding scheme based on cognitive, emotional, a...   \n",
       "4  Four domains of belonging framework (Ahn & Dav...   \n",
       "5  Quantitative Ethnography; construct validity; ...   \n",
       "6                                      not_specified   \n",
       "7  codebook-driven deductive analysis based on pr...   \n",
       "\n",
       "  methodological_framework.sample_characteristics.interview_count  ...  \\\n",
       "0                                      not_specified               ...   \n",
       "1                                                 10               ...   \n",
       "2                                                  3               ...   \n",
       "3                                               7482               ...   \n",
       "4                                                860               ...   \n",
       "5                                                200               ...   \n",
       "6                                                  4               ...   \n",
       "7                                                125               ...   \n",
       "\n",
       "  empirical_results.evaluation_framework.performance_indicators  \\\n",
       "0  High inter-coder reliability between human ann...              \n",
       "1  87% practitioner satisfaction rate; positive q...              \n",
       "2  LLMs demonstrated performance equivalent to or...              \n",
       "3  Cohen's Kappa scores for each coding dimension...              \n",
       "4  Substantial agreement between human and LLM co...              \n",
       "5  nCoder achieved higher average Kappa (0.77 tra...              \n",
       "6  Hybrid approaches achieved the highest ratings...              \n",
       "7  LLMq values plateaued and stabilized after app...              \n",
       "\n",
       "        empirical_results.methodological_limitations  \\\n",
       "0  [Increased cost and resource requirements due ...   \n",
       "1  [Limited sample size for practitioner evaluati...   \n",
       "2  [Strong dependence on well-structured prompt e...   \n",
       "3  [Limited size of expert-labeled dataset (204 c...   \n",
       "4  [Human-coded sample comprised only 2% of the f...   \n",
       "5  [nCoder's reliance on regular expressions limi...   \n",
       "6  [Study focused on inductive codebook developme...   \n",
       "7  [LLMs rely on patterns in their training data;...   \n",
       "\n",
       "              quality_assurance.reliability_measures  \\\n",
       "0  [Involvement of at least two qualified researc...   \n",
       "1  [Practitioner-based evaluation involving 10 pr...   \n",
       "2  [Comparison with human analysis, Accuracy, Pre...   \n",
       "3  [Cohen‚Äôs Kappa calculated between human raters...   \n",
       "4  [Inter-Rater Reliability (IRR) between human c...   \n",
       "5  [Cohen's Kappa, Precision, Recall, Shaffer's r...   \n",
       "6  [Cohen‚Äôs kappa (Œ∫) used to assess consistency ...   \n",
       "7  [Inter-rater reliability using Cohen‚Äôs kappa s...   \n",
       "\n",
       "               quality_assurance.validity_approaches  \\\n",
       "0  [Establishment and iterative refinement of cle...   \n",
       "1  [Engagement of practitioners from academia and...   \n",
       "2  [Manual review and full reading of articles to...   \n",
       "3  [Expert-labeled data used as ground truth for ...   \n",
       "4  [Theory-driven codebook development grounded i...   \n",
       "5  [Construct validity assessment via iterative r...   \n",
       "6  [Human review and revision of codebooks genera...   \n",
       "7  [Use of a detailed codebook with clear definit...   \n",
       "\n",
       "        quality_assurance.bias_mitigation_strategies  \\\n",
       "0  [Involvement of multiple researchers to dissol...   \n",
       "1  [Selection of practitioners from diverse domai...   \n",
       "2  [Human verification of LLM outputs and transla...   \n",
       "3  [Iterative prompt engineering to reduce ambigu...   \n",
       "4  [Human-in-the-loop iterative prompt and codebo...   \n",
       "5  [Inclusion of original dataset author to reduc...   \n",
       "6  [Independent codebook development by separate ...   \n",
       "7  [Multiple coders independently coding and reso...   \n",
       "\n",
       "                 research_impact.novel_contributions  \\\n",
       "0  [Proposes a multi-phase, human-in-the-loop met...   \n",
       "1  [Introduction of an LLM-based multi-agent mode...   \n",
       "2  [Systematic mapping of the state of the art on...   \n",
       "3  [First systematic evaluation of GPT-based prom...   \n",
       "4  [Introduction of Grounded Prompt Engineering (...   \n",
       "5  [Systematic comparison of ChatGPT (GPT-4) and ...   \n",
       "6  [Systematic comparison of four codebook develo...   \n",
       "7  [Proposes a systematic methodology for using L...   \n",
       "\n",
       "              research_impact.practical_applications  \\\n",
       "0  [Automated and semi-automated labeling of inte...   \n",
       "1  [Automated and expedited qualitative analysis ...   \n",
       "2  [Automation of open and axial coding in qualit...   \n",
       "3  [Automated or semi-automated deductive coding ...   \n",
       "4  [Automated or semi-automated deductive coding ...   \n",
       "5  [Use of ChatGPT as a semi-automated coding ass...   \n",
       "6  [Acceleration of qualitative codebook developm...   \n",
       "7  [LLMs can be used as an efficient screening to...   \n",
       "\n",
       "          research_impact.future_research_directions  \\\n",
       "0  [Exploring the automation of human-in-the-loop...   \n",
       "1  [Exploration of the model's performance and ad...   \n",
       "2  [Refinement of prompt engineering techniques t...   \n",
       "3  [Expansion of expert-labeled datasets and vali...   \n",
       "4  [Testing the GROPROE process and developed pro...   \n",
       "5  [Exploration of ChatGPT and other LLMs for sup...   \n",
       "6  [Further investigation into optimal division o...   \n",
       "7  [Exploration of LLMs for inductive qualitative...   \n",
       "\n",
       "          research_impact.scalability_considerations  \\\n",
       "0  The methodology increases process cost by at l...   \n",
       "1  The multi-agent LLM-based model significantly ...   \n",
       "2  LLMs enable scalable qualitative analysis by a...   \n",
       "3  The pipeline is designed to be scalable to lar...   \n",
       "4  GROPROE enables deductive qualitative analysis...   \n",
       "5  Manual coding is unsuitable for large datasets...   \n",
       "6  Hybrid human-LLM approaches demonstrated impro...   \n",
       "7  LLMs offer high scalability for qualitative an...   \n",
       "\n",
       "                                         source_file  \n",
       "0                                  2401.04122v3.json  \n",
       "1                                  2402.01386v1.json  \n",
       "2                                  2411.14473v4.json  \n",
       "3                               3636555.3636910.json  \n",
       "4                               3706468.3706564.json  \n",
       "5                     ChatGPT_ICQE_FinalVersion.json  \n",
       "6                       extracted_pages_134_149.json  \n",
       "7  tai-et-al-2024-an-examination-of-the-use-of-la...  \n",
       "\n",
       "[8 rows x 36 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quali_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
