{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic Models\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "\n",
    "class BibliographicMetadata(BaseModel):\n",
    "    paper_id: str\n",
    "    title: str\n",
    "    authors: List[str]\n",
    "    publication_year: int\n",
    "    venue: str\n",
    "    doi_url: str\n",
    "    research_domain: str\n",
    "\n",
    "\n",
    "class SampleCharacteristics(BaseModel):\n",
    "    interview_count: Optional[Union[int, str]] = None\n",
    "    participant_demographics: str\n",
    "    data_collection_method: str\n",
    "\n",
    "\n",
    "class MethodologicalFramework(BaseModel):\n",
    "    analysis_type: str\n",
    "    theoretical_framework: str\n",
    "    sample_characteristics: SampleCharacteristics\n",
    "\n",
    "\n",
    "class WorkflowArchitecture(BaseModel):\n",
    "    preprocessing_steps: List[str]\n",
    "    analysis_pipeline: List[str]\n",
    "    postprocessing_steps: List[str]\n",
    "\n",
    "\n",
    "class TechnicalPipeline(BaseModel):\n",
    "    automation_level: str\n",
    "    software_tools: List[str]\n",
    "    computational_methods: List[str]\n",
    "    workflow_architecture: WorkflowArchitecture\n",
    "    coding_framework: str\n",
    "\n",
    "\n",
    "class PromptingStrategy(BaseModel):\n",
    "    approach_type: str\n",
    "    prompt_examples: List[str]\n",
    "    engineering_techniques: List[str]\n",
    "    model_specifications: str\n",
    "\n",
    "\n",
    "class PromptEngineering(BaseModel):\n",
    "    prompting_strategy: PromptingStrategy\n",
    "\n",
    "\n",
    "class EvaluationFramework(BaseModel):\n",
    "    metrics_employed: List[str]\n",
    "    validation_methodology: str\n",
    "    performance_indicators: str\n",
    "\n",
    "\n",
    "class EmpiricalResults(BaseModel):\n",
    "    primary_findings: List[str]\n",
    "    evaluation_framework: EvaluationFramework\n",
    "    methodological_limitations: List[str]\n",
    "\n",
    "\n",
    "class QualityAssurance(BaseModel):\n",
    "    reliability_measures: List[str]\n",
    "    validity_approaches: List[str]\n",
    "    bias_mitigation_strategies: List[str]\n",
    "\n",
    "\n",
    "class ResearchImpact(BaseModel):\n",
    "    novel_contributions: List[str]\n",
    "    practical_applications: List[str]\n",
    "    future_research_directions: List[str]\n",
    "    scalability_considerations: str\n",
    "\n",
    "\n",
    "class ResearchStudy(BaseModel):\n",
    "    bibliographic_metadata: BibliographicMetadata\n",
    "    methodological_framework: MethodologicalFramework\n",
    "    technical_pipeline: TechnicalPipeline\n",
    "    prompt_engineering: PromptEngineering\n",
    "    empirical_results: EmpiricalResults\n",
    "    quality_assurance: QualityAssurance\n",
    "    research_impact: ResearchImpact\n",
    "\n",
    "class ResearchState(BaseModel):\n",
    "    bibliographic_metadata: Optional[BibliographicMetadata] = None\n",
    "    methodological_framework: Optional[MethodologicalFramework] = None\n",
    "    technical_pipeline: Optional[TechnicalPipeline] = None\n",
    "    prompt_engineering: Optional[PromptEngineering] = None\n",
    "    empirical_results: Optional[EmpiricalResults] = None\n",
    "    quality_assurance: Optional[QualityAssurance] = None\n",
    "    research_impact: Optional[ResearchImpact] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction Schema\n",
    "\n",
    "section_schemas = {\n",
    "    \"bibliographic_metadata\": {\n",
    "        \"bibliographic_metadata\": {\n",
    "            \"paper_id\": \"string\",\n",
    "            \"title\": \"string\",\n",
    "            \"authors\": [\"string\"],\n",
    "            \"publication_year\": \"integer\",\n",
    "            \"venue\": \"string\",\n",
    "            \"doi_url\": \"string\",\n",
    "            \"research_domain\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"methodological_framework\": {\n",
    "        \"methodological_framework\": {\n",
    "            \"analysis_type\": \"string\",\n",
    "            \"theoretical_framework\": \"string\",\n",
    "            \"sample_characteristics\": {\n",
    "                \"interview_count\": \"integer\",\n",
    "                \"participant_demographics\": \"string\",\n",
    "                \"data_collection_method\": \"string\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"technical_pipeline\": {\n",
    "        \"technical_pipeline\": {\n",
    "            \"automation_level\": \"string\",\n",
    "            \"software_tools\": [\"string\"],\n",
    "            \"computational_methods\": [\"string\"],\n",
    "            \"workflow_architecture\": {\n",
    "                \"preprocessing_steps\": [\"string\"],\n",
    "                \"analysis_pipeline\": [\"string\"],\n",
    "                \"postprocessing_steps\": [\"string\"]\n",
    "            },\n",
    "            \"coding_framework\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"prompt_engineering\": {\n",
    "        \"prompt_engineering\": {\n",
    "            \"prompting_strategy\": {\n",
    "                \"approach_type\": \"string\",\n",
    "                \"prompt_examples\": [\"string\"],\n",
    "                \"engineering_techniques\": [\"string\"],\n",
    "                \"model_specifications\": \"string\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"empirical_results\": {\n",
    "        \"empirical_results\": {\n",
    "            \"primary_findings\": [\"string\"],\n",
    "            \"evaluation_framework\": {\n",
    "                \"metrics_employed\": [\"string\"],\n",
    "                \"validation_methodology\": \"string\",\n",
    "                \"performance_indicators\": \"string\"\n",
    "            },\n",
    "            \"methodological_limitations\": [\"string\"]\n",
    "        }\n",
    "    },\n",
    "    \"quality_assurance\": {\n",
    "        \"quality_assurance\": {\n",
    "            \"reliability_measures\": [\"string\"],\n",
    "            \"validity_approaches\": [\"string\"],\n",
    "            \"bias_mitigation_strategies\": [\"string\"]\n",
    "        }\n",
    "    },\n",
    "    \"research_impact\": {\n",
    "        \"research_impact\": {\n",
    "            \"novel_contributions\": [\"string\"],\n",
    "            \"practical_applications\": [\"string\"],\n",
    "            \"future_research_directions\": [\"string\"],\n",
    "            \"scalability_considerations\": \"string\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_base = \"\"\"\n",
    "You are an expert research assistant specializing in qualitative research methodology and computational text analysis. Your task is to perform systematic data extraction from academic literature focusing on deductive qualitative analysis pipelines for interview data.\n",
    "\n",
    "# Task Definition\n",
    "Analyze the provided research document and extract structured information related to deductive qualitative analysis methodologies, computational workflows, and evaluation frameworks. Focus on identifying technical specifications, methodological approaches, and empirical findings relevant to automated or semi-automated qualitative data analysis pipelines.\n",
    "\n",
    "# Extraction Schema\n",
    "\n",
    "Extract the following information and return it as a structured JSON object with the specified schema:\n",
    "{schema}\n",
    "\n",
    "Extraction Guidelines\n",
    "- Completeness: Extract all available information; use \"not_specified\" for missing data\n",
    "- Precision: Maintain technical terminology and methodological specificity\n",
    "- Contextual Accuracy: Preserve the original meaning and technical context\n",
    "- Standardization: Use consistent terminology across extractions\n",
    "- Null Handling: Use empty arrays [] for missing list items, null for missing values\n",
    "\n",
    "# IMPORTANT: Return ONLY valid JSON without any additional text, explanations, or markdown formatting.\n",
    "\n",
    "# Texto a analizar\n",
    "{document_text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0.002)\n",
    "\n",
    "# 1. Define prompts for each section\n",
    "prompts = {}\n",
    "for section, schema in section_schemas.items():\n",
    "    prompts[section] = PromptTemplate(\n",
    "        input_variables=[\"document_text\", \"schema\"],\n",
    "        template=template_base\n",
    "    )\n",
    "\n",
    "# 2. Create LLMChain for each section\n",
    "chains = {}\n",
    "for section, prompt_template in prompts.items():\n",
    "    chains[section] = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt_template.partial(schema=json.dumps(section_schemas[section], indent=2)),\n",
    "        output_key=section\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def run_extraction_pipeline(chains: Dict[str, LLMChain], output_dir=\"outputs\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for file in Path(\"../files\").glob(\"*.pdf\"):\n",
    "        print(f\"\\n�� Procesando archivo: {file.name}\")\n",
    "        document_text = extract_text_from_pdf(str(file))\n",
    "\n",
    "        state_data = {}\n",
    "\n",
    "        for section, chain in chains.items():\n",
    "            try:\n",
    "                print(f\"  �� Extrayendo sección: {section}\")\n",
    "                response_dict = chain.invoke(input=document_text)\n",
    "                \n",
    "                # Extract the actual response text from the dictionary\n",
    "                response = response_dict[section]\n",
    "                \n",
    "                # Debug: Print the actual response to see what we're getting\n",
    "                print(f\"    Raw response: {response[:200]}...\")  # First 200 chars\n",
    "                \n",
    "                # Clean the response - remove any non-JSON text\n",
    "                response = response.strip()\n",
    "                \n",
    "                # Try to extract JSON from the response if it contains extra text\n",
    "                if response.startswith('```json'):\n",
    "                    response = response.split('```json')[1].split('```')[0].strip()\n",
    "                elif response.startswith('```'):\n",
    "                    response = response.split('```')[1].split('```')[0].strip()\n",
    "                \n",
    "                # Validate that we have content before parsing\n",
    "                if not response:\n",
    "                    print(f\"    ⚠️ Empty response for {section}\")\n",
    "                    state_data[section] = None\n",
    "                    continue\n",
    "                \n",
    "                parsed = json.loads(response)\n",
    "                if section in parsed:\n",
    "                    state_data[section] = parsed[section]\n",
    "                else:\n",
    "                    print(f\"  ⚠️ Clave esperada '{section}' no encontrada en la respuesta.\")\n",
    "                    state_data[section] = None\n",
    "\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"  ⚠️ JSON parsing error for '{section}': {e}\")\n",
    "                print(f\"    Response content: {response}\")\n",
    "                state_data[section] = None\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Error al procesar '{section}': {e}\")\n",
    "                state_data[section] = None\n",
    "\n",
    "        # Construye el objeto ResearchState\n",
    "        try:\n",
    "            research_state = ResearchState.model_validate(state_data)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error al crear ResearchState: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Guarda JSON\n",
    "        json_path = Path(output_dir) / f\"{file.stem}.json\"\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(research_state.model_dump_json(indent=2))\n",
    "\n",
    "        # Guarda CSV (aplanado)\n",
    "        df = pd.json_normalize(research_state.model_dump())\n",
    "        csv_path = Path(output_dir) / f\"{file.stem}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "        print(f\"✅ Archivo procesado: {file.name} → {json_path.name}, {csv_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "�� Procesando archivo: 2401.04122v3.pdf\n",
      "  �� Extrayendo sección: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"arXiv:2401.04122v3\",\n",
      "    \"title\": \"From Prompt Engineering to Prompt Science With Human in the Loop\",\n",
      "    \"authors\": [\n",
      "      \"Chirag Shah\"\n",
      "    ],\n",
      "    \"...\n",
      "  �� Extrayendo sección: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive qualitative analysis with human-in-the-loop validation, inspired by qualitative codebook construction and coding\",\n",
      "    \"theoretical_fr...\n",
      "  �� Extrayendo sección: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"semi-automated (human-in-the-loop with LLMs for labeling, coding, and prompt generation; automation increases with validated prompts and codebooks,...\n",
      "  �� Extrayendo sección: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"multi-phase, human-in-the-loop, codebook-inspired deductive prompt construction and validation\",\n",
      "      \"prompt_examples\"...\n",
      "  �� Extrayendo sección: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"A multi-phase, human-in-the-loop methodology inspired by qualitative codebook construction increases the reliability, transparency, and gene...\n",
      "  �� Extrayendo sección: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Involvement of at least two qualified researchers in the entire process\",\n",
      "      \"Independent application of criteria by multiple assesso...\n",
      "  �� Extrayendo sección: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Proposes a multi-phase, human-in-the-loop methodology for transforming ad-hoc prompt engineering into a systematic, verifiable, and replica...\n",
      "✅ Archivo procesado: 2401.04122v3.pdf → 2401.04122v3.json, 2401.04122v3.csv\n",
      "\n",
      "�� Procesando archivo: 2402.01386v1.pdf\n",
      "  �� Extrayendo sección: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"arXiv:2402.01386v1\",\n",
      "    \"title\": \"Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis\",\n",
      "  ...\n",
      "  �� Extrayendo sección: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive_qualitative_analysis\",\n",
      "    \"theoretical_framework\": \"multi-agent LLM-based automation of qualitative data analysis (encompassing thema...\n",
      "  �� Extrayendo sección: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"fully_automated (autonomous execution of qualitative data analysis tasks by LLM-based multi-agent system with minimal manual intervention)\",\n",
      "    \"s...\n",
      "  �� Extrayendo sección: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"multi-agent, task-specialized prompting with user-specified qualitative analysis method\",\n",
      "      \"prompt_examples\": [\n",
      "   ...\n",
      "  �� Extrayendo sección: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"The LLM-based multi-agent model can autonomously perform various qualitative data analysis approaches, including thematic analysis, grounded...\n",
      "  �� Extrayendo sección: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Practitioner-based evaluation involving 10 practitioners from diverse professional backgrounds to assess model performance\",\n",
      "      \"Use ...\n",
      "  �� Extrayendo sección: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Introduction of an LLM-based multi-agent model that automates the entire qualitative data analysis pipeline, including thematic analysis, g...\n",
      "✅ Archivo procesado: 2402.01386v1.pdf → 2402.01386v1.json, 2402.01386v1.csv\n",
      "\n",
      "�� Procesando archivo: 2411.14473v4.pdf\n",
      "  �� Extrayendo sección: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"arXiv:2411.14473v4\",\n",
      "    \"title\": \"Large Language Model for Qualitative Research: A Systematic Mapping Study\",\n",
      "    \"authors\": [\n",
      "      \"Cauã Ferreira Ba...\n",
      "  �� Extrayendo sección: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive_qualitative_analysis\",\n",
      "    \"theoretical_framework\": \"content_analysis; grounded_theory; thematic_analysis\",\n",
      "    \"sample_characteristic...\n",
      "  �� Extrayendo sección: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"semi-automated to fully automated (depending on study and workflow; LLMs automate coding, theme extraction, and categorization, but human oversight...\n",
      "  �� Extrayendo sección: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"structured prompt engineering with iterative refinement\",\n",
      "      \"prompt_examples\": [\n",
      "        \"Prompts designed to extrac...\n",
      "  �� Extrayendo sección: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"LLMs, particularly ChatGPT and its variants, are increasingly used to automate and enhance qualitative analysis across domains such as healt...\n",
      "  �� Extrayendo sección: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Comparison with human analysis\",\n",
      "      \"Accuracy\",\n",
      "      \"Precision\",\n",
      "      \"Recall\",\n",
      "      \"F1 Score\",\n",
      "      \"Cohen’s Kappa\",\n",
      "      \"Ag...\n",
      "  �� Extrayendo sección: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Systematic mapping of the state of the art on the use of Large Language Models (LLMs) in deductive and inductive qualitative analysis, with...\n",
      "✅ Archivo procesado: 2411.14473v4.pdf → 2411.14473v4.json, 2411.14473v4.csv\n",
      "\n",
      "�� Procesando archivo: 3636555.3636910.pdf\n",
      "  �� Extrayendo sección: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"not_specified\",\n",
      "    \"title\": \"Prompt-based and Fine-tuned GPT Models for Context-Dependent and -Independent Deductive Coding in Social Annotation\",\n",
      "   ...\n",
      "  �� Extrayendo sección: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive qualitative coding; automated and semi-automated content analysis using prompt-based and fine-tuned GPT models\",\n",
      "    \"theoretical_fram...\n",
      "  �� Extrayendo sección: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"semi-automated (prompt-based and fine-tuned GPT models for deductive coding; human-in-the-loop for codebook development and evaluation)\",\n",
      "    \"soft...\n",
      "  �� Extrayendo sección: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"iterative manual prompt engineering with codebook-centered, chain-of-thought, step-based, and multi-prompt techniques\",\n",
      "...\n",
      "  �� Extrayendo sección: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"Prompt engineering with GPT-3.5-turbo achieves fair to substantial agreement with expert-labeled data for context-independent deductive codi...\n",
      "  �� Extrayendo sección: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Cohen’s Kappa calculated between human raters for each coding dimension (e.g., .63 for Appraisal, .95 for Questioning, .80 for Theorizin...\n",
      "  �� Extrayendo sección: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"First systematic evaluation of GPT-based prompt engineering and fine-tuning for both context-dependent and context-independent deductive co...\n",
      "✅ Archivo procesado: 3636555.3636910.pdf → 3636555.3636910.json, 3636555.3636910.csv\n",
      "\n",
      "�� Procesando archivo: 3706468.3706564.pdf\n",
      "  �� Extrayendo sección: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"not_specified\",\n",
      "    \"title\": \"When the Prompt becomes the Codebook: Grounded Prompt Engineering (GROPROE) and its application to Belonging Analytics\",\n",
      "...\n",
      "  �� Extrayendo sección: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive qualitative analysis using automated and semi-automated computational workflows\",\n",
      "    \"theoretical_framework\": \"Four domains of belong...\n",
      "  �� Extrayendo sección: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"semi-automated\",\n",
      "    \"software_tools\": [\n",
      "      \"Azure Playground\",\n",
      "      \"GPT-4\"\n",
      "    ],\n",
      "    \"computational_methods\": [\n",
      "      \"Large Language Model-...\n",
      "  �� Extrayendo sección: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"Grounded Prompt Engineering (GROPROE); theory-driven, codebook-based, iterative refinement\",\n",
      "      \"prompt_examples\": [\n",
      "...\n",
      "  �� Extrayendo sección: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"The GROPROE (Grounded Prompt Engineering) process enables systematic, theory-grounded prompt engineering for deductive qualitative analysis ...\n",
      "  �� Extrayendo sección: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Inter-Rater Reliability (IRR) between human coders calculated using Cohen’s Kappa\",\n",
      "      \"Inter-Annotator Reliability (IAR) between hum...\n",
      "  �� Extrayendo sección: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Introduction of Grounded Prompt Engineering (GROPROE) as a systematic, literature-grounded process for developing prompts for deductive qua...\n",
      "✅ Archivo procesado: 3706468.3706564.pdf → 3706468.3706564.json, 3706468.3706564.csv\n",
      "\n",
      "�� Procesando archivo: ChatGPT_ICQE_FinalVersion.pdf\n",
      "  �� Extrayendo sección: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"not_specified\",\n",
      "    \"title\": \"From nCoder to ChatGPT: From Automated Coding to Refining Human Coding\",\n",
      "    \"authors\": [\n",
      "      \"Andres Felipe Zambrano\",...\n",
      "  �� Extrayendo sección: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive qualitative coding; automated and semi-automated coding comparison\",\n",
      "    \"theoretical_framework\": \"Quantitative Ethnography; construct...\n",
      "  �� Extrayendo sección: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"semi-automated\",\n",
      "    \"software_tools\": [\n",
      "      \"nCoder\",\n",
      "      \"ChatGPT (GPT-4)\"\n",
      "    ],\n",
      "    \"computational_methods\": [\n",
      "      \"regular expressions\",...\n",
      "  �� Extrayendo sección: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"iterative refinement with human-in-the-loop\",\n",
      "      \"prompt_examples\": [\n",
      "        \"For each construct, provide ChatGPT wi...\n",
      "  �� Extrayendo sección: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"Both ChatGPT (GPT-4) and nCoder have distinct advantages and disadvantages for automated coding of qualitative interview data, depending on ...\n",
      "  �� Extrayendo sección: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Cohen's Kappa\",\n",
      "      \"Precision\",\n",
      "      \"Recall\",\n",
      "      \"Shaffer's rho\",\n",
      "      \"Inter-rater agreement\"\n",
      "    ],\n",
      "    \"validity_approaches\"...\n",
      "  �� Extrayendo sección: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Systematic comparison of ChatGPT (GPT-4) and nCoder for automated coding in Quantitative Ethnography (QE) using a real-world dataset of gov...\n",
      "✅ Archivo procesado: ChatGPT_ICQE_FinalVersion.pdf → ChatGPT_ICQE_FinalVersion.json, ChatGPT_ICQE_FinalVersion.csv\n",
      "\n",
      "�� Procesando archivo: extracted_pages_134_149.pdf\n",
      "  �� Extrayendo sección: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"not_specified\",\n",
      "    \"title\": \"ChatGPT for Education Research: Exploring the Potential of Large Language Models for Qualitative Codebook Development\",\n",
      " ...\n",
      "  �� Extrayendo sección: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"inductive qualitative coding (codebook development and evaluation), with comparative analysis of manual, automated, and hybrid (human-LLM) workf...\n",
      "  �� Extrayendo sección: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"hybrid (manual, semi-automated, and fully automated approaches compared; focus on hybrid human-LLM collaboration)\",\n",
      "    \"software_tools\": [\n",
      "      \"...\n",
      "  �� Extrayendo sección: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"hybrid and automated prompting for codebook development and refinement in qualitative analysis\",\n",
      "      \"prompt_examples\"...\n",
      "  �� Extrayendo sección: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"Hybrid approaches (combining human and ChatGPT involvement in codebook development or refinement) produced codebooks that were more reliably...\n",
      "  �� Extrayendo sección: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Cohen’s kappa (κ) used to assess consistency of code applications between coders\",\n",
      "      \"Percent agreement calculated across two rounds...\n",
      "  �� Extrayendo sección: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Systematic comparison of four codebook development pipelines for qualitative interview data: fully manual, fully automated (ChatGPT-only), ...\n",
      "✅ Archivo procesado: extracted_pages_134_149.pdf → extracted_pages_134_149.json, extracted_pages_134_149.csv\n",
      "\n",
      "�� Procesando archivo: tai-et-al-2024-an-examination-of-the-use-of-large-language-models-to-aid-analysis-of-textual-data.pdf\n",
      "  �� Extrayendo sección: bibliographic_metadata\n",
      "    Raw response: {\n",
      "  \"bibliographic_metadata\": {\n",
      "    \"paper_id\": \"16094069241231168\",\n",
      "    \"title\": \"An Examination of the Use of Large Language Models to Aid Analysis of Textual Data\",\n",
      "    \"authors\": [\n",
      "      \"Robert H...\n",
      "  �� Extrayendo sección: methodological_framework\n",
      "    Raw response: {\n",
      "  \"methodological_framework\": {\n",
      "    \"analysis_type\": \"deductive qualitative coding using large language models (LLMs) as analytic instruments, with comparison to traditional human coding\",\n",
      "    \"theo...\n",
      "  �� Extrayendo sección: technical_pipeline\n",
      "    Raw response: {\n",
      "  \"technical_pipeline\": {\n",
      "    \"automation_level\": \"semi-automated (human-in-the-loop, LLM-assisted deductive coding)\",\n",
      "    \"software_tools\": [\n",
      "      \"ChatGPT 3.5 (OpenAI)\",\n",
      "      \"Dedoose (qualitati...\n",
      "  �� Extrayendo sección: prompt_engineering\n",
      "    Raw response: {\n",
      "  \"prompt_engineering\": {\n",
      "    \"prompting_strategy\": {\n",
      "      \"approach_type\": \"codebook-driven deductive binary prompting\",\n",
      "      \"prompt_examples\": [\n",
      "        \"Can you find the five characteristics i...\n",
      "  �� Extrayendo sección: empirical_results\n",
      "    Raw response: {\n",
      "  \"empirical_results\": {\n",
      "    \"primary_findings\": [\n",
      "      \"Large Language Models (LLMs), specifically ChatGPT 3.5, can be systematically used to support deductive qualitative coding of interview data...\n",
      "  �� Extrayendo sección: quality_assurance\n",
      "    Raw response: {\n",
      "  \"quality_assurance\": {\n",
      "    \"reliability_measures\": [\n",
      "      \"Inter-rater reliability using Cohen’s kappa statistic\",\n",
      "      \"Inter-rater reliability using Fleiss’ kappa statistic\",\n",
      "      \"Calculatio...\n",
      "  �� Extrayendo sección: research_impact\n",
      "    Raw response: {\n",
      "  \"research_impact\": {\n",
      "    \"novel_contributions\": [\n",
      "      \"Proposes a systematic methodology for using Large Language Models (LLMs) to support traditional deductive coding in qualitative research.\",...\n",
      "✅ Archivo procesado: tai-et-al-2024-an-examination-of-the-use-of-large-language-models-to-aid-analysis-of-textual-data.pdf → tai-et-al-2024-an-examination-of-the-use-of-large-language-models-to-aid-analysis-of-textual-data.json, tai-et-al-2024-an-examination-of-the-use-of-large-language-models-to-aid-analysis-of-textual-data.csv\n"
     ]
    }
   ],
   "source": [
    "run_extraction_pipeline(chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded: 2401.04122v3.json\n",
      "✅ Loaded: 2402.01386v1.json\n",
      "✅ Loaded: 2411.14473v4.json\n",
      "✅ Loaded: 3636555.3636910.json\n",
      "✅ Loaded: 3706468.3706564.json\n",
      "✅ Loaded: ChatGPT_ICQE_FinalVersion.json\n",
      "✅ Loaded: extracted_pages_134_149.json\n",
      "✅ Loaded: tai-et-al-2024-an-examination-of-the-use-of-large-language-models-to-aid-analysis-of-textual-data.json\n",
      "\n",
      "📊 Combined DataFrame shape: (8, 36)\n",
      "📁 Total files processed: 8\n"
     ]
    }
   ],
   "source": [
    "def load_all_json_to_dataframe(outputs_path=\"../research_assistant/outputs\"):\n",
    "    \"\"\"\n",
    "    Load all JSON files from the outputs directory and combine them into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        outputs_path (str): Path to the directory containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with all JSON data\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    \n",
    "    # Get all JSON files in the directory\n",
    "    json_files = list(Path(outputs_path).glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(\"No JSON files found in the specified directory\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # List to store all dataframes\n",
    "    all_dataframes = []\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            # Read JSON file\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.json_normalize(data)\n",
    "            \n",
    "            # Add source file information\n",
    "            df['source_file'] = json_file.name\n",
    "            \n",
    "            all_dataframes.append(df)\n",
    "            print(f\"✅ Loaded: {json_file.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {json_file.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_dataframes:\n",
    "        print(\"No valid JSON files could be loaded\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    \n",
    "    print(f\"\\n📊 Combined DataFrame shape: {combined_df.shape}\")\n",
    "    print(f\"📁 Total files processed: {len(all_dataframes)}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Example usage\n",
    "df_combined = load_all_json_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 36 columns):\n",
      " #   Column                                                                    Non-Null Count  Dtype \n",
      "---  ------                                                                    --------------  ----- \n",
      " 0   bibliographic_metadata.paper_id                                           8 non-null      object\n",
      " 1   bibliographic_metadata.title                                              8 non-null      object\n",
      " 2   bibliographic_metadata.authors                                            8 non-null      object\n",
      " 3   bibliographic_metadata.publication_year                                   8 non-null      int64 \n",
      " 4   bibliographic_metadata.venue                                              8 non-null      object\n",
      " 5   bibliographic_metadata.doi_url                                            8 non-null      object\n",
      " 6   bibliographic_metadata.research_domain                                    8 non-null      object\n",
      " 7   methodological_framework.analysis_type                                    8 non-null      object\n",
      " 8   methodological_framework.theoretical_framework                            8 non-null      object\n",
      " 9   methodological_framework.sample_characteristics.interview_count           8 non-null      object\n",
      " 10  methodological_framework.sample_characteristics.participant_demographics  8 non-null      object\n",
      " 11  methodological_framework.sample_characteristics.data_collection_method    8 non-null      object\n",
      " 12  technical_pipeline.automation_level                                       8 non-null      object\n",
      " 13  technical_pipeline.software_tools                                         8 non-null      object\n",
      " 14  technical_pipeline.computational_methods                                  8 non-null      object\n",
      " 15  technical_pipeline.workflow_architecture.preprocessing_steps              8 non-null      object\n",
      " 16  technical_pipeline.workflow_architecture.analysis_pipeline                8 non-null      object\n",
      " 17  technical_pipeline.workflow_architecture.postprocessing_steps             8 non-null      object\n",
      " 18  technical_pipeline.coding_framework                                       8 non-null      object\n",
      " 19  prompt_engineering.prompting_strategy.approach_type                       8 non-null      object\n",
      " 20  prompt_engineering.prompting_strategy.prompt_examples                     8 non-null      object\n",
      " 21  prompt_engineering.prompting_strategy.engineering_techniques              8 non-null      object\n",
      " 22  prompt_engineering.prompting_strategy.model_specifications                8 non-null      object\n",
      " 23  empirical_results.primary_findings                                        8 non-null      object\n",
      " 24  empirical_results.evaluation_framework.metrics_employed                   8 non-null      object\n",
      " 25  empirical_results.evaluation_framework.validation_methodology             8 non-null      object\n",
      " 26  empirical_results.evaluation_framework.performance_indicators             8 non-null      object\n",
      " 27  empirical_results.methodological_limitations                              8 non-null      object\n",
      " 28  quality_assurance.reliability_measures                                    8 non-null      object\n",
      " 29  quality_assurance.validity_approaches                                     8 non-null      object\n",
      " 30  quality_assurance.bias_mitigation_strategies                              8 non-null      object\n",
      " 31  research_impact.novel_contributions                                       8 non-null      object\n",
      " 32  research_impact.practical_applications                                    8 non-null      object\n",
      " 33  research_impact.future_research_directions                                8 non-null      object\n",
      " 34  research_impact.scalability_considerations                                8 non-null      object\n",
      " 35  source_file                                                               8 non-null      object\n",
      "dtypes: int64(1), object(35)\n",
      "memory usage: 2.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bibliographic_metadata.paper_id</th>\n",
       "      <th>bibliographic_metadata.title</th>\n",
       "      <th>bibliographic_metadata.authors</th>\n",
       "      <th>bibliographic_metadata.publication_year</th>\n",
       "      <th>bibliographic_metadata.venue</th>\n",
       "      <th>bibliographic_metadata.doi_url</th>\n",
       "      <th>bibliographic_metadata.research_domain</th>\n",
       "      <th>methodological_framework.analysis_type</th>\n",
       "      <th>methodological_framework.theoretical_framework</th>\n",
       "      <th>methodological_framework.sample_characteristics.interview_count</th>\n",
       "      <th>...</th>\n",
       "      <th>empirical_results.evaluation_framework.performance_indicators</th>\n",
       "      <th>empirical_results.methodological_limitations</th>\n",
       "      <th>quality_assurance.reliability_measures</th>\n",
       "      <th>quality_assurance.validity_approaches</th>\n",
       "      <th>quality_assurance.bias_mitigation_strategies</th>\n",
       "      <th>research_impact.novel_contributions</th>\n",
       "      <th>research_impact.practical_applications</th>\n",
       "      <th>research_impact.future_research_directions</th>\n",
       "      <th>research_impact.scalability_considerations</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arXiv:2401.04122v3</td>\n",
       "      <td>From Prompt Engineering to Prompt Science With...</td>\n",
       "      <td>[Chirag Shah]</td>\n",
       "      <td>2023</td>\n",
       "      <td>Proceedings of ACM Conference (Conference’17)</td>\n",
       "      <td>https://doi.org/10.1145/nnnnnnn.nnnnnnn</td>\n",
       "      <td>Computational qualitative analysis, Human-cent...</td>\n",
       "      <td>deductive qualitative analysis with human-in-t...</td>\n",
       "      <td>systematic, multi-phase prompt science methodo...</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>...</td>\n",
       "      <td>High inter-coder reliability between human ann...</td>\n",
       "      <td>[Increased cost and resource requirements due ...</td>\n",
       "      <td>[Involvement of at least two qualified researc...</td>\n",
       "      <td>[Establishment and iterative refinement of cle...</td>\n",
       "      <td>[Involvement of multiple researchers to dissol...</td>\n",
       "      <td>[Proposes a multi-phase, human-in-the-loop met...</td>\n",
       "      <td>[Automated and semi-automated labeling of inte...</td>\n",
       "      <td>[Exploring the automation of human-in-the-loop...</td>\n",
       "      <td>The methodology increases process cost by at l...</td>\n",
       "      <td>2401.04122v3.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arXiv:2402.01386v1</td>\n",
       "      <td>Can Large Language Models Serve as Data Analys...</td>\n",
       "      <td>[Zeeshan Rasheed, Muhammad Waseem, Aakash Ahma...</td>\n",
       "      <td>2018</td>\n",
       "      <td>Proceedings of ACM Conference (Conference’17)</td>\n",
       "      <td>https://doi.org/XXXXXXX.XXXXXXX</td>\n",
       "      <td>Software Engineering, Qualitative Data Analysi...</td>\n",
       "      <td>deductive_qualitative_analysis</td>\n",
       "      <td>multi-agent LLM-based automation of qualitativ...</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>87% practitioner satisfaction rate; positive q...</td>\n",
       "      <td>[Limited sample size for practitioner evaluati...</td>\n",
       "      <td>[Practitioner-based evaluation involving 10 pr...</td>\n",
       "      <td>[Engagement of practitioners from academia and...</td>\n",
       "      <td>[Selection of practitioners from diverse domai...</td>\n",
       "      <td>[Introduction of an LLM-based multi-agent mode...</td>\n",
       "      <td>[Automated and expedited qualitative analysis ...</td>\n",
       "      <td>[Exploration of the model's performance and ad...</td>\n",
       "      <td>The multi-agent LLM-based model significantly ...</td>\n",
       "      <td>2402.01386v1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arXiv:2411.14473v4</td>\n",
       "      <td>Large Language Model for Qualitative Research:...</td>\n",
       "      <td>[Cauã Ferreira Barros, Bruna Borges Azevedo, V...</td>\n",
       "      <td>2025</td>\n",
       "      <td>arXiv</td>\n",
       "      <td>https://arxiv.org/abs/2411.14473</td>\n",
       "      <td>Qualitative Research Methodology, Computationa...</td>\n",
       "      <td>deductive_qualitative_analysis</td>\n",
       "      <td>content_analysis; grounded_theory; thematic_an...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>LLMs demonstrated performance equivalent to or...</td>\n",
       "      <td>[Strong dependence on well-structured prompt e...</td>\n",
       "      <td>[Comparison with human analysis, Accuracy, Pre...</td>\n",
       "      <td>[Manual review and full reading of articles to...</td>\n",
       "      <td>[Human verification of LLM outputs and transla...</td>\n",
       "      <td>[Systematic mapping of the state of the art on...</td>\n",
       "      <td>[Automation of open and axial coding in qualit...</td>\n",
       "      <td>[Refinement of prompt engineering techniques t...</td>\n",
       "      <td>LLMs enable scalable qualitative analysis by a...</td>\n",
       "      <td>2411.14473v4.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not_specified</td>\n",
       "      <td>Prompt-based and Fine-tuned GPT Models for Con...</td>\n",
       "      <td>[Chenyu Hou, Gaoxia Zhu, Juan Zheng, Lishan Zh...</td>\n",
       "      <td>2024</td>\n",
       "      <td>The 14th Learning Analytics and Knowledge Conf...</td>\n",
       "      <td>https://doi.org/10.1145/3636555.3636910</td>\n",
       "      <td>Learning Analytics, Educational Technology, Co...</td>\n",
       "      <td>deductive qualitative coding; automated and se...</td>\n",
       "      <td>coding scheme based on cognitive, emotional, a...</td>\n",
       "      <td>7482</td>\n",
       "      <td>...</td>\n",
       "      <td>Cohen's Kappa scores for each coding dimension...</td>\n",
       "      <td>[Limited size of expert-labeled dataset (204 c...</td>\n",
       "      <td>[Cohen’s Kappa calculated between human raters...</td>\n",
       "      <td>[Expert-labeled data used as ground truth for ...</td>\n",
       "      <td>[Iterative prompt engineering to reduce ambigu...</td>\n",
       "      <td>[First systematic evaluation of GPT-based prom...</td>\n",
       "      <td>[Automated or semi-automated deductive coding ...</td>\n",
       "      <td>[Expansion of expert-labeled datasets and vali...</td>\n",
       "      <td>The pipeline is designed to be scalable to lar...</td>\n",
       "      <td>3636555.3636910.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not_specified</td>\n",
       "      <td>When the Prompt becomes the Codebook: Grounded...</td>\n",
       "      <td>[Sriram Ramanathan, Lisa-Angelique Lim, Nazani...</td>\n",
       "      <td>2025</td>\n",
       "      <td>LAK 2025: The 15th International Learning Anal...</td>\n",
       "      <td>https://doi.org/10.1145/3706468.3706564</td>\n",
       "      <td>Learning Analytics, Qualitative Research Metho...</td>\n",
       "      <td>deductive qualitative analysis using automated...</td>\n",
       "      <td>Four domains of belonging framework (Ahn &amp; Dav...</td>\n",
       "      <td>860</td>\n",
       "      <td>...</td>\n",
       "      <td>Substantial agreement between human and LLM co...</td>\n",
       "      <td>[Human-coded sample comprised only 2% of the f...</td>\n",
       "      <td>[Inter-Rater Reliability (IRR) between human c...</td>\n",
       "      <td>[Theory-driven codebook development grounded i...</td>\n",
       "      <td>[Human-in-the-loop iterative prompt and codebo...</td>\n",
       "      <td>[Introduction of Grounded Prompt Engineering (...</td>\n",
       "      <td>[Automated or semi-automated deductive coding ...</td>\n",
       "      <td>[Testing the GROPROE process and developed pro...</td>\n",
       "      <td>GROPROE enables deductive qualitative analysis...</td>\n",
       "      <td>3706468.3706564.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>not_specified</td>\n",
       "      <td>From nCoder to ChatGPT: From Automated Coding ...</td>\n",
       "      <td>[Andres Felipe Zambrano, Xiner Liu, Amanda Bar...</td>\n",
       "      <td>2024</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>Qualitative Research Methodology, Computationa...</td>\n",
       "      <td>deductive qualitative coding; automated and se...</td>\n",
       "      <td>Quantitative Ethnography; construct validity; ...</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>nCoder achieved higher average Kappa (0.77 tra...</td>\n",
       "      <td>[nCoder's reliance on regular expressions limi...</td>\n",
       "      <td>[Cohen's Kappa, Precision, Recall, Shaffer's r...</td>\n",
       "      <td>[Construct validity assessment via iterative r...</td>\n",
       "      <td>[Inclusion of original dataset author to reduc...</td>\n",
       "      <td>[Systematic comparison of ChatGPT (GPT-4) and ...</td>\n",
       "      <td>[Use of ChatGPT as a semi-automated coding ass...</td>\n",
       "      <td>[Exploration of ChatGPT and other LLMs for sup...</td>\n",
       "      <td>Manual coding is unsuitable for large datasets...</td>\n",
       "      <td>ChatGPT_ICQE_FinalVersion.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>not_specified</td>\n",
       "      <td>ChatGPT for Education Research: Exploring the ...</td>\n",
       "      <td>[Amanda Barany, Nidhi Nasiar, Chelsea Porter, ...</td>\n",
       "      <td>2024</td>\n",
       "      <td>AIED 2024, LNAI 14830, pp. 134–149, Springer N...</td>\n",
       "      <td>https://doi.org/10.1007/978-3-031-64299-9_10</td>\n",
       "      <td>Educational Research / Qualitative Data Analys...</td>\n",
       "      <td>inductive qualitative coding (codebook develop...</td>\n",
       "      <td>not_specified</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>Hybrid approaches achieved the highest ratings...</td>\n",
       "      <td>[Study focused on inductive codebook developme...</td>\n",
       "      <td>[Cohen’s kappa (κ) used to assess consistency ...</td>\n",
       "      <td>[Human review and revision of codebooks genera...</td>\n",
       "      <td>[Independent codebook development by separate ...</td>\n",
       "      <td>[Systematic comparison of four codebook develo...</td>\n",
       "      <td>[Acceleration of qualitative codebook developm...</td>\n",
       "      <td>[Further investigation into optimal division o...</td>\n",
       "      <td>Hybrid human-LLM approaches demonstrated impro...</td>\n",
       "      <td>extracted_pages_134_149.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16094069241231168</td>\n",
       "      <td>An Examination of the Use of Large Language Mo...</td>\n",
       "      <td>[Robert H. Tai, Lillian R. Bentley, Xin Xia, J...</td>\n",
       "      <td>2024</td>\n",
       "      <td>International Journal of Qualitative Methods</td>\n",
       "      <td>https://doi.org/10.1177/16094069241231168</td>\n",
       "      <td>qualitative research methodology, computationa...</td>\n",
       "      <td>deductive qualitative coding using large langu...</td>\n",
       "      <td>codebook-driven deductive analysis based on pr...</td>\n",
       "      <td>125</td>\n",
       "      <td>...</td>\n",
       "      <td>LLMq values plateaued and stabilized after app...</td>\n",
       "      <td>[LLMs rely on patterns in their training data;...</td>\n",
       "      <td>[Inter-rater reliability using Cohen’s kappa s...</td>\n",
       "      <td>[Use of a detailed codebook with clear definit...</td>\n",
       "      <td>[Multiple coders independently coding and reso...</td>\n",
       "      <td>[Proposes a systematic methodology for using L...</td>\n",
       "      <td>[LLMs can be used as an efficient screening to...</td>\n",
       "      <td>[Exploration of LLMs for inductive qualitative...</td>\n",
       "      <td>LLMs offer high scalability for qualitative an...</td>\n",
       "      <td>tai-et-al-2024-an-examination-of-the-use-of-la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  bibliographic_metadata.paper_id  \\\n",
       "0              arXiv:2401.04122v3   \n",
       "1              arXiv:2402.01386v1   \n",
       "2              arXiv:2411.14473v4   \n",
       "3                   not_specified   \n",
       "4                   not_specified   \n",
       "5                   not_specified   \n",
       "6                   not_specified   \n",
       "7               16094069241231168   \n",
       "\n",
       "                        bibliographic_metadata.title  \\\n",
       "0  From Prompt Engineering to Prompt Science With...   \n",
       "1  Can Large Language Models Serve as Data Analys...   \n",
       "2  Large Language Model for Qualitative Research:...   \n",
       "3  Prompt-based and Fine-tuned GPT Models for Con...   \n",
       "4  When the Prompt becomes the Codebook: Grounded...   \n",
       "5  From nCoder to ChatGPT: From Automated Coding ...   \n",
       "6  ChatGPT for Education Research: Exploring the ...   \n",
       "7  An Examination of the Use of Large Language Mo...   \n",
       "\n",
       "                      bibliographic_metadata.authors  \\\n",
       "0                                      [Chirag Shah]   \n",
       "1  [Zeeshan Rasheed, Muhammad Waseem, Aakash Ahma...   \n",
       "2  [Cauã Ferreira Barros, Bruna Borges Azevedo, V...   \n",
       "3  [Chenyu Hou, Gaoxia Zhu, Juan Zheng, Lishan Zh...   \n",
       "4  [Sriram Ramanathan, Lisa-Angelique Lim, Nazani...   \n",
       "5  [Andres Felipe Zambrano, Xiner Liu, Amanda Bar...   \n",
       "6  [Amanda Barany, Nidhi Nasiar, Chelsea Porter, ...   \n",
       "7  [Robert H. Tai, Lillian R. Bentley, Xin Xia, J...   \n",
       "\n",
       "   bibliographic_metadata.publication_year  \\\n",
       "0                                     2023   \n",
       "1                                     2018   \n",
       "2                                     2025   \n",
       "3                                     2024   \n",
       "4                                     2025   \n",
       "5                                     2024   \n",
       "6                                     2024   \n",
       "7                                     2024   \n",
       "\n",
       "                        bibliographic_metadata.venue  \\\n",
       "0      Proceedings of ACM Conference (Conference’17)   \n",
       "1      Proceedings of ACM Conference (Conference’17)   \n",
       "2                                              arXiv   \n",
       "3  The 14th Learning Analytics and Knowledge Conf...   \n",
       "4  LAK 2025: The 15th International Learning Anal...   \n",
       "5                                      not_specified   \n",
       "6  AIED 2024, LNAI 14830, pp. 134–149, Springer N...   \n",
       "7       International Journal of Qualitative Methods   \n",
       "\n",
       "                 bibliographic_metadata.doi_url  \\\n",
       "0       https://doi.org/10.1145/nnnnnnn.nnnnnnn   \n",
       "1               https://doi.org/XXXXXXX.XXXXXXX   \n",
       "2              https://arxiv.org/abs/2411.14473   \n",
       "3       https://doi.org/10.1145/3636555.3636910   \n",
       "4       https://doi.org/10.1145/3706468.3706564   \n",
       "5                                 not_specified   \n",
       "6  https://doi.org/10.1007/978-3-031-64299-9_10   \n",
       "7     https://doi.org/10.1177/16094069241231168   \n",
       "\n",
       "              bibliographic_metadata.research_domain  \\\n",
       "0  Computational qualitative analysis, Human-cent...   \n",
       "1  Software Engineering, Qualitative Data Analysi...   \n",
       "2  Qualitative Research Methodology, Computationa...   \n",
       "3  Learning Analytics, Educational Technology, Co...   \n",
       "4  Learning Analytics, Qualitative Research Metho...   \n",
       "5  Qualitative Research Methodology, Computationa...   \n",
       "6  Educational Research / Qualitative Data Analys...   \n",
       "7  qualitative research methodology, computationa...   \n",
       "\n",
       "              methodological_framework.analysis_type  \\\n",
       "0  deductive qualitative analysis with human-in-t...   \n",
       "1                     deductive_qualitative_analysis   \n",
       "2                     deductive_qualitative_analysis   \n",
       "3  deductive qualitative coding; automated and se...   \n",
       "4  deductive qualitative analysis using automated...   \n",
       "5  deductive qualitative coding; automated and se...   \n",
       "6  inductive qualitative coding (codebook develop...   \n",
       "7  deductive qualitative coding using large langu...   \n",
       "\n",
       "      methodological_framework.theoretical_framework  \\\n",
       "0  systematic, multi-phase prompt science methodo...   \n",
       "1  multi-agent LLM-based automation of qualitativ...   \n",
       "2  content_analysis; grounded_theory; thematic_an...   \n",
       "3  coding scheme based on cognitive, emotional, a...   \n",
       "4  Four domains of belonging framework (Ahn & Dav...   \n",
       "5  Quantitative Ethnography; construct validity; ...   \n",
       "6                                      not_specified   \n",
       "7  codebook-driven deductive analysis based on pr...   \n",
       "\n",
       "  methodological_framework.sample_characteristics.interview_count  ...  \\\n",
       "0                                      not_specified               ...   \n",
       "1                                                 10               ...   \n",
       "2                                                  3               ...   \n",
       "3                                               7482               ...   \n",
       "4                                                860               ...   \n",
       "5                                                200               ...   \n",
       "6                                                  4               ...   \n",
       "7                                                125               ...   \n",
       "\n",
       "  empirical_results.evaluation_framework.performance_indicators  \\\n",
       "0  High inter-coder reliability between human ann...              \n",
       "1  87% practitioner satisfaction rate; positive q...              \n",
       "2  LLMs demonstrated performance equivalent to or...              \n",
       "3  Cohen's Kappa scores for each coding dimension...              \n",
       "4  Substantial agreement between human and LLM co...              \n",
       "5  nCoder achieved higher average Kappa (0.77 tra...              \n",
       "6  Hybrid approaches achieved the highest ratings...              \n",
       "7  LLMq values plateaued and stabilized after app...              \n",
       "\n",
       "        empirical_results.methodological_limitations  \\\n",
       "0  [Increased cost and resource requirements due ...   \n",
       "1  [Limited sample size for practitioner evaluati...   \n",
       "2  [Strong dependence on well-structured prompt e...   \n",
       "3  [Limited size of expert-labeled dataset (204 c...   \n",
       "4  [Human-coded sample comprised only 2% of the f...   \n",
       "5  [nCoder's reliance on regular expressions limi...   \n",
       "6  [Study focused on inductive codebook developme...   \n",
       "7  [LLMs rely on patterns in their training data;...   \n",
       "\n",
       "              quality_assurance.reliability_measures  \\\n",
       "0  [Involvement of at least two qualified researc...   \n",
       "1  [Practitioner-based evaluation involving 10 pr...   \n",
       "2  [Comparison with human analysis, Accuracy, Pre...   \n",
       "3  [Cohen’s Kappa calculated between human raters...   \n",
       "4  [Inter-Rater Reliability (IRR) between human c...   \n",
       "5  [Cohen's Kappa, Precision, Recall, Shaffer's r...   \n",
       "6  [Cohen’s kappa (κ) used to assess consistency ...   \n",
       "7  [Inter-rater reliability using Cohen’s kappa s...   \n",
       "\n",
       "               quality_assurance.validity_approaches  \\\n",
       "0  [Establishment and iterative refinement of cle...   \n",
       "1  [Engagement of practitioners from academia and...   \n",
       "2  [Manual review and full reading of articles to...   \n",
       "3  [Expert-labeled data used as ground truth for ...   \n",
       "4  [Theory-driven codebook development grounded i...   \n",
       "5  [Construct validity assessment via iterative r...   \n",
       "6  [Human review and revision of codebooks genera...   \n",
       "7  [Use of a detailed codebook with clear definit...   \n",
       "\n",
       "        quality_assurance.bias_mitigation_strategies  \\\n",
       "0  [Involvement of multiple researchers to dissol...   \n",
       "1  [Selection of practitioners from diverse domai...   \n",
       "2  [Human verification of LLM outputs and transla...   \n",
       "3  [Iterative prompt engineering to reduce ambigu...   \n",
       "4  [Human-in-the-loop iterative prompt and codebo...   \n",
       "5  [Inclusion of original dataset author to reduc...   \n",
       "6  [Independent codebook development by separate ...   \n",
       "7  [Multiple coders independently coding and reso...   \n",
       "\n",
       "                 research_impact.novel_contributions  \\\n",
       "0  [Proposes a multi-phase, human-in-the-loop met...   \n",
       "1  [Introduction of an LLM-based multi-agent mode...   \n",
       "2  [Systematic mapping of the state of the art on...   \n",
       "3  [First systematic evaluation of GPT-based prom...   \n",
       "4  [Introduction of Grounded Prompt Engineering (...   \n",
       "5  [Systematic comparison of ChatGPT (GPT-4) and ...   \n",
       "6  [Systematic comparison of four codebook develo...   \n",
       "7  [Proposes a systematic methodology for using L...   \n",
       "\n",
       "              research_impact.practical_applications  \\\n",
       "0  [Automated and semi-automated labeling of inte...   \n",
       "1  [Automated and expedited qualitative analysis ...   \n",
       "2  [Automation of open and axial coding in qualit...   \n",
       "3  [Automated or semi-automated deductive coding ...   \n",
       "4  [Automated or semi-automated deductive coding ...   \n",
       "5  [Use of ChatGPT as a semi-automated coding ass...   \n",
       "6  [Acceleration of qualitative codebook developm...   \n",
       "7  [LLMs can be used as an efficient screening to...   \n",
       "\n",
       "          research_impact.future_research_directions  \\\n",
       "0  [Exploring the automation of human-in-the-loop...   \n",
       "1  [Exploration of the model's performance and ad...   \n",
       "2  [Refinement of prompt engineering techniques t...   \n",
       "3  [Expansion of expert-labeled datasets and vali...   \n",
       "4  [Testing the GROPROE process and developed pro...   \n",
       "5  [Exploration of ChatGPT and other LLMs for sup...   \n",
       "6  [Further investigation into optimal division o...   \n",
       "7  [Exploration of LLMs for inductive qualitative...   \n",
       "\n",
       "          research_impact.scalability_considerations  \\\n",
       "0  The methodology increases process cost by at l...   \n",
       "1  The multi-agent LLM-based model significantly ...   \n",
       "2  LLMs enable scalable qualitative analysis by a...   \n",
       "3  The pipeline is designed to be scalable to lar...   \n",
       "4  GROPROE enables deductive qualitative analysis...   \n",
       "5  Manual coding is unsuitable for large datasets...   \n",
       "6  Hybrid human-LLM approaches demonstrated impro...   \n",
       "7  LLMs offer high scalability for qualitative an...   \n",
       "\n",
       "                                         source_file  \n",
       "0                                  2401.04122v3.json  \n",
       "1                                  2402.01386v1.json  \n",
       "2                                  2411.14473v4.json  \n",
       "3                               3636555.3636910.json  \n",
       "4                               3706468.3706564.json  \n",
       "5                     ChatGPT_ICQE_FinalVersion.json  \n",
       "6                       extracted_pages_134_149.json  \n",
       "7  tai-et-al-2024-an-examination-of-the-use-of-la...  \n",
       "\n",
       "[8 rows x 36 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quali_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
