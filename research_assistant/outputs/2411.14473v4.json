{
  "bibliographic_metadata": {
    "paper_id": "arXiv:2411.14473v4",
    "title": "Large Language Model for Qualitative Research: A Systematic Mapping Study",
    "authors": [
      "Cauã Ferreira Barros",
      "Bruna Borges Azevedo",
      "Valdemar Vicente Graciano Neto",
      "Mohamad Kassab",
      "Marcos Kalinowski",
      "Hugo Alexandre D. do Nascimento",
      "Michelle C.G.S.P. Bandeira"
    ],
    "publication_year": 2025,
    "venue": "arXiv",
    "doi_url": "https://arxiv.org/abs/2411.14473",
    "research_domain": "Qualitative Research Methodology, Computational Text Analysis, Artificial Intelligence, Large Language Models"
  },
  "methodological_framework": {
    "analysis_type": "deductive_qualitative_analysis",
    "theoretical_framework": "content_analysis; grounded_theory; thematic_analysis",
    "sample_characteristics": {
      "interview_count": 3,
      "participant_demographics": "healthcare (patients), education (students), general/cultural (not_specified)",
      "data_collection_method": "semi_structured_interviews; open_ended_survey_responses; documents; social_media; song_lyrics"
    }
  },
  "technical_pipeline": {
    "automation_level": "semi-automated to fully automated (depending on study and workflow; LLMs automate coding, theme extraction, and categorization, but human oversight and interpretation are often retained, especially for nuanced or ambiguous data)",
    "software_tools": [
      "ChatGPT",
      "GPT-4",
      "LLaMA-2",
      "ATLAS.TI",
      "BERT",
      "Sabiá-2 medium",
      "BERTopic",
      "Parsif.al",
      "CollabCoder"
    ],
    "computational_methods": [
      "prompt-based instruction",
      "automated extraction",
      "fine-tuning of LLMs",
      "pseudo response generation",
      "topic modeling",
      "collaborative AI-assisted coding",
      "manual content analysis (for validation)",
      "zero-shot classification"
    ],
    "workflow_architecture": {
      "preprocessing_steps": [
        "data collection (interviews, documents, social media, song lyrics, book reviews)",
        "data cleaning",
        "translation (if needed, e.g., via ChatGPT)",
        "prompt engineering (design and testing of prompts for LLMs)",
        "model selection and configuration (e.g., fine-tuning, adaptation, or prompt-only use)"
      ],
      "analysis_pipeline": [
        "application of LLMs for open coding, axial coding, or deductive coding",
        "theme extraction and categorization",
        "topic modeling (e.g., with BERTopic or BERT)",
        "automated code suggestion (e.g., CollabCoder)",
        "collaborative codebook development (AI-assisted workflows)",
        "integration of human review for ambiguous or nuanced cases",
        "comparison with traditional/manual qualitative analysis"
      ],
      "postprocessing_steps": [
        "evaluation of coding outputs (accuracy, precision, recall, F1 score, agreement rates)",
        "consensus building among coders (e.g., via CollabCoder discussions)",
        "visualization (limited; some graphical representations require human intervention)",
        "reporting and documentation of prompt engineering and workflow",
        "manual review for validation and clarification"
      ]
    },
    "coding_framework": "Content analysis, Grounded Theory, Thematic Analysis, Deductive coding, Inductive coding (frameworks vary by study; prompt engineering is essential for reproducibility; human-in-the-loop recommended for interpretative validity)"
  },
  "prompt_engineering": {
    "prompting_strategy": {
      "approach_type": "structured prompt engineering with iterative refinement",
      "prompt_examples": [
        "Prompts designed to extract specific data extraction criteria (DE1-DE16) mapped to research questions (RQ1-RQ5), e.g., 'Does the study primarily focus on analyzing the use of LLMs to support qualitative data analysis?'",
        "Prompts for coding open-ended responses using pseudo response generation by LLMs",
        "Prompts for instructing LLMs to perform open coding, theme extraction, categorization, and topic modeling",
        "Prompts for automating article screening in systematic reviews",
        "Prompts for generating code suggestions and facilitating coder discussions in collaborative workflows"
      ],
      "engineering_techniques": [
        "Iterative prompt refinement and testing for alignment with research objectives",
        "Use of pre-defined response options to ensure consistency in data extraction",
        "Manual review and validation of LLM outputs against full-text articles",
        "Exclusion of studies not detailing prompt engineering to ensure reproducibility",
        "Public sharing of prompt versions for transparency (e.g., via Zenodo)",
        "Prompt-based instruction for qualitative coding tasks (open coding, theme extraction, categorization)",
        "Integration of prompt engineering with automated extraction tools (e.g., BERT, ATLAS.TI)",
        "Optimization of prompt design for AI-assisted code generation and collaborative workflows"
      ],
      "model_specifications": "ChatGPT (including GPT-4), LLaMA-2, ATLAS.TI, BERT, Sabiá-2 medium; ChatGPT 4.0 (paid version) used for data extraction and translation; models used with and without fine-tuning, with a focus on prompt engineering for task adaptation"
    }
  },
  "empirical_results": {
    "primary_findings": [
      "LLMs, particularly ChatGPT and its variants, are increasingly used to automate and enhance qualitative analysis across domains such as healthcare, education, and cultural studies.",
      "LLMs can automate open coding and theme extraction, providing consistent and reproducible outputs, and significantly reducing analysis time from weeks to hours.",
      "Effectiveness of LLM-assisted qualitative analysis is generally equivalent to traditional manual methods, with some studies reporting superior performance and others noting lower effectiveness compared to human analysis.",
      "Prompt engineering is critical for reproducibility and accuracy; studies that did not detail prompt engineering were excluded from the mapping.",
      "LLMs are most effective for automation-friendly tasks (e.g., initial coding, theme identification), while interpretative and creative processes still require human oversight.",
      "Collaborative workflows integrating LLMs (e.g., CollabCoder) can improve coder agreement, streamline discussions, and lower the learning curve compared to traditional tools.",
      "Current LLMs struggle with complex integrations, visualizations, and nuanced contextual or emotional interpretation."
    ],
    "evaluation_framework": {
      "metrics_employed": [
        "Comparison with human analysis",
        "Accuracy",
        "Precision",
        "Recall",
        "F1 Score",
        "Cohen’s Kappa",
        "Agreement Rate"
      ],
      "validation_methodology": "LLM-assisted qualitative analyses were compared against traditional manual methods using both quantitative metrics (accuracy, precision, recall, F1 Score, Cohen’s Kappa, agreement rate) and qualitative assessments (user studies, coder consensus, workflow efficiency). Studies required detailed reporting of prompt engineering to ensure reproducibility. Manual review and consensus discussions were used to mitigate bias and validate extracted data.",
      "performance_indicators": "LLMs demonstrated performance equivalent to or better than traditional methods in most studies, with high agreement rates and efficiency gains (e.g., coding time reduced from weeks to hours). Some studies reported lower effectiveness or challenges in nuanced interpretation. Collaborative LLM workflows improved coder consensus and reduced disagreement."
    },
    "methodological_limitations": [
      "Strong dependence on well-structured prompt engineering for accurate and reproducible results.",
      "LLMs may generate hallucinations—fabricated responses not grounded in source data.",
      "Inherent model biases, especially when handling sensitive or subjective information.",
      "Difficulty in assigning topics to subjective or ambiguous expressions and lack of context sensitivity and emotional nuance.",
      "Over-segmentation and over-categorization of responses by LLMs.",
      "Challenges in maintaining coder independence and avoiding premature influence from LLM-generated suggestions in collaborative workflows.",
      "Potential for loss of nuanced or detailed categorization important to human researchers.",
      "Inconsistencies in zero-shot classification and risk of echo chamber effects (reproduction of pre-trained patterns).",
      "Limited ability to perform complex integrations or visualizations required for advanced qualitative analysis.",
      "Possible confirmation bias due to using LLMs both for data extraction and for evaluating LLM effectiveness.",
      "Exclusion of studies lacking detailed prompt engineering may restrict the comprehensiveness of the mapping.",
      "Absence of snowballing in the systematic mapping may have led to omission of relevant studies.",
      "LLMs may lack deep contextual understanding, resulting in superficial or incomplete information extraction."
    ]
  },
  "quality_assurance": {
    "reliability_measures": [
      "Comparison with human analysis",
      "Accuracy",
      "Precision",
      "Recall",
      "F1 Score",
      "Cohen’s Kappa",
      "Agreement Rate",
      "Manual review of extracted data",
      "Standardized selection criteria for study inclusion",
      "Iterative prompt engineering with validation using test articles"
    ],
    "validity_approaches": [
      "Manual review and full reading of articles to clarify doubts and ensure analytical depth",
      "Inclusion of studies from diverse domains to enhance external validity",
      "Iterative prompt engineering process with transparency (versions made available on Zenodo)",
      "Validation of model outputs against known responses",
      "Requirement for detailed prompt engineering descriptions to ensure reproducibility",
      "Consensus discussions between researchers for study selection",
      "Use of established qualitative analysis frameworks (e.g., Grounded Theory, Content Analysis, Thematic Analysis)",
      "Comparison with traditional qualitative analysis methods"
    ],
    "bias_mitigation_strategies": [
      "Human verification of LLM outputs and translations",
      "Joint discussions between researchers to resolve inclusion uncertainties",
      "Reliance on solid references from analyzed articles to avoid confirmation bias",
      "Manual review to mitigate superficiality and incomplete extraction by LLMs",
      "Requirement for studies to detail prompt engineering to ensure replicability and reduce methodological bias",
      "Recommendations for LLMs to serve as aids rather than replacements for human analysts, with humans retaining interpretative authority",
      "Emphasis on user control over AI-generated suggestions to prevent over-reliance and premature influence",
      "Recognition and reporting of LLM limitations such as hallucinations, over-segmentation, and model bias"
    ]
  },
  "research_impact": {
    "novel_contributions": [
      "Systematic mapping of the state of the art on the use of Large Language Models (LLMs) in deductive and inductive qualitative analysis, with a focus on interview and textual data.",
      "Identification and synthesis of application contexts, LLM configurations, methodologies, and evaluation metrics for automated and semi-automated qualitative analysis pipelines.",
      "Establishment of a standardized data extraction protocol and inclusion/exclusion criteria emphasizing reproducibility through detailed prompt engineering documentation.",
      "Empirical comparison of LLM-assisted qualitative analysis with traditional manual methods across multiple domains (healthcare, education, culture, technology).",
      "Highlighting the effectiveness of LLMs (e.g., ChatGPT, LLaMA-2, BERT) in automating open coding, theme extraction, and categorization, with performance often equivalent or superior to human analysis.",
      "Introduction of collaborative workflows (e.g., CollabCoder) integrating LLMs for code suggestion, coder discussion, and codebook development."
    ],
    "practical_applications": [
      "Automation of open and axial coding in qualitative research, drastically reducing analysis time from weeks to hours.",
      "Integration of LLMs into collaborative qualitative analysis workflows to streamline coder discussions, decision-making, and codebook refinement.",
      "Use of LLMs for rapid theme extraction and categorization in large-scale interview datasets and open-ended survey responses.",
      "Application of LLMs in healthcare, education, and cultural studies for efficient processing of patient feedback, student surveys, and textual corpora.",
      "Development of user-friendly interfaces and workflows (e.g., CollabCoder) to lower the barrier for non-AI experts in conducting rigorous qualitative analysis."
    ],
    "future_research_directions": [
      "Refinement of prompt engineering techniques to enhance LLM accuracy, robustness, and flexibility in qualitative analysis tasks.",
      "Exploration of advanced LLM architectures and fine-tuning strategies to improve semantic and subjective nuance capture.",
      "Development of methods to mitigate LLM hallucinations and biases, including validation and filtering techniques.",
      "Expansion of open-source LLMs and user-friendly tools to increase accessibility and reduce costs for qualitative researchers.",
      "Establishment of standardized, robust evaluation metrics tailored to the complexity of qualitative analysis performed by LLMs.",
      "Investigation of LLM integration in software engineering research, particularly for requirements engineering and user feedback analysis.",
      "Optimization of AI-assisted collaborative workflows to balance coder independence and consensus, and to prevent premature influence from LLM-generated suggestions.",
      "Incorporation of multimodal data and interaction models to further enhance coder collaboration and analysis depth."
    ],
    "scalability_considerations": "LLMs enable scalable qualitative analysis by automating coding and theme extraction across large unstructured datasets (e.g., hundreds of interviews, millions of social media posts), overcoming traditional bottlenecks of manual analysis. However, scalability is contingent on well-structured prompt engineering, model robustness, and the ability to maintain accuracy and contextual sensitivity at scale. Collaborative workflows and user-friendly interfaces further support scalability by lowering technical barriers and facilitating coder alignment in large research teams."
  }
}