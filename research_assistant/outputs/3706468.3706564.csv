bibliographic_metadata.paper_id,bibliographic_metadata.title,bibliographic_metadata.authors,bibliographic_metadata.publication_year,bibliographic_metadata.venue,bibliographic_metadata.doi_url,bibliographic_metadata.research_domain,methodological_framework.analysis_type,methodological_framework.theoretical_framework,methodological_framework.sample_characteristics.interview_count,methodological_framework.sample_characteristics.participant_demographics,methodological_framework.sample_characteristics.data_collection_method,technical_pipeline.automation_level,technical_pipeline.software_tools,technical_pipeline.computational_methods,technical_pipeline.workflow_architecture.preprocessing_steps,technical_pipeline.workflow_architecture.analysis_pipeline,technical_pipeline.workflow_architecture.postprocessing_steps,technical_pipeline.coding_framework,prompt_engineering.prompting_strategy.approach_type,prompt_engineering.prompting_strategy.prompt_examples,prompt_engineering.prompting_strategy.engineering_techniques,prompt_engineering.prompting_strategy.model_specifications,empirical_results.primary_findings,empirical_results.evaluation_framework.metrics_employed,empirical_results.evaluation_framework.validation_methodology,empirical_results.evaluation_framework.performance_indicators,empirical_results.methodological_limitations,quality_assurance.reliability_measures,quality_assurance.validity_approaches,quality_assurance.bias_mitigation_strategies,research_impact.novel_contributions,research_impact.practical_applications,research_impact.future_research_directions,research_impact.scalability_considerations
not_specified,When the Prompt becomes the Codebook: Grounded Prompt Engineering (GROPROE) and its application to Belonging Analytics,"['Sriram Ramanathan', 'Lisa-Angelique Lim', 'Nazanin Rezazadeh Mottaghi', 'Simon Buckingham Shum']",2025,LAK 2025: The 15th International Learning Analytics and Knowledge Conference,https://doi.org/10.1145/3706468.3706564,"Learning Analytics, Qualitative Research Methodology, Computational Text Analysis",deductive qualitative analysis using automated and semi-automated computational workflows,"Four domains of belonging framework (Ahn & Davis, 2020) extended with 'experiencing emotions' as a higher order code; codebook grounded in literature on affective engagement and belonging in higher education",860,undergraduate business students at University of Technology Sydney; further demographic details not_specified,written reflections collected at the start and end of semester (Stage I: goals for the subject; Stage II: evaluation of goals); maximum length 500 words per reflection,semi-automated,"['Azure Playground', 'GPT-4']","['Large Language Model-based deductive coding', 'Chain-of-Thought (CoT) prompting', 'Few-shot prompting', 'Prompt decomposition', 'Multi-prompt learning', 'Seed-word prompting']","['Development of codebook grounded in literature', 'Grouping indicators into higher order categories and sub-categories', 'Manual coding of sample set by human coders', 'Consensus meetings to resolve coding disagreements']","['Iterative refinement of codebook and translation into system prompt', 'Design and testing of prompt using GPT-4 in Azure Playground', 'Application of prompt to sample reflections for deductive coding', 'Inclusion of Chain-of-Thought reasoning in LLM outputs', 'Few-shot prompt construction with examples and explicit definitions', 'Iterative prompt engineering based on LLM output and human discussion', 'Repeated LLM coding of sample texts to assess output stability (LLMq metric)']","['Calculation of Inter-Annotator Reliability (IAR) using Cohen’s Kappa', 'Calculation of Large Language Model Quotient (LLMq) for coding consistency', 'Comparison of LLM coding with human coding', 'Analysis of LLM reasoning for code assignment', 'Reporting of findings and discussion of discrepancies']","Theory-driven, literature-grounded codebook based on the four domains of belonging (academic belonging, interpersonal belonging, belonging to surroundings, mattering as belonging) and experiencing emotions, with explicit subcategories and indicators; codebook iteratively refined and operationalized as a system prompt for LLM-based deductive coding.","Grounded Prompt Engineering (GROPROE); theory-driven, codebook-based, iterative refinement","['Zero-shot prompt: Initial prompt based on codebook definitions without examples (see Figure 4).', 'Final few-shot prompt: System prompt including explicit definitions, hierarchical codebook, multiple examples per code, step-by-step instructions, and Chain-of-Thought (CoT) reasoning (see Appendix).']","['Translation of literature-derived codebook into system prompt', 'Iterative prompt refinement based on LLM output and human discussion', 'Prompt decomposition', 'Multi-prompt learning with codebook', 'Inclusion of input-output exemplars (few-shot prompting)', 'Explicit construct/label directives', 'Chain-of-Thought (CoT) prompting for reasoning transparency', 'Seed-word prompting for clarity', 'Modification of instructions, formatting, and output criteria', 'Inclusion of supporting context and simplified definitions', 'Human-in-the-loop prompt validation and revision']",GPT-4 via Azure Playground; prompt tested and iteratively refined in this environment,"['The GROPROE (Grounded Prompt Engineering) process enables systematic, theory-grounded prompt engineering for deductive qualitative analysis using LLMs.', 'A codebook derived from literature on affective engagement and belonging was successfully translated into a system prompt for LLM-based coding.', 'Substantial agreement was achieved between human and LLM coding, with Cohen’s Kappa values of 0.74 (human-human) and 0.76 (human-LLM), indicating reliable automated coding.', 'LLM coding consistency, measured by the LLM Quotient (LLMq), showed stable application of most codes across 60 iterations, though some codes (e.g., interpersonal belonging, belonging to surroundings) exhibited more variability.', 'Iterative prompt refinement, including Chain-of-Thought (CoT) reasoning and few-shot examples, improved LLM coding accuracy and alignment with human coders.', 'The LLM’s reasoning sometimes prompted revision of the codebook, demonstrating productive human-AI interaction but also highlighting risks of LLM sycophancy.', 'The GROPROE pipeline is the first reported use of LLMs for deductive coding of affective engagement and belonging in student reflections.']","['Cohen’s Kappa (Inter-Annotator Reliability, IAR, and Inter-Rater Reliability, IRR)', 'Large Language Model Quotient (LLMq) for coding consistency']","Manual coding of a sample set (20 reflections) by two human coders using the codebook, calculation of IRR (Cohen’s Kappa), comparison of LLM coding to human coding (IAR, Cohen’s Kappa), repeated LLM coding (60 iterations per text) to compute LLMq for consistency assessment.","Substantial agreement between human and LLM coding (Cohen’s Kappa = 0.76); stable LLMq for most codes across multiple iterations; some variability in specific codes (e.g., interpersonal belonging, belonging to surroundings).","['Human-coded sample comprised only 2% of the full dataset, which may not be representative.', 'LLMq was calculated over 60 iterations per text, fewer than the 160 iterations in prior studies, due to technical limitations.', 'LLM exhibited occasional misclassification by considering factors outside the intended university context.', 'Potential for LLM sycophancy (bias to please the user) affecting coding decisions.', 'Generalizability of GROPROE to other domains or text types remains to be tested.']","['Inter-Rater Reliability (IRR) between human coders calculated using Cohen’s Kappa', 'Inter-Annotator Reliability (IAR) between human and LLM coding calculated using Cohen’s Kappa', 'Large Language Model Quotient (LLMq) to assess consistency of LLM coding across multiple iterations']","['Theory-driven codebook development grounded in established literature and frameworks (e.g., four domains of belonging)', 'Iterative refinement of codebook and prompt based on empirical outputs and researcher discussion', 'Use of few-shot and chain-of-thought (CoT) prompting to elicit reasoning and ensure alignment with theoretical constructs', 'Manual coding of a sample set by researchers to validate LLM outputs']","['Human-in-the-loop iterative prompt and codebook refinement to address discrepancies and update coding scheme', 'Explicit instruction to LLM to provide reasoning for code assignments (CoT) to enable researcher review and challenge', 'Awareness and monitoring of LLM sycophancy (bias to please user) with researchers taking responsibility for final coding decisions', 'Comparison of LLM coding with human coding to identify and address systematic differences']","['Introduction of Grounded Prompt Engineering (GROPROE) as a systematic, literature-grounded process for developing prompts for deductive qualitative analysis using Large Language Models (LLMs).', 'First demonstration of using LLMs to infer affective engagement and sense of belonging from student reflective writing in higher education.', 'Detailed worked example of translating theoretical constructs and empirical indicators into a hierarchical codebook and then into an LLM prompt for automated coding.', 'Empirical evaluation of human-LLM inter-annotator reliability (IAR) and LLM coding consistency using the Large Language Model Quotient (LLMq) metric.', ""Critical reflection on the dynamics of human-AI interaction in qualitative analysis, including the phenomenon of 'prompt becoming the codebook' and LLM agency/sycophancy.""]","['Automated or semi-automated deductive coding of large-scale qualitative datasets (e.g., student reflections) in educational research and learning analytics.', 'Embedding theory-grounded LLM prompts into survey analytics for efficient analysis of open-ended student responses.', 'Potential integration of GROPROE-based LLM prompts into student-facing chatbots to adapt conversational strategies based on detected affective engagement and belonging.', 'Augmentation of human qualitative analysis workflows with LLMs to improve coding consistency, scalability, and transparency.']","['Testing the GROPROE process and developed prompts on other qualitative datasets and educational contexts to assess generalizability.', 'Scaling up human validation of LLM-coded data to larger samples for more robust evaluation of reliability and validity.', 'Further investigation into mitigating LLM biases such as sycophancy and ensuring interpretive agency remains with human researchers.', 'Exploring advanced infrastructure to enable higher iteration counts for LLMq and deeper analysis of LLM coding stability.', 'Embedding LLM-based deductive coding tools into real-time analytics platforms and educational interventions.']","GROPROE enables deductive qualitative analysis at scale by leveraging LLMs to code large textual corpora (e.g., 860 student reflections) with substantial agreement to human coders. The process supports iterative refinement and validation of prompts/codebooks, but current human validation is limited to small samples due to resource constraints. LLMq allows for high-throughput consistency checks impractical for humans. Scalability is contingent on computational resources, prompt clarity, and ongoing human oversight to ensure reliability and mitigate model biases."
