{
  "bibliographic_metadata": {
    "paper_id": "not_specified",
    "title": "From nCoder to ChatGPT: From Automated Coding to Refining Human Coding",
    "authors": [
      "Andres Felipe Zambrano",
      "Xiner Liu",
      "Amanda Barany",
      "Ryan S. Baker",
      "Juhan Kim",
      "Nidhi Nasiar"
    ],
    "publication_year": 2024,
    "venue": "not_specified",
    "doi_url": "not_specified",
    "research_domain": "Qualitative Research Methodology, Computational Text Analysis, Quantitative Ethnography"
  },
  "methodological_framework": {
    "analysis_type": "deductive qualitative coding; automated and semi-automated coding comparison",
    "theoretical_framework": "Quantitative Ethnography; construct validity; interrater reliability; codebook-driven deductive coding",
    "sample_characteristics": {
      "interview_count": 200,
      "participant_demographics": "governmental leaders from seven countries (press releases and public addresses); specific demographic details not_specified",
      "data_collection_method": "transcripts of press releases and public addresses delivered by governmental leaders"
    }
  },
  "technical_pipeline": {
    "automation_level": "semi-automated",
    "software_tools": [
      "nCoder",
      "ChatGPT (GPT-4)"
    ],
    "computational_methods": [
      "regular expressions",
      "large language models (LLMs)",
      "semantic similarity analysis",
      "iterative prompt engineering",
      "agreement metrics calculation (Kappa, precision, recall, Shaffer’s rho)"
    ],
    "workflow_architecture": {
      "preprocessing_steps": [
        "random selection of training and test sets from dataset",
        "definition and refinement of codebook/categories",
        "provision of construct definitions to coding tools"
      ],
      "analysis_pipeline": [
        "application of regular expressions in nCoder to code training set",
        "iterative refinement of regular expressions based on disagreements with human coders",
        "application of optimized regular expressions to test set",
        "provision of code definitions and examples to ChatGPT",
        "iterative coding with ChatGPT in batches (due to prompt length limits)",
        "requesting and reviewing ChatGPT explanations for disagreements",
        "refinement of code definitions/prompts for ChatGPT based on explanations",
        "application of refined prompts to code test set"
      ],
      "postprocessing_steps": [
        "calculation of agreement metrics (Kappa, precision, recall, Shaffer’s rho)",
        "comparison of machine and human coding results",
        "review of ChatGPT explanations for construct validity and consistency checks",
        "identification and analysis of coding inconsistencies",
        "potential revision of codebook based on insights"
      ]
    },
    "coding_framework": "deductive coding using a predefined codebook with seven categories (Medical Positive, Medical Negative, Economic Positive, Economic Negative, Social Positive, Social Negative, Political Positive) refined through iterative human-machine interaction"
  },
  "prompt_engineering": {
    "prompting_strategy": {
      "approach_type": "iterative refinement with human-in-the-loop",
      "prompt_examples": [
        "For each construct, provide ChatGPT with the construct name and original definition from the codebook.",
        "After a disagreement between ChatGPT and human coders, request an explanation from ChatGPT regarding its coding decision.",
        "Incorporate ChatGPT's explanations to enhance and clarify code definitions in the prompt, including clarifying statements and examples of appropriate and inappropriate phrases.",
        "Limit each definition provided to ChatGPT to a maximum of five sentences to avoid information overload.",
        "Conduct coding in subsets of 25 lines per prompt due to ChatGPT's maximum prompt length."
      ],
      "engineering_techniques": [
        "Iterative prompt refinement based on disagreement analysis between ChatGPT and human coders.",
        "Inclusion of construct definitions, clarifying statements, and positive/negative examples in prompts.",
        "Soliciting ChatGPT's suggestions for updating code definitions and reviewing these suggestions with human researchers.",
        "Prompt length management to fit within ChatGPT's input constraints.",
        "Human review and fine-tuning of any ChatGPT-suggested definition changes before re-prompting.",
        "Disregarding revised definitions that negatively impact agreement and proceeding to next disagreement."
      ],
      "model_specifications": "ChatGPT (GPT-4 model; see OpenAI GPT-4 Technical Report, 2023)"
    }
  },
  "empirical_results": {
    "primary_findings": [
      "Both ChatGPT (GPT-4) and nCoder have distinct advantages and disadvantages for automated coding of qualitative interview data, depending on the context, data nature, and research goals.",
      "nCoder, which relies on regular expressions, achieves higher precision and agreement with human coders for constructs that can be explicitly defined with specific language patterns, but suffers from lower recall due to its inability to generalize beyond observed examples.",
      "ChatGPT demonstrates higher recall than nCoder, effectively capturing semantic and contextual variations in language, but often exhibits lower precision due to overgeneralization, especially for constructs that are open to interpretation or less thematically discrete.",
      "ChatGPT's ability to provide detailed explanations for its coding decisions supports human coders in refining code definitions, identifying ambiguities, and improving construct validity and interrater reliability.",
      "Disagreements between automated tools and human coders can reveal inconsistencies or ambiguities in human coding schemes, and ChatGPT can serve as a peer-like support for critical reflection and systematic review of coding decisions.",
      "ChatGPT outperforms nCoder for constructs with broad or complex semantic fields (e.g., Economic Positive), but underperforms for constructs prone to subjective interpretation (e.g., Social Positive/Negative).",
      "Automated coding tools do not necessarily establish ground truth; lower agreement metrics may reflect fuzziness or ambiguity in human coding rather than tool inaccuracy."
    ],
    "evaluation_framework": {
      "metrics_employed": [
        "Cohen's Kappa",
        "Precision",
        "Recall",
        "Shaffer's rho"
      ],
      "validation_methodology": "Comparison of automated coding outputs (nCoder and ChatGPT) to human-generated codes using a training set (100 lines) and a test set (100 unobserved lines) from a dataset of governmental press releases and public addresses. Iterative refinement of code definitions and regular expressions was performed based on disagreements. Agreement metrics were calculated for both training and test sets.",
      "performance_indicators": "nCoder achieved higher average Kappa (0.77 train, 0.53 test) and precision (0.79) but lower recall (0.60) compared to ChatGPT (Kappa: 0.54 train, 0.46 test; precision: 0.52; recall: 0.80). ChatGPT's performance varied by construct, with high precision and recall for concrete categories (e.g., Economic Positive) and lower precision for open or ambiguous constructs (e.g., Social Negative)."
    },
    "methodological_limitations": [
      "nCoder's reliance on regular expressions limits its ability to generalize to unseen language patterns, resulting in lower recall.",
      "ChatGPT tends to overgeneralize for constructs that are open to interpretation or not thematically discrete, leading to lower precision.",
      "ChatGPT may misinterpret or overlook relevant nuances in the data, especially for subjective or complex constructs.",
      "The study did not re-code the dataset with human coders after refining code definitions based on ChatGPT's explanations, limiting the assessment of improvements in human coding consistency.",
      "Agreement metrics only measure (dis-)agreement between human and machine coding, not absolute accuracy or correctness.",
      "Potential for construct drift and coder inconsistency over time was not systematically addressed.",
      "The approach may be less effective for inductive code development or for constructs that are not mutually exclusive and collectively exhaustive.",
      "Prompt length and information overload constraints in ChatGPT required limiting code definitions to five sentences and coding in small batches."
    ]
  },
  "quality_assurance": {
    "reliability_measures": [
      "Cohen's Kappa",
      "Precision",
      "Recall",
      "Shaffer's rho",
      "Inter-rater agreement"
    ],
    "validity_approaches": [
      "Construct validity assessment via iterative refinement of code definitions",
      "Use of ChatGPT explanations to identify ambiguity in code definitions",
      "Critical examination of disagreements between human and machine coding",
      "Review and refinement of code definitions based on machine explanations",
      "Comparison of automated coding outputs to human-generated codes"
    ],
    "bias_mitigation_strategies": [
      "Inclusion of original dataset author to reduce misinterpretation of codebook",
      "Iterative review and refinement of regular expressions in nCoder",
      "Human review and fine-tuning of ChatGPT-suggested code definitions",
      "Systematic examination of disagreements to identify inconsistencies and idiosyncrasies in human coding",
      "Use of explanations from ChatGPT to support reflexivity and critical reflection in human coding",
      "Limiting prompt length to avoid information overload in ChatGPT",
      "Review of ChatGPT's suggestions for updating code definitions before adoption"
    ]
  },
  "research_impact": {
    "novel_contributions": [
      "Systematic comparison of ChatGPT (GPT-4) and nCoder for automated coding in Quantitative Ethnography (QE) using a real-world dataset of governmental press releases and addresses.",
      "Development of an interactive, iterative workflow for refining code definitions and prompts in ChatGPT based on explanations of coding disagreements.",
      "Demonstration of ChatGPT's ability to provide detailed, grounded explanations for coding decisions, supporting construct validity and consistency checks in human coding.",
      "Empirical evaluation of agreement metrics (Kappa, precision, recall, Shaffer’s rho) for both nCoder and ChatGPT across multiple code categories.",
      "Identification of the affordances and limitations of LLM-based coding versus regular expression-based automated coding tools."
    ],
    "practical_applications": [
      "Use of ChatGPT as a semi-automated coding assistant to support human coders in refining codebooks and resolving ambiguities in qualitative data analysis.",
      "Application of ChatGPT explanations to enhance construct validity and interrater reliability in coding schemes.",
      "Integration of ChatGPT into existing qualitative analysis pipelines to facilitate consistency checks and code definition refinement.",
      "Potential for ChatGPT to assist in reviewing and improving consistency across large, human-coded datasets, including detection of coder drift."
    ],
    "future_research_directions": [
      "Exploration of ChatGPT and other LLMs for supporting inductive code development and thematic analysis in qualitative research.",
      "Investigation of ChatGPT's utility for addressing coder drift and improving consistency in longitudinal or large-scale qualitative coding projects.",
      "Further study of the impact of ChatGPT explanations on human coder reflexivity and the iterative refinement of coding schemes.",
      "Assessment of the potential for integrating regular expressions or other rule-based elements into ChatGPT prompts to emulate or enhance nCoder-like functionality.",
      "Evaluation of LLM-based coding tools in diverse qualitative research contexts and with more complex or less discrete constructs."
    ],
    "scalability_considerations": "Manual coding is unsuitable for large datasets due to time and error constraints; nCoder scales well for constructs definable by regular expressions but struggles with recall and semantic nuance. ChatGPT, as a pre-trained LLM, can generalize to unseen vocabulary and semantic structures, offering higher recall and potential scalability for complex or varied data. However, ChatGPT's current prompt length limitations require batching data, and its performance may decrease for constructs that are open to interpretation or not mutually exclusive. Both tools require iterative refinement for optimal performance, but ChatGPT's interactive explanations can facilitate scalable, semi-automated codebook refinement and consistency checking."
  }
}