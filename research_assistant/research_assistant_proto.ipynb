{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PDF Processing\n",
    "import PyPDF2\n",
    "import fitz  # PyMuPDF for better text extraction\n",
    "import re\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# OpenAI Integration\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Vector Database (we'll use Chroma for local development)\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Utilities\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 12:13:00,468 - INFO - Directory created/verified: ..\\output\n",
      "2025-06-19 12:13:00,470 - INFO - Directory created/verified: ..\\cache\n",
      "2025-06-19 12:13:00,475 - INFO - Directory created/verified: ..\\vector_db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Configuration initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 12:13:02,367 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-19 12:13:02,383 - INFO - âœ… OpenAI API connection successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup validation passed!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Configuration and Setup\n",
    "class Config:\n",
    "    \"\"\"Configuration class for the research assistant pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # API Configuration\n",
    "        self.openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        self.openai_model = \"gpt-4o\"  \n",
    "        \n",
    "        # File paths\n",
    "        self.files_dir = Path(\"../files\")\n",
    "        self.output_dir = Path(\"../output\")\n",
    "        self.cache_dir = Path(\"../cache\")\n",
    "        \n",
    "        # Vector Database\n",
    "        self.vector_db_path = Path(\"../vector_db\")\n",
    "        self.embedding_model_name = \"all-MiniLM-L6-v2\"  # Fast and effective\n",
    "        \n",
    "        # Processing settings\n",
    "        self.max_chunk_size = 1000  # characters per chunk\n",
    "        self.chunk_overlap = 200    # characters overlap between chunks\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        self._create_directories()\n",
    "    \n",
    "    def _create_directories(self):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        for directory in [self.output_dir, self.cache_dir, self.vector_db_path]:\n",
    "            directory.mkdir(exist_ok=True)\n",
    "            logger.info(f\"Directory created/verified: {directory}\")\n",
    "    \n",
    "    def validate_setup(self):\n",
    "        \"\"\"Validate that all required components are properly configured\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # Check OpenAI API key\n",
    "        if not self.openai_api_key:\n",
    "            issues.append(\"OpenAI API key not found. Please set OPENAI_API_KEY in your .env file.\")\n",
    "        \n",
    "        # Check if files directory exists\n",
    "        if not self.files_dir.exists():\n",
    "            issues.append(f\"Files directory not found: {self.files_dir}\")\n",
    "        \n",
    "        # Check if we can access OpenAI\n",
    "        if self.openai_api_key:\n",
    "            try:\n",
    "                client = OpenAI(api_key=self.openai_api_key)\n",
    "                # Simple test call\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                    max_tokens=5\n",
    "                )\n",
    "                logger.info(\"âœ… OpenAI API connection successful\")\n",
    "            except Exception as e:\n",
    "                issues.append(f\"OpenAI API connection failed: {str(e)}\")\n",
    "        \n",
    "        return issues\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "print(\"ðŸ”§ Configuration initialized\")\n",
    "\n",
    "# Validate setup\n",
    "issues = config.validate_setup()\n",
    "if issues:\n",
    "    print(\"âŒ Setup issues found:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"âœ… Setup validation passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PDF Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ PDF Processor initialized\n"
     ]
    }
   ],
   "source": [
    "# PDF Processing Utilities\n",
    "class PDFProcessor:\n",
    "    \"\"\"Handles PDF text extraction and processing\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: Path) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract text from PDF with page information and metadata\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing text, pages, and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use PyMuPDF for better text extraction\n",
    "            doc = fitz.open(pdf_path)\n",
    "            \n",
    "            total_pages = len(doc)\n",
    "            \n",
    "            extracted_data = {\n",
    "                'file_path': str(pdf_path),\n",
    "                'file_name': pdf_path.name,\n",
    "                'total_pages': len(doc),\n",
    "                'pages': [],\n",
    "                'full_text': \"\",\n",
    "                'metadata': doc.metadata,\n",
    "                'extraction_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            # Extract text from each page\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc.load_page(page_num)\n",
    "                text = page.get_text()\n",
    "                \n",
    "                page_data = {\n",
    "                    'page_number': page_num + 1,\n",
    "                    'text': text,\n",
    "                    'text_length': len(text)\n",
    "                }\n",
    "                \n",
    "                extracted_data['pages'].append(page_data)\n",
    "                extracted_data['full_text'] += f\"\\n--- Page {page_num + 1} ---\\n{text}\"\n",
    "            \n",
    "            doc.close()\n",
    "            logger.info(f\"Successfully extracted text from {pdf_path.name} ({total_pages} pages)\")\n",
    "            return extracted_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting text from {pdf_path}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def detect_titles_and_sections(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Detect titles and sections in the text using regex patterns\n",
    "        \n",
    "        Args:\n",
    "            text: Full text of the document\n",
    "            \n",
    "        Returns:\n",
    "            List of detected sections with their titles and content\n",
    "        \"\"\"\n",
    "        # Common title patterns\n",
    "        title_patterns = [\n",
    "            r'^(\\d+\\.\\s+[A-Z][^.\\n]+)',  # 1. Title\n",
    "            r'^([A-Z][A-Z\\s]{3,}[A-Z])',  # ALL CAPS TITLES\n",
    "            r'^(\\d+\\.\\d+\\s+[A-Z][^.\\n]+)',  # 1.1. Subtitle\n",
    "            r'^([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s*:)',  # Title:\n",
    "        ]\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        sections = []\n",
    "        current_section = None\n",
    "        \n",
    "        for line_num, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Check if line matches any title pattern\n",
    "            is_title = False\n",
    "            title_level = 0\n",
    "            \n",
    "            for pattern in title_patterns:\n",
    "                match = re.match(pattern, line)\n",
    "                if match:\n",
    "                    is_title = True\n",
    "                    title_level = len(pattern.split('\\\\d+')) - 1  # Rough level estimation\n",
    "                    break\n",
    "            \n",
    "            if is_title:\n",
    "                # Save previous section if exists\n",
    "                if current_section:\n",
    "                    sections.append(current_section)\n",
    "                \n",
    "                # Start new section\n",
    "                current_section = {\n",
    "                    'title': line,\n",
    "                    'title_level': title_level,\n",
    "                    'content': line + '\\n',\n",
    "                    'start_line': line_num,\n",
    "                    'end_line': line_num\n",
    "                }\n",
    "            elif current_section:\n",
    "                # Add line to current section\n",
    "                current_section['content'] += line + '\\n'\n",
    "                current_section['end_line'] = line_num\n",
    "        \n",
    "        # Add the last section\n",
    "        if current_section:\n",
    "            sections.append(current_section)\n",
    "        \n",
    "        # If no sections detected, create one section with all content\n",
    "        if not sections:\n",
    "            sections = [{\n",
    "                'title': 'Document Content',\n",
    "                'title_level': 0,\n",
    "                'content': text,\n",
    "                'start_line': 0,\n",
    "                'end_line': len(lines) - 1\n",
    "            }]\n",
    "        \n",
    "        logger.info(f\"Detected {len(sections)} sections in the document\")\n",
    "        return sections\n",
    "    \n",
    "    def create_chunks(self, text: str, max_size: int = None, overlap: int = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Create overlapping chunks from text for embedding\n",
    "        \n",
    "        Args:\n",
    "            text: Text to chunk\n",
    "            max_size: Maximum chunk size in characters\n",
    "            overlap: Overlap size in characters\n",
    "            \n",
    "        Returns:\n",
    "            List of text chunks\n",
    "        \"\"\"\n",
    "        max_size = max_size or self.config.max_chunk_size\n",
    "        overlap = overlap or self.config.chunk_overlap\n",
    "        \n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + max_size\n",
    "            \n",
    "            # Try to break at sentence boundary\n",
    "            if end < len(text):\n",
    "                # Look for sentence endings\n",
    "                for i in range(end, max(start, end - 100), -1):\n",
    "                    if text[i] in '.!?':\n",
    "                        end = i + 1\n",
    "                        break\n",
    "            \n",
    "            chunk = text[start:end].strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "            start = end - overlap\n",
    "            if start >= len(text):\n",
    "                break\n",
    "        \n",
    "        logger.info(f\"Created {len(chunks)} chunks from text\")\n",
    "        return chunks\n",
    "\n",
    "# Initialize PDF processor\n",
    "pdf_processor = PDFProcessor(config)\n",
    "print(\"ðŸ“„ PDF Processor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 12:06:34,340 - INFO - Successfully extracted text from extracted_pages_134_149.pdf (16 pages)\n",
      "2025-06-19 12:06:34,348 - INFO - Detected 86 sections in the document\n",
      "2025-06-19 12:06:34,350 - INFO - Created 59 chunks from text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“– Processing: ..\\files\\extracted_pages_134_149.pdf\n",
      "âœ… Extracted 16 pages\n",
      "ï¿½ï¿½ Total text length: 44942 characters\n",
      "ðŸ“‹ Detected 86 sections:\n",
      "  1. Keywords: Large Language Models Â· ChatGPT Â· Induct...\n",
      "  2. 1. Assistance...\n",
      "  3. 2. Encouragement...\n",
      "  4. 3. Checking in/concern...\n",
      "  5. 4. Comfort/consolation...\n",
      "ðŸ”— Created 59 chunks for embedding\n"
     ]
    }
   ],
   "source": [
    "# Test the PDF processing with your extracted pages\n",
    "def test_pdf_processing():\n",
    "    \"\"\"Test the PDF processing pipeline with the extracted pages\"\"\"\n",
    "    \n",
    "    # Path to the extracted pages PDF\n",
    "    extracted_pdf_path = config.files_dir / \"extracted_pages_134_149.pdf\"\n",
    "    \n",
    "    if not extracted_pdf_path.exists():\n",
    "        print(f\"âŒ Extracted PDF not found: {extracted_pdf_path}\")\n",
    "        print(\"Please run the extract_pdf_pages.py script first to create the extracted pages.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"ðŸ“– Processing: {extracted_pdf_path}\")\n",
    "    \n",
    "    # Extract text\n",
    "    extracted_data = pdf_processor.extract_text_from_pdf(extracted_pdf_path)\n",
    "    \n",
    "    print(f\"âœ… Extracted {extracted_data['total_pages']} pages\")\n",
    "    print(f\"ï¿½ï¿½ Total text length: {len(extracted_data['full_text'])} characters\")\n",
    "    \n",
    "    # Detect sections\n",
    "    sections = pdf_processor.detect_titles_and_sections(extracted_data['full_text'])\n",
    "    \n",
    "    print(f\"ðŸ“‹ Detected {len(sections)} sections:\")\n",
    "    for i, section in enumerate(sections[:5]):  # Show first 5 sections\n",
    "        print(f\"  {i+1}. {section['title'][:50]}...\")\n",
    "    \n",
    "    # Create chunks\n",
    "    chunks = pdf_processor.create_chunks(extracted_data['full_text'])\n",
    "    \n",
    "    print(f\"ðŸ”— Created {len(chunks)} chunks for embedding\")\n",
    "    \n",
    "    return {\n",
    "        'extracted_data': extracted_data,\n",
    "        'sections': sections,\n",
    "        'chunks': chunks\n",
    "    }\n",
    "\n",
    "# Run the test\n",
    "test_results = test_pdf_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vector DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 12:08:48,101 - INFO - Use pytorch device_name: cpu\n",
      "2025-06-19 12:08:48,101 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-06-19 12:08:53,513 - INFO - âœ… Vector database initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—„ï¸ Vector Database initialized\n"
     ]
    }
   ],
   "source": [
    "# Vector Database Setup\n",
    "class VectorDatabase:\n",
    "    \"\"\"Handles vector database operations for document embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self.embedding_model = None\n",
    "        self._initialize()\n",
    "    \n",
    "    def _initialize(self):\n",
    "        \"\"\"Initialize the vector database and embedding model\"\"\"\n",
    "        try:\n",
    "            # Initialize ChromaDB\n",
    "            self.client = chromadb.PersistentClient(\n",
    "                path=str(self.config.vector_db_path),\n",
    "                settings=Settings(anonymized_telemetry=False)\n",
    "            )\n",
    "            \n",
    "            # Initialize embedding model\n",
    "            self.embedding_model = SentenceTransformer(self.config.embedding_model_name)\n",
    "            \n",
    "            # Create or get collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=\"research_documents\",\n",
    "                metadata={\"description\": \"Research document embeddings\"}\n",
    "            )\n",
    "            \n",
    "            logger.info(\"âœ… Vector database initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing vector database: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def add_documents(self, documents: List[Dict[str, Any]], collection_name: str = None) -> bool:\n",
    "        \"\"\"\n",
    "        Add documents to the vector database\n",
    "        \n",
    "        Args:\n",
    "            documents: List of document dictionaries with 'text' and 'metadata' keys\n",
    "            collection_name: Optional collection name\n",
    "            \n",
    "        Returns:\n",
    "            Success status\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not documents:\n",
    "                logger.warning(\"No documents to add\")\n",
    "                return False\n",
    "            \n",
    "            # Prepare documents for embedding\n",
    "            texts = [doc['text'] for doc in documents]\n",
    "            metadatas = [doc.get('metadata', {}) for doc in documents]\n",
    "            ids = [doc.get('id', f\"doc_{i}\") for i, doc in enumerate(documents)]\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = self.embedding_model.encode(texts).tolist()\n",
    "            \n",
    "            # Add to collection\n",
    "            collection = self.collection if collection_name is None else self.client.get_or_create_collection(collection_name)\n",
    "            collection.add(\n",
    "                embeddings=embeddings,\n",
    "                documents=texts,\n",
    "                metadatas=metadatas,\n",
    "                ids=ids\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"âœ… Added {len(documents)} documents to vector database\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error adding documents to vector database: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def search_similar(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search for similar documents\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            n_results: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of similar documents with scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate query embedding\n",
    "            query_embedding = self.embedding_model.encode([query]).tolist()\n",
    "            \n",
    "            # Search in collection\n",
    "            results = self.collection.query(\n",
    "                query_embeddings=query_embedding,\n",
    "                n_results=n_results\n",
    "            )\n",
    "            \n",
    "            # Format results\n",
    "            formatted_results = []\n",
    "            for i in range(len(results['documents'][0])):\n",
    "                formatted_results.append({\n",
    "                    'document': results['documents'][0][i],\n",
    "                    'metadata': results['metadatas'][0][i],\n",
    "                    'distance': results['distances'][0][i],\n",
    "                    'id': results['ids'][0][i]\n",
    "                })\n",
    "            \n",
    "            return formatted_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching vector database: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "# Initialize vector database\n",
    "vector_db = VectorDatabase(config)\n",
    "print(\"ðŸ—„ï¸ Vector Database initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. OpenAI Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– OpenAI Client initialized\n"
     ]
    }
   ],
   "source": [
    "# OpenAI Integration Setup\n",
    "class OpenAIClient:\n",
    "    \"\"\"Handles OpenAI API interactions for document processing\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.client = OpenAI(api_key=config.openai_api_key)\n",
    "    \n",
    "    def extract_information(self, text: str, schema: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract structured information from text using OpenAI\n",
    "        \n",
    "        Args:\n",
    "            text: Text to analyze\n",
    "            schema: Schema defining what to extract\n",
    "            \n",
    "        Returns:\n",
    "            Extracted information in structured format\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create prompt for information extraction\n",
    "            schema_description = json.dumps(schema, indent=2)\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "            You are an expert research assistant specializing in qualitative research methodology and computational text analysis. Your task is to perform systematic data extraction from academic literature focusing on deductive qualitative analysis pipelines for interview data. Extract information with precision and technical accuracy, maintaining methodological rigor throughout the analysis.\n",
    "            \n",
    "            # Task Definition\n",
    "            Analyze the provided research document and extract structured information related to deductive qualitative analysis methodologies, computational workflows, and evaluation frameworks. Focus on identifying technical specifications, methodological approaches, and empirical findings relevant to automated or semi-automated qualitative data analysis pipelines.\n",
    "            \n",
    "            # Extraction Schema\n",
    "            Extract the following information and return it as a structured JSON object with the specified keys:\n",
    "            \n",
    "            # Bibliographic Metadata\n",
    "\n",
    "            paper_id: Generate a unique identifier (format: YYYY_AuthorLastName_KeywordAbbrev)\n",
    "            title: Complete paper title\n",
    "            authors: Array of author names\n",
    "            publication_year: Year of publication\n",
    "            venue: Journal name or conference proceedings\n",
    "            doi_url: Digital Object Identifier or URL\n",
    "            research_domain: Primary research field or application domain\n",
    "\n",
    "            # Methodological Framework\n",
    "\n",
    "            analysis_type: Classification of analytical approach (deductive, inductive, abductive, mixed-methods)\n",
    "            theoretical_framework: Underlying theoretical model or conceptual framework guiding the deductive approach\n",
    "            sample_characteristics: Object containing:\n",
    "\n",
    "            interview_count: Number of interviews analyzed\n",
    "            participant_demographics: Demographic composition of study participants\n",
    "            data_collection_method: Interview format (structured, semi-structured, unstructured, focus_groups)\n",
    "\n",
    "\n",
    "\n",
    "            # Technical Pipeline Architecture\n",
    "\n",
    "            automation_level: Degree of computational automation (manual, semi_automated, fully_automated, hybrid)\n",
    "            software_tools: Array of software platforms, libraries, or frameworks utilized\n",
    "            computational_methods: Array of AI/ML techniques employed (llm_based, traditional_nlp, rule_based, statistical_methods)\n",
    "            workflow_architecture: Object containing:\n",
    "\n",
    "            preprocessing_steps: Data preparation and cleaning procedures\n",
    "            analysis_pipeline: Sequential processing stages\n",
    "            postprocessing_steps: Output refinement and validation procedures\n",
    "\n",
    "\n",
    "            coding_framework: Predetermined coding scheme or categorization system\n",
    "\n",
    "            # Prompt Engineering Specifications\n",
    "\n",
    "            prompting_strategy: Object containing:\n",
    "\n",
    "            approach_type: Prompting methodology (zero_shot, few_shot, chain_of_thought, tree_of_thought, role_based)\n",
    "            prompt_examples: Array of actual prompt templates or examples (if available)\n",
    "            engineering_techniques: Prompt optimization strategies employed\n",
    "            model_specifications: LLM or AI model details (model_name, version, parameters)\n",
    "\n",
    "\n",
    "            # Empirical Results and Evaluation\n",
    "\n",
    "            primary_findings: Array of key thematic discoveries or pattern identifications\n",
    "            evaluation_framework: Object containing:\n",
    "\n",
    "            metrics_employed: Quantitative evaluation measures (inter_rater_reliability, precision, recall, f1_score, kappa_statistic)\n",
    "            validation_methodology: Validation approach (expert_review, ground_truth_comparison, cross_validation)\n",
    "            performance_indicators: Numerical results or performance benchmarks\n",
    "\n",
    "\n",
    "            methodological_limitations: Acknowledged constraints or methodological shortcomings\n",
    "\n",
    "            # Quality Assurance and Reliability\n",
    "\n",
    "            reliability_measures: Statistical measures of consistency (cohens_kappa, fleiss_kappa, percentage_agreement, cronbach_alpha)\n",
    "            validity_approaches: Validity enhancement strategies (member_checking, triangulation, peer_debriefing, audit_trail)\n",
    "            bias_mitigation_strategies: Approaches to minimize analytical bias or systematic errors\n",
    "\n",
    "            # Research Contributions and Impact\n",
    "\n",
    "            novel_contributions: Methodological innovations or technical advances\n",
    "            practical_applications: Real-world implementation scenarios or use cases\n",
    "            future_research_directions: Suggested extensions or research opportunities\n",
    "            scalability_considerations: Discussion of scalability to larger datasets or different contexts\n",
    "\n",
    "            Output Format Requirements\n",
    "            Return a valid JSON object with the following structure:\n",
    "            json\n",
    "            Extraction Guidelines\n",
    "\n",
    "            Completeness: Extract all available information; use \"not_specified\" for missing data\n",
    "            Precision: Maintain technical terminology and methodological specificity\n",
    "            Contextual Accuracy: Preserve the original meaning and technical context\n",
    "            Standardization: Use consistent terminology across extractions\n",
    "            Null Handling: Use empty arrays [] for missing list items, null for missing values\n",
    "\n",
    "            Quality Control Instructions\n",
    "\n",
    "            Verify technical terminology accuracy\n",
    "            Cross-reference methodological claims with reported procedures\n",
    "            Distinguish between claimed capabilities and empirically demonstrated results\n",
    "            Flag any ambiguous or contradictory information in the source document\n",
    "            Prioritize explicit methodological descriptions over implicit assumptions\n",
    "\n",
    "            Begin extraction analysis upon document provision.\n",
    "            \n",
    "            {\n",
    "                {\n",
    "            \"bibliographic_metadata\": {\n",
    "                \"paper_id\": \"string\",\n",
    "                \"title\": \"string\",\n",
    "                \"authors\": [\"string\"],\n",
    "                \"publication_year\": \"integer\",\n",
    "                \"venue\": \"string\",\n",
    "                \"doi_url\": \"string\",\n",
    "                \"research_domain\": \"string\"\n",
    "            },\n",
    "            \"methodological_framework\": {\n",
    "                \"analysis_type\": \"string\",\n",
    "                \"theoretical_framework\": \"string\",\n",
    "                \"sample_characteristics\": {\n",
    "                \"interview_count\": \"integer\",\n",
    "                \"participant_demographics\": \"string\",\n",
    "                \"data_collection_method\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"technical_pipeline\": {\n",
    "                \"automation_level\": \"string\",\n",
    "                \"software_tools\": [\"string\"],\n",
    "                \"computational_methods\": [\"string\"],\n",
    "                \"workflow_architecture\": {\n",
    "                \"preprocessing_steps\": [\"string\"],\n",
    "                \"analysis_pipeline\": [\"string\"],\n",
    "                \"postprocessing_steps\": [\"string\"]\n",
    "                },\n",
    "                \"coding_framework\": \"string\"\n",
    "            },\n",
    "            \"prompt_engineering\": {\n",
    "                \"prompting_strategy\": {\n",
    "                \"approach_type\": \"string\",\n",
    "                \"prompt_examples\": [\"string\"],\n",
    "                \"engineering_techniques\": [\"string\"],\n",
    "                \"model_specifications\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"empirical_results\": {\n",
    "                \"primary_findings\": [\"string\"],\n",
    "                \"evaluation_framework\": {\n",
    "                \"metrics_employed\": [\"string\"],\n",
    "                \"validation_methodology\": \"string\",\n",
    "                \"performance_indicators\": \"string\"\n",
    "                },\n",
    "                \"methodological_limitations\": [\"string\"]\n",
    "            },\n",
    "            \"quality_assurance\": {\n",
    "                \"reliability_measures\": [\"string\"],\n",
    "                \"validity_approaches\": [\"string\"],\n",
    "                \"bias_mitigation_strategies\": [\"string\"]\n",
    "            },\n",
    "            \"research_impact\": {\n",
    "                \"novel_contributions\": [\"string\"],\n",
    "                \"practical_applications\": [\"string\"],\n",
    "                \"future_research_directions\": [\"string\"],\n",
    "                \"scalability_considerations\": \"string\"\n",
    "            }\n",
    "            }\n",
    "            }\n",
    "            \n",
    "            Text to analyze:\n",
    "            {text[:4000]}  # Limit text length for API efficiency\n",
    "            \n",
    "            Please return the extracted information in valid JSON format matching the schema.\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.config.openai_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a research assistant that extracts structured information from academic documents. Always respond with valid JSON.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1,  # Low temperature for consistent extraction\n",
    "                max_tokens=10000\n",
    "            )\n",
    "            \n",
    "            # Parse response\n",
    "            extracted_data = json.loads(response.choices[0].message.content)\n",
    "            logger.info(\"âœ… Information extraction completed\")\n",
    "            return extracted_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting information: {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def answer_question(self, question: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Answer a question based on provided context\n",
    "        \n",
    "        Args:\n",
    "            question: User question\n",
    "            context: Relevant context from documents\n",
    "            \n",
    "        Returns:\n",
    "            Answer to the question\n",
    "        \"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following context, please answer the question. If the context doesn't contain enough information to answer the question, say so.\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Answer:\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.config.openai_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful research assistant. Answer questions based on the provided context.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error answering question: {str(e)}\")\n",
    "            return \"Sorry, I encountered an error while processing your question.\"\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAIClient(config)\n",
    "print(\"ðŸ¤– OpenAI Client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Pipeline Class\n",
    "class ResearchAssistantPipeline:\n",
    "    \"\"\"Main pipeline that orchestrates document processing and AI interactions\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.pdf_processor = PDFProcessor(config)\n",
    "        self.vector_db = VectorDatabase(config)\n",
    "        self.openai_client = OpenAIClient(config)\n",
    "    \n",
    "    def process_document(self, pdf_path: Path) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a document through the complete pipeline\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the PDF document\n",
    "            \n",
    "        Returns:\n",
    "            Processing results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"ðŸš€ Starting pipeline for: {pdf_path.name}\")\n",
    "            \n",
    "            # Step 1: Extract text from PDF\n",
    "            extracted_data = self.pdf_processor.extract_text_from_pdf(pdf_path)\n",
    "            \n",
    "            # Step 2: Detect sections\n",
    "            sections = self.pdf_processor.detect_titles_and_sections(extracted_data['full_text'])\n",
    "            \n",
    "            # Step 3: Create chunks for embedding\n",
    "            chunks = self.pdf_processor.create_chunks(extracted_data['full_text'])\n",
    "            \n",
    "            # Step 4: Prepare documents for vector database\n",
    "            documents_for_embedding = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                doc_id = f\"{pdf_path.stem}_chunk_{i}\"\n",
    "                documents_for_embedding.append({\n",
    "                    'id': doc_id,\n",
    "                    'text': chunk,\n",
    "                    'metadata': {\n",
    "                        'source_file': pdf_path.name,\n",
    "                        'chunk_index': i,\n",
    "                        'total_chunks': len(chunks),\n",
    "                        'processing_timestamp': datetime.now().isoformat()\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            # Step 5: Add to vector database\n",
    "            self.vector_db.add_documents(documents_for_embedding)\n",
    "            \n",
    "            # Step 6: Extract structured information (placeholder for now)\n",
    "            # We'll implement this when we define the schema\n",
    "            \n",
    "            results = {\n",
    "                'file_name': pdf_path.name,\n",
    "                'total_pages': extracted_data['total_pages'],\n",
    "                'sections_detected': len(sections),\n",
    "                'chunks_created': len(chunks),\n",
    "                'embeddings_stored': len(documents_for_embedding),\n",
    "                'processing_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"âœ… Pipeline completed for {pdf_path.name}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in pipeline: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def ask_question(self, question: str, n_context_chunks: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Ask a question about the processed documents\n",
    "        \n",
    "        Args:\n",
    "            question: User question\n",
    "            n_context_chunks: Number of context chunks to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Answer to the question\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Search for relevant context\n",
    "            similar_docs = self.vector_db.search_similar(question, n_context_chunks)\n",
    "            \n",
    "            if not similar_docs:\n",
    "                return \"I couldn't find any relevant information in the processed documents.\"\n",
    "            \n",
    "            # Combine context\n",
    "            context = \"\\n\\n\".join([doc['document'] for doc in similar_docs])\n",
    "            \n",
    "            # Get answer from OpenAI\n",
    "            answer = self.openai_client.answer_question(question, context)\n",
    "            \n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error answering question: {str(e)}\")\n",
    "            return \"Sorry, I encountered an error while processing your question.\"\n",
    "\n",
    "# Initialize the main pipeline\n",
    "pipeline = ResearchAssistantPipeline(config)\n",
    "print(\"ðŸŽ¯ Research Assistant Pipeline initialized\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quali_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
